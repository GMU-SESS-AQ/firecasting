[{
  "history_id" : "ka7jia2ox13",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717260625843,
  "history_end_time" : 1717260625843,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gfqyccj46ug",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717258417401,
  "history_end_time" : 1717260624665,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "09san20m3b9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717233644386,
  "history_end_time" : 1717258424645,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "hqxp1n4luts",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717233051782,
  "history_end_time" : 1717233634237,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "viy3bfspzxj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717183214970,
  "history_end_time" : 1717233633763,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ryfb6goc772",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717182654887,
  "history_end_time" : 1717182654887,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ouyf734gwi7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716612552456,
  "history_end_time" : 1717233631813,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "knu6hpw2nbd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716611582534,
  "history_end_time" : 1716612551157,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3qk6fp5kid5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716610207108,
  "history_end_time" : 1716611579212,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "b1hovltbwa3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716606646661,
  "history_end_time" : 1716610196717,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "939u73u2302",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716120710369,
  "history_end_time" : 1716610159515,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "butnrbl8my2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716068762995,
  "history_end_time" : 1716076012594,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "hgsn3ct8ewc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716068608563,
  "history_end_time" : 1716068760868,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ur20qh5bwn3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714808468673,
  "history_end_time" : 1714838057262,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "92sv47atdza",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714161440460,
  "history_end_time" : 1714161440460,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "32486w19dan",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713966604350,
  "history_end_time" : 1713966604350,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "oh4jiy1cyi5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713935811085,
  "history_end_time" : 1713935811085,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "z94a6r65to3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1712452760505,
  "history_end_time" : 1712452760505,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "a36ez5qbap4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1712452402389,
  "history_end_time" : 1712452749962,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3mo1nnwe3fr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1712445947146,
  "history_end_time" : 1712445947146,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1menu9jotoc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711723378021,
  "history_end_time" : 1711723378021,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nra8jkqwmna",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711722163451,
  "history_end_time" : 1711723321978,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "427iabvqce0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711203851072,
  "history_end_time" : 1712452724075,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "inm46ezo5g1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711136729445,
  "history_end_time" : 1711136729445,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lwx5qpz81xy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711136573121,
  "history_end_time" : 1711136662817,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "m4bfaw1atse",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709930548095,
  "history_end_time" : 1712452725019,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "un54kd7qyzu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709930050151,
  "history_end_time" : 1709930509962,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "0vqiqm8tqai",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709444795635,
  "history_end_time" : 1712452725798,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "4e8q8c1fyub",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709444438278,
  "history_end_time" : 1709444687349,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "RlFMeGplNrnu",
  "history_input" : "#!/bin/bash\n# This file is dedicated to prepare the testing data. \n\n# 1) we need a complete rewrite of this process.\n# 2) separate the training data preparation and testing data preparation.\n# 3) All the share functions should go to the util process. \n\necho \"start to run test_data_slurm_generated.sh\"\npwd\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"fc_model_data_preprocess_slurm_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J fc_model_data_preprocessing       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 12               # Number of CPUs per task (threads)\n#SBATCH --mem=50G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-01:00         # Runtime in D-HH:MM format\n## Slurm can send you updates via email\n#SBATCH --mail-type=FAIL  # BEGIN,END,FAIL         # ALL,NONE,BEGIN,END,FAIL,REQUEUE,..\n#SBATCH --mail-user=zsun@gmu.edu     # Put your GMU email address here\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython << INNER_EOF\n\nfrom fc_test_data_preparation import prepare_testing_data_for_2_weeks_forecasting\n\nif __name__ == \"__main__\":\n  #training_end_date = \"20200715\"\n  #prepare_training_data(training_end_date)\n  output_folder_full_path = f'/groups/ESS3/zsun/firecasting/data/output/test_if_predicted_frp_used/20210718/'\n  prepare_testing_data_for_2_weeks_forecasting(\"20210714\", \"20210714\", output_folder_full_path)\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nwhile true; do\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName%50,State,ExitCode,MaxRSS,Start,End -j $job_id\nfind /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\n#cat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n\n",
  "history_output" : "start to run test_data_slurm_generated.sh\n/home/zsun/gw-workspace/RlFMeGplNrnu\nwrite the slurm script into fc_model_data_preprocess_slurm_generated.sh\nsbatch fc_model_data_preprocess_slurm_generated.sh\njob_id=1448707\nJob 1448707 has finished with state: JobState=COMPLETED\nSlurm job (1448707) has finished.\nPrint the job's output logs\nJobID                                                   JobName      State ExitCode     MaxRSS               Start                 End \n------------ -------------------------------------------------- ---------- -------- ---------- ------------------- ------------------- \n1448707                             fc_model_data_preprocessing  COMPLETED      0:0            2024-01-19T11:34:33 2024-01-19T11:38:30 \n1448707.bat+                                              batch  COMPLETED      0:0    407884K 2024-01-19T11:34:33 2024-01-19T11:38:30 \n1448707.ext+                                             extern  COMPLETED      0:0          0 2024-01-19T11:34:33 2024-01-19T11:38:30 \nIndex(['LAT', ' LON', 'Nearest_1', 'Nearest_2', 'Nearest_3', 'Nearest_4',\n       'Nearest_5', 'Nearest_6', 'Nearest_7', 'Nearest_8', 'Nearest_9',\n       'Nearest_10', 'Nearest_11', 'Nearest_12', 'Nearest_13', 'Nearest_14',\n       'Nearest_15', 'Nearest_16', 'Nearest_17', 'Nearest_18', 'Nearest_19',\n       'Nearest_20', 'Nearest_21', 'Nearest_22', 'Nearest_23', 'Nearest_24'],\n      dtype='object')\nRead from original folder for current date: 20210714\ncurrent_start_dt is: 2021-07-14 00:00:00\nreading past files for 2021-07-13 00:00:00\nreading from original folder\nreading past files for 2021-07-12 00:00:00\nreading from original folder\nreading past files for 2021-07-11 00:00:00\nreading from original folder\nreading past files for 2021-07-10 00:00:00\nreading from original folder\nreading past files for 2021-07-09 00:00:00\nreading from original folder\nreading past files for 2021-07-08 00:00:00\nreading from original folder\nreading past files for 2021-07-07 00:00:00\nreading from original folder\n    LAT         LON   FRP  ...   FRP_5_days_ago   FRP_6_days_ago   FRP_7_days_ago\n0  24.5 -126.000000   0.0  ...              0.0              0.0              0.0\n1  24.5 -125.899994   0.0  ...              0.0              0.0              0.0\n2  24.5 -125.800003   0.0  ...              0.0              0.0              0.0\n3  24.5 -125.699997   0.0  ...              0.0              0.0              0.0\n4  24.5 -125.599998   0.0  ...              0.0              0.0              0.0\n[5 rows x 22 columns]\nnearest columns:  Index(['Nearest_1', 'Nearest_2', 'Nearest_3', 'Nearest_4', 'Nearest_5',\n       'Nearest_6', 'Nearest_7', 'Nearest_8', 'Nearest_9', 'Nearest_10',\n       'Nearest_11', 'Nearest_12', 'Nearest_13', 'Nearest_14', 'Nearest_15',\n       'Nearest_16', 'Nearest_17', 'Nearest_18', 'Nearest_19', 'Nearest_20',\n       'Nearest_21', 'Nearest_22', 'Nearest_23', 'Nearest_24'],\n      dtype='object')\ndf.shape:  (156861, 22)\ndf.iloc[100] =  0.0\nnearest_columns length:  24\nnew_df.shape =  (156861, 24)\ndf.shape =  (156861, 22)\nNew time series dataframe:      LAT         LON   FRP  ...  Nearest_22  Nearest_23  Nearest_24\n0  24.5 -126.000000   0.0  ...         0.0         0.0         0.0\n1  24.5 -125.899994   0.0  ...         0.0         0.0         0.0\n2  24.5 -125.800003   0.0  ...         0.0         0.0         0.0\n3  24.5 -125.699997   0.0  ...         0.0         0.0         0.0\n4  24.5 -125.599998   0.0  ...         0.0         0.0         0.0\n[5 rows x 46 columns]\nAll slurm job for fc_model_data_preprocess_slurm_generated.sh finishes.\n",
  "history_begin_time" : 1705682071562,
  "history_end_time" : 1705682313191,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "dMiCeSknr4aX",
  "history_input" : "#!/bin/bash\n# This file is dedicated to prepare the testing data. \n\n# 1) we need a complete rewrite of this process.\n# 2) separate the training data preparation and testing data preparation.\n# 3) All the share functions should go to the util process. \n\necho \"start to run test_data_slurm_generated.sh\"\npwd\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"fc_model_data_preprocess_slurm_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J fc_model_data_preprocessing       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 12               # Number of CPUs per task (threads)\n#SBATCH --mem=50G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-01:00         # Runtime in D-HH:MM format\n## Slurm can send you updates via email\n#SBATCH --mail-type=FAIL  # BEGIN,END,FAIL         # ALL,NONE,BEGIN,END,FAIL,REQUEUE,..\n#SBATCH --mail-user=zsun@gmu.edu     # Put your GMU email address here\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython << INNER_EOF\n\nfrom fc_test_data_preparation import prepare_testing_data_for_2_weeks_forecasting\n\nif __name__ == \"__main__\":\n  #training_end_date = \"20200715\"\n  #prepare_training_data(training_end_date)\n  output_folder_full_path = f'/groups/ESS3/zsun/firecasting/data/output/test_if_predicted_frp_used/20210718/'\n  prepare_testing_data_for_2_weeks_forecasting(\"20210714\", \"20210714\", output_folder_full_path)\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nwhile true; do\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName%50,State,ExitCode,MaxRSS,Start,End -j $job_id\nfind /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\n#cat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n\n",
  "history_output" : "start to run test_data_slurm_generated.sh\n/home/zsun/gw-workspace/dMiCeSknr4aX\nwrite the slurm script into fc_model_data_preprocess_slurm_generated.sh\nsbatch fc_model_data_preprocess_slurm_generated.sh\njob_id=1448704\nJob 1448704 has finished with state: JobState=COMPLETED\nSlurm job (1448704) has finished.\nPrint the job's output logs\nJobID                                                   JobName      State ExitCode     MaxRSS               Start                 End \n------------ -------------------------------------------------- ---------- -------- ---------- ------------------- ------------------- \n1448704                             fc_model_data_preprocessing  COMPLETED      0:0            2024-01-19T11:27:08 2024-01-19T11:31:08 \n1448704.bat+                                              batch  COMPLETED      0:0    401936K 2024-01-19T11:27:08 2024-01-19T11:31:08 \n1448704.ext+                                             extern  COMPLETED      0:0          0 2024-01-19T11:27:08 2024-01-19T11:31:08 \nIndex(['LAT', ' LON', 'Nearest_1', 'Nearest_2', 'Nearest_3', 'Nearest_4',\n       'Nearest_5', 'Nearest_6', 'Nearest_7', 'Nearest_8', 'Nearest_9',\n       'Nearest_10', 'Nearest_11', 'Nearest_12', 'Nearest_13', 'Nearest_14',\n       'Nearest_15', 'Nearest_16', 'Nearest_17', 'Nearest_18', 'Nearest_19',\n       'Nearest_20', 'Nearest_21', 'Nearest_22', 'Nearest_23', 'Nearest_24'],\n      dtype='object')\nRead from original folder for current date: 20210714\ncurrent_start_dt is: 2021-07-14 00:00:00\nreading past files for 2021-07-13 00:00:00\nreading from original folder\nreading past files for 2021-07-12 00:00:00\nreading from original folder\nreading past files for 2021-07-11 00:00:00\nreading from original folder\nreading past files for 2021-07-10 00:00:00\nreading from original folder\nreading past files for 2021-07-09 00:00:00\nreading from original folder\nreading past files for 2021-07-08 00:00:00\nreading from original folder\nreading past files for 2021-07-07 00:00:00\nreading from original folder\n    LAT         LON   FRP  ...   FRP_5_days_ago   FRP_6_days_ago   FRP_7_days_ago\n0  24.5 -126.000000   0.0  ...              0.0              0.0              0.0\n1  24.5 -125.899994   0.0  ...              0.0              0.0              0.0\n2  24.5 -125.800003   0.0  ...              0.0              0.0              0.0\n3  24.5 -125.699997   0.0  ...              0.0              0.0              0.0\n4  24.5 -125.599998   0.0  ...              0.0              0.0              0.0\n[5 rows x 22 columns]\nnearest columns:  Index(['Nearest_1', 'Nearest_2', 'Nearest_3', 'Nearest_4', 'Nearest_5',\n       'Nearest_6', 'Nearest_7', 'Nearest_8', 'Nearest_9', 'Nearest_10',\n       'Nearest_11', 'Nearest_12', 'Nearest_13', 'Nearest_14', 'Nearest_15',\n       'Nearest_16', 'Nearest_17', 'Nearest_18', 'Nearest_19', 'Nearest_20',\n       'Nearest_21', 'Nearest_22', 'Nearest_23', 'Nearest_24'],\n      dtype='object')\ndf.shape:  (156861, 22)\ndf.iloc[100] =  0.0\nnearest_columns length:  24\nnew_df.shape =  (156861, 24)\ndf.shape =  (156861, 22)\nNew time series dataframe:      LAT         LON   FRP  ...  Nearest_22  Nearest_23  Nearest_24\n0  24.5 -126.000000   0.0  ...         0.0         0.0         0.0\n1  24.5 -125.899994   0.0  ...         0.0         0.0         0.0\n2  24.5 -125.800003   0.0  ...         0.0         0.0         0.0\n3  24.5 -125.699997   0.0  ...         0.0         0.0         0.0\n4  24.5 -125.599998   0.0  ...         0.0         0.0         0.0\n[5 rows x 46 columns]\nAll slurm job for fc_model_data_preprocess_slurm_generated.sh finishes.\n",
  "history_begin_time" : 1705681627130,
  "history_end_time" : 1705681878844,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "bf9oNBIr9lBg",
  "history_input" : "#!/bin/bash\n# This file is dedicated to prepare the testing data. \n\n# 1) we need a complete rewrite of this process.\n# 2) separate the training data preparation and testing data preparation.\n# 3) All the share functions should go to the util process. \n\necho \"start to run test_data_slurm_generated.sh\"\npwd\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"fc_model_data_preprocess_slurm_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J fc_model_data_preprocessing       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 12               # Number of CPUs per task (threads)\n#SBATCH --mem=50G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-01:00         # Runtime in D-HH:MM format\n## Slurm can send you updates via email\n#SBATCH --mail-type=FAIL  # BEGIN,END,FAIL         # ALL,NONE,BEGIN,END,FAIL,REQUEUE,..\n#SBATCH --mail-user=zsun@gmu.edu     # Put your GMU email address here\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython << INNER_EOF\n\n\n# Step 1: read and prepare the txt files by yunyao\n\nimport os\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n\n# Folder path containing the text files\nfolder_path = '/groups/ESS3/yli74/data/AI_Emis/firedata'  # The folder yunyao provided with two years of txt files\nmy_file_path = \"/groups/ESS3/zsun/firecasting/data/others/\"\ngrid_to_window_mapper_csv = f\"{my_file_path}/grid_cell_nearest_neight_mapper.csv\"\ntraining_data_folder = \"/groups/ESS3/zsun/firecasting/data/train/\"\n\nstart_date = \"20200107\"\nend_date = \"20211231\"\n\n\ndef read_original_txt_files(datestr):\n  # time range: 2020-01-01 to 2021-12-31\n  # Specify chunk size\n  #chunk_size = 1000\n  #row_limit = 1000\n\n  # Initialize an empty DataFrame\n  df_list = []\n  #total_rows = 0\n\n  # Traverse through files in the folder\n  # firedata_20201208.txt\n  file_path = os.path.join(folder_path, f\"firedata_{datestr}.txt\")\n  file_df = pd.read_csv(file_path)  # Adjust separator if needed\n  #for chunk in chunk_generator:\n  df_list.append(file_df)\n  # Concatenate all chunks into a single DataFrame\n  final_df = pd.concat(df_list, ignore_index=True)\n  \n  \n  # Display the DataFrame\n  #print(final_df)\n  return final_df\n\ndef read_txt_from_predicted_folder(target_datestr, current_prediction_output_folder):\n  # time range: 2020-01-01 to 2021-12-31\n  # Specify chunk size\n  #chunk_size = 1000\n  #row_limit = 1000\n\n  # Initialize an empty DataFrame\n#   df_list = []\n  #total_rows = 0\n\n  # Traverse through files in the folder\n  \n  # firedata_20201208.txt\n  file_path = os.path.join(current_prediction_output_folder, f\"firedata_{target_datestr}_predicted.txt\")\n  if not os.path.exists(file_path):\n    raise ValueError(f\"File {file_path} does not exist.\")\n  \n  #print(f\"Reading data from file : {file_path}\")\n  file_df = pd.read_csv(file_path)  # Adjust separator if needed\n  #for chunk in chunk_generator:\n#   df_list.append(file_df)\n  #total_rows += len(file_df)\n\n  #if total_rows >= row_limit:\n  #    break  # Stop reading files if row limit is reached\n  \n  \n  # Concatenate all chunks into a single DataFrame\n  #final_df = pd.concat(df_list, ignore_index=True)\n  final_df = file_df\n  print(\"current final_df head: \", final_df.head())\n  print(\"renaming Predicted_FRP to FRP\")\n  final_df[' FRP'] = final_df['Predicted_FRP']\n  # Remove the original column 'A'\n  print(\"remove the current predicted_frp\")\n  final_df.drop(columns=['Predicted_FRP'], inplace=True)\n\n\n  # Display the DataFrame\n  #print(final_df)\n  return final_df\n\n\ndef get_one_day_time_series_for_2_weeks_testing_data(\n  target_day, \n  current_start_day, \n  current_prediction_output_folder):\n  # read the txt from original folder if the target date is within the 7 days before the current_start_day. `current_start_day` is actually the target day where the prediction begins. \n  # read the txt from the predicted folder if the target date is equal or after the current start day. \n  if current_start_day == None or current_prediction_output_folder == None:\n    return get_one_day_time_series_training_data(target_day)\n  else:\n    # get grid to window mapper csv\n    grid_to_window_mapper_df = pd.read_csv(grid_to_window_mapper_csv)\n    print(grid_to_window_mapper_df.columns)\n    target_dt = datetime.strptime(target_day, '%Y%m%d')\n    current_start_dt = datetime.strptime(current_start_day, '%Y%m%d')\n    \n    # always read from original folder for current target day. there is no file for current target day in the predicted folder.\n    print(f\"Read from original folder for current date: {target_day}\")\n    df = read_original_txt_files(target_day)\n    # go back 7 days to get all the history FRP and attach to the df with matched coordinates\n    # \n    print(f\"current_start_dt is: {current_start_dt}\")\n    for i in range(7):\n      past_dt = target_dt - timedelta(days=i+1)\n      print(f\"reading past files for {past_dt}\")\n      if past_dt >= current_start_dt and past_dt < target_dt:\n        print(f\"reading from predicted folder\")\n        past_df = read_txt_from_predicted_folder(past_dt.strftime('%Y%m%d'), current_prediction_output_folder)\n      else:\n        print(f\"reading from original folder\")\n        past_df = read_original_txt_files(past_dt.strftime('%Y%m%d'))\n      column_to_append = past_df[\" FRP\"]\n      df[f' FRP_{i+1}_days_ago'] = column_to_append\n\n    # here missed the neighbor cells\n    print(df.head())\n  \n    grid_to_window_mapper_df.set_index(['LAT', ' LON'], inplace=True)\n\n    nearest_columns = grid_to_window_mapper_df.columns\n    print(\"nearest columns: \", nearest_columns)\n    print(\"df.shape: \", df.shape)\n    print(\"df.iloc[100] = \", df.iloc[100][\" FRP_1_days_ago\"])\n\n    original_df = df\n\n    def add_window_grid_cells(row):\n      result = grid_to_window_mapper_df.loc[row['LAT'], row[' LON']]\n      values = []\n      for column in nearest_columns:\n          #print(\"column = \", column)\n          nearest_index = result[column]\n          #print(\"nearest_index = \", nearest_index)\n          # for all the nearest grid cells, we will use yesterday (-1 day ago) value to fill. So all the neighbor grid cells' history will be used to inference the target day's current grid cell's FRP.\n          values.append(original_df.iloc[nearest_index][\" FRP_1_days_ago\"])\n      if len(values) != 24:\n        raise ValueError(\"The nearest values are not 24.\")\n      return pd.Series(values)\n\n  #   #dropped_df = grid_to_window_mapper_df.drop([\"LAT\", \"LON\"], axis=1)\n  #   print(\"new columns: \", grid_to_window_mapper_df.columns)\n  #   print(new_df.describe())\n    print(\"nearest_columns length: \", len(nearest_columns))\n    new_df = df.apply(add_window_grid_cells, axis=1)\n    print(\"new_df.shape = \", new_df.shape)\n    print(\"df.shape = \", df.shape)\n    df[nearest_columns] = new_df\n\n    print(\"New time series dataframe: \", df.head())\n      \n      \n    #print(\"New time series dataframe: \", df.head())\n    return df\n\ndef prepare_testing_data_for_2_weeks_forecasting(target_date, current_start_day, current_prediction_output_folder):\n  \"\"\"\n  Prepare testing data for a 2-week forecasting model.\n\n  Parameters:\n    - target_date (str): The target date for forecasting.\n    - current_start_day (str): The current start day for fetching time series data.\n    - current_prediction_output_folder (str): The folder path for the prediction output.\n\n  Returns:\n    - X (pd.DataFrame): Features DataFrame for model input.\n    - y (pd.Series): Target Series for model output (prediction).\n\n  Assumes the existence of a function get_one_day_time_series_for_2_weeks_testing_data(target_date, current_start_day, current_prediction_output_folder)\n    to fetch time series data for the given target date and start day.\n  \"\"\"\n  # Assuming 'target' is the column to predict\n  target_col = ' FRP'\n  original_df = get_one_day_time_series_for_2_weeks_testing_data(target_date, current_start_day, current_prediction_output_folder)\n  df = original_df\n\n  #print(\"Lag/Shift the data for previous days' information\")\n  num_previous_days = 7  # Adjust the number of previous days to consider\n\n  # Drop rows with NaN values from the shifted columns\n  df_filled = df.fillna(-9999)\n\n  # Define features and target\n  X = df.drop([target_col], axis=1)\n  y = df[target_col]\n  return X, y\n  \n\n\n# target column is current day's FRP, previous days' FRP and all the other columns are inputs\n\n#read_original_txt_files()\n\nif __name__ == \"__main__\":\n  #training_end_date = \"20200715\"\n  #prepare_training_data(training_end_date)\n  output_folder_full_path = f'/groups/ESS3/zsun/firecasting/data/output/test_if_predicted_frp_used/20210718/'\n  prepare_testing_data_for_2_weeks_forecasting(\"20210714\", \"20210714\", output_folder_full_path)\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nwhile true; do\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName%50,State,ExitCode,MaxRSS,Start,End -j $job_id\nfind /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\n#cat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n\n",
  "history_output" : "start to run test_data_slurm_generated.sh\n/home/zsun/gw-workspace/WopCs1wq0nu7\nwrite the slurm script into fc_model_creation_train_slurm_generated.sh\nsbatch fc_model_creation_train_slurm_generated.sh\njob_id=1415555\nJob 1415555 has finished with state: JobState=FAILED\n\nStream closed",
  "history_begin_time" : 1705095501055,
  "history_end_time" : 1705097652142,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "kmc0fGKTltMc",
  "history_input" : "#!/bin/bash\n# This file is dedicated to prepare the testing data. \n\n# 1) we need a complete rewrite of this process.\n# 2) separate the training data preparation and testing data preparation.\n# 3) All the share functions should go to the util process. \n\necho \"start to run test_data_slurm_generated.sh\"\npwd\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"fc_model_data_preprocess_slurm_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J fc_model_data_preprocessing       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 12               # Number of CPUs per task (threads)\n#SBATCH --mem=50G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-01:00         # Runtime in D-HH:MM format\n## Slurm can send you updates via email\n#SBATCH --mail-type=FAIL  # BEGIN,END,FAIL         # ALL,NONE,BEGIN,END,FAIL,REQUEUE,..\n#SBATCH --mail-user=zsun@gmu.edu     # Put your GMU email address here\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython << INNER_EOF\n\n\n# Step 1: read and prepare the txt files by yunyao\n\nimport os\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n\n# Folder path containing the text files\nfolder_path = '/groups/ESS3/yli74/data/AI_Emis/firedata'  # The folder yunyao provided with two years of txt files\nmy_file_path = \"/groups/ESS3/zsun/firecasting/data/others/\"\ngrid_to_window_mapper_csv = f\"{my_file_path}/grid_cell_nearest_neight_mapper.csv\"\ntraining_data_folder = \"/groups/ESS3/zsun/firecasting/data/train/\"\n\nstart_date = \"20200107\"\nend_date = \"20211231\"\n\n\ndef read_original_txt_files(datestr):\n  # time range: 2020-01-01 to 2021-12-31\n  # Specify chunk size\n  #chunk_size = 1000\n  #row_limit = 1000\n\n  # Initialize an empty DataFrame\n  df_list = []\n  #total_rows = 0\n\n  # Traverse through files in the folder\n  # firedata_20201208.txt\n  file_path = os.path.join(folder_path, f\"firedata_{datestr}.txt\")\n  file_df = pd.read_csv(file_path)  # Adjust separator if needed\n  #for chunk in chunk_generator:\n  df_list.append(file_df)\n  # Concatenate all chunks into a single DataFrame\n  final_df = pd.concat(df_list, ignore_index=True)\n  \n  \n  # Display the DataFrame\n  #print(final_df)\n  return final_df\n\ndef read_txt_from_predicted_folder(target_datestr, current_prediction_output_folder):\n  # time range: 2020-01-01 to 2021-12-31\n  # Specify chunk size\n  #chunk_size = 1000\n  #row_limit = 1000\n\n  # Initialize an empty DataFrame\n#   df_list = []\n  #total_rows = 0\n\n  # Traverse through files in the folder\n  \n  # firedata_20201208.txt\n  file_path = os.path.join(current_prediction_output_folder, f\"firedata_{target_datestr}_predicted.txt\")\n  if not os.path.exists(file_path):\n    raise ValueError(f\"File {file_path} does not exist.\")\n  \n  #print(f\"Reading data from file : {file_path}\")\n  file_df = pd.read_csv(file_path)  # Adjust separator if needed\n  #for chunk in chunk_generator:\n#   df_list.append(file_df)\n  #total_rows += len(file_df)\n\n  #if total_rows >= row_limit:\n  #    break  # Stop reading files if row limit is reached\n  \n  \n  # Concatenate all chunks into a single DataFrame\n  #final_df = pd.concat(df_list, ignore_index=True)\n  final_df = file_df\n  print(\"current final_df head: \", final_df.head())\n  print(\"renaming Predicted_FRP to FRP\")\n  final_df[' FRP'] = final_df['Predicted_FRP']\n  # Remove the original column 'A'\n  print(\"remove the current predicted_frp\")\n  final_df.drop(columns=['Predicted_FRP'], inplace=True)\n\n\n  # Display the DataFrame\n  #print(final_df)\n  return final_df\n\n\ndef get_one_day_time_series_for_2_weeks_testing_data(\n  target_day, \n  current_start_day, \n  current_prediction_output_folder):\n  # read the txt from original folder if the target date is within the 7 days before the current_start_day. `current_start_day` is actually the target day where the prediction begins. \n  # read the txt from the predicted folder if the target date is equal or after the current start day. \n  if current_start_day == None or current_prediction_output_folder == None:\n    return get_one_day_time_series_training_data(target_day)\n  else:\n    # get grid to window mapper csv\n    grid_to_window_mapper_df = pd.read_csv(grid_to_window_mapper_csv)\n    print(grid_to_window_mapper_df.columns)\n    target_dt = datetime.strptime(target_day, '%Y%m%d')\n    current_start_dt = datetime.strptime(current_start_day, '%Y%m%d')\n    \n    # always read from original folder for current target day. there is no file for current target day in the predicted folder.\n    print(f\"Read from original folder for current date: {target_day}\")\n    df = read_original_txt_files(target_day)\n    # go back 7 days to get all the history FRP and attach to the df with matched coordinates\n    # \n    print(f\"current_start_dt is: {current_start_dt}\")\n    for i in range(7):\n      past_dt = target_dt - timedelta(days=i+1)\n      print(f\"reading past files for {past_dt}\")\n      if past_dt >= current_start_dt and past_dt < target_dt:\n        print(f\"reading from predicted folder\")\n        past_df = read_txt_from_predicted_folder(past_dt.strftime('%Y%m%d'), current_prediction_output_folder)\n      else:\n        print(f\"reading from original folder\")\n        past_df = read_original_txt_files(past_dt.strftime('%Y%m%d'))\n      column_to_append = past_df[\" FRP\"]\n      df[f' FRP_{i+1}_days_ago'] = column_to_append\n\n    # here missed the neighbor cells\n    print(df.head())\n  \n    grid_to_window_mapper_df.set_index(['LAT', ' LON'], inplace=True)\n\n    nearest_columns = grid_to_window_mapper_df.columns\n    print(\"nearest columns: \", nearest_columns)\n    print(\"df.shape: \", df.shape)\n    print(\"df.iloc[100] = \", df.iloc[100][\" FRP_1_days_ago\"])\n\n    original_df = df\n\n    def add_window_grid_cells(row):\n      result = grid_to_window_mapper_df.loc[row['LAT'], row[' LON']]\n      values = []\n      for column in nearest_columns:\n          #print(\"column = \", column)\n          nearest_index = result[column]\n          #print(\"nearest_index = \", nearest_index)\n          # for all the nearest grid cells, we will use yesterday (-1 day ago) value to fill. So all the neighbor grid cells' history will be used to inference the target day's current grid cell's FRP.\n          values.append(original_df.iloc[nearest_index][\" FRP_1_days_ago\"])\n      if len(values) != 24:\n        raise ValueError(\"The nearest values are not 24.\")\n      return pd.Series(values)\n\n  #   #dropped_df = grid_to_window_mapper_df.drop([\"LAT\", \"LON\"], axis=1)\n  #   print(\"new columns: \", grid_to_window_mapper_df.columns)\n  #   print(new_df.describe())\n    print(\"nearest_columns length: \", len(nearest_columns))\n    new_df = df.apply(add_window_grid_cells, axis=1)\n    print(\"new_df.shape = \", new_df.shape)\n    print(\"df.shape = \", df.shape)\n    df[nearest_columns] = new_df\n\n    print(\"New time series dataframe: \", df.head())\n      \n      \n    #print(\"New time series dataframe: \", df.head())\n    return df\n\ndef prepare_testing_data_for_2_weeks_forecasting(target_date, current_start_day, current_prediction_output_folder):\n  \"\"\"\n  Prepare testing data for a 2-week forecasting model.\n\n  Parameters:\n    - target_date (str): The target date for forecasting.\n    - current_start_day (str): The current start day for fetching time series data.\n    - current_prediction_output_folder (str): The folder path for the prediction output.\n\n  Returns:\n    - X (pd.DataFrame): Features DataFrame for model input.\n    - y (pd.Series): Target Series for model output (prediction).\n\n  Assumes the existence of a function get_one_day_time_series_for_2_weeks_testing_data(target_date, current_start_day, current_prediction_output_folder)\n    to fetch time series data for the given target date and start day.\n  \"\"\"\n  # Assuming 'target' is the column to predict\n  target_col = ' FRP'\n  original_df = get_one_day_time_series_for_2_weeks_testing_data(target_date, current_start_day, current_prediction_output_folder)\n  df = original_df\n\n  #print(\"Lag/Shift the data for previous days' information\")\n  num_previous_days = 7  # Adjust the number of previous days to consider\n\n  # Drop rows with NaN values from the shifted columns\n  df_filled = df.fillna(-9999)\n\n  # Define features and target\n  X = df.drop([target_col], axis=1)\n  y = df[target_col]\n  return X, y\n  \n\n\n# target column is current day's FRP, previous days' FRP and all the other columns are inputs\n\n#read_original_txt_files()\n\nif __name__ == \"__main__\":\n  #training_end_date = \"20200715\"\n  #prepare_training_data(training_end_date)\n  output_folder_full_path = f'/groups/ESS3/zsun/firecasting/data/output/test_if_predicted_frp_used/20210718/'\n  prepare_testing_data_for_2_weeks_forecasting(\"20210715\", \"20210714\", output_folder_full_path)\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nwhile true; do\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName%50,State,ExitCode,MaxRSS,Start,End -j $job_id\nfind /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\n#cat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n\n",
  "history_output" : "start to run test_data_slurm_generated.sh\n/home/zsun/gw-workspace/kmc0fGKTltMc\nwrite the slurm script into fc_model_data_preprocess_slurm_generated.sh\n/home/zsun/gw-workspace/kmc0fGKTltMc/gw-hd31SON6toc7MsQzG5ISLVbGxX-kmc0fGKTltMc.sh: line 14: current_start_day: command not found\nsbatch fc_model_data_preprocess_slurm_generated.sh\njob_id=1415568\nJob 1415568 has finished with state: JobState=FAILED\nSlurm job (1415568) has finished.\nPrint the job's output logs\nJobID                                                   JobName      State ExitCode     MaxRSS               Start                 End \n------------ -------------------------------------------------- ---------- -------- ---------- ------------------- ------------------- \n1415568                             fc_model_data_preprocessing     FAILED      1:0            2024-01-12T16:36:31 2024-01-12T16:37:05 \n1415568.bat+                                              batch     FAILED      1:0          0 2024-01-12T16:36:31 2024-01-12T16:37:05 \n1415568.ext+                                             extern  COMPLETED      0:0          0 2024-01-12T16:36:31 2024-01-12T16:37:05 \nIndex(['LAT', ' LON', 'Nearest_1', 'Nearest_2', 'Nearest_3', 'Nearest_4',\n       'Nearest_5', 'Nearest_6', 'Nearest_7', 'Nearest_8', 'Nearest_9',\n       'Nearest_10', 'Nearest_11', 'Nearest_12', 'Nearest_13', 'Nearest_14',\n       'Nearest_15', 'Nearest_16', 'Nearest_17', 'Nearest_18', 'Nearest_19',\n       'Nearest_20', 'Nearest_21', 'Nearest_22', 'Nearest_23', 'Nearest_24'],\n      dtype='object')\nRead from original folder for current date: 20210715\ncurrent_start_dt is: 2021-07-14 00:00:00\nreading past files for 2021-07-14 00:00:00\nreading from predicted folder\nTraceback (most recent call last):\n  File \"<stdin>\", line 202, in <module>\n  File \"<stdin>\", line 178, in prepare_testing_data_for_2_weeks_forecasting\n  File \"<stdin>\", line 113, in get_one_day_time_series_for_2_weeks_testing_data\n  File \"<stdin>\", line 59, in read_txt_from_predicted_folder\nValueError: File /groups/ESS3/zsun/firecasting/data/output/test_if_predicted_frp_used/20210718/firedata_20210714_predicted.txt does not exist.\nAll slurm job for fc_model_data_preprocess_slurm_generated.sh finishes.\n",
  "history_begin_time" : 1705095390258,
  "history_end_time" : 1705095431490,
  "history_notes" : null,
  "history_process" : "uhet3k",
  "host_id" : null,
  "indicator" : "Done"
},]
