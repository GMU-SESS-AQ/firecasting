[{
  "id" : "1jepv8",
  "name" : "fc_train_data_preprocess",
  "description" : null,
  "code" : "# Step 1: read and prepare the txt files by yunyao\n\nimport os\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom data_preparation_utils import create_grid_to_window_mapper\n\n# Folder path containing the text files\nfolder_path = '/groups/ESS3/yli74/data/AI_Emis/firedata'  # The folder yunyao provided with two years of txt files\nmy_file_path = \"/groups/ESS3/zsun/firecasting/data/others/\"\ngrid_to_window_mapper_csv = f\"{my_file_path}/grid_cell_nearest_neight_mapper.csv\"\ntraining_data_folder = \"/groups/ESS3/zsun/firecasting/data/train/\"\n\nstart_date = \"20200107\"\nend_date = \"20211231\"\n\n# Define the columns to check for zero values\ncolumns_to_check = [' FRP_1_days_ago', ' FRP_2_days_ago',\n\n' FRP_3_days_ago', ' FRP_4_days_ago', ' FRP_5_days_ago',\n\n' FRP_6_days_ago', ' FRP_7_days_ago', 'Nearest_1', 'Nearest_2',\n\n'Nearest_3', 'Nearest_4', 'Nearest_5', 'Nearest_6', 'Nearest_7',\n\n'Nearest_8', 'Nearest_9', 'Nearest_10', 'Nearest_11', 'Nearest_12',\n\n'Nearest_13', 'Nearest_14', 'Nearest_15', 'Nearest_16', 'Nearest_17',\n\n'Nearest_18', 'Nearest_19', 'Nearest_20', 'Nearest_21', 'Nearest_22',\n\n'Nearest_23', 'Nearest_24', ' FRP']\n\n\ndef read_original_txt_files(datestr):\n  # time range: 2020-01-01 to 2021-12-31\n  # Specify chunk size\n  #chunk_size = 1000\n  #row_limit = 1000\n\n  # Initialize an empty DataFrame\n  df_list = []\n  #total_rows = 0\n\n  # Traverse through files in the folder\n  # firedata_20201208.txt\n  file_path = os.path.join(folder_path, f\"firedata_{datestr}.txt\")\n  file_df = pd.read_csv(file_path)  # Adjust separator if needed\n  #for chunk in chunk_generator:\n  df_list.append(file_df)\n  # Concatenate all chunks into a single DataFrame\n  final_df = pd.concat(df_list, ignore_index=True)\n  \n  \n  # Display the DataFrame\n  #print(final_df)\n  return final_df\n\ndef get_one_day_time_series_training_data(target_day):\n  # this function is used to get 7 days time series for one day prediction\n  # From now on, `target_day` will be Day_0. \n  # So remember to change all the `Dayx` columents to `FRP_{i+1}_days_ago` \n  # to eliminate the confusion. \n  print(\"preparing training data for \", target_day)\n  df = read_original_txt_files(target_day)\n  # go back 7 days to get all the history FRP and attach to the df with matched coordinates\n  \n  # get grid to window mapper csv\n  grid_to_window_mapper_df = pd.read_csv(grid_to_window_mapper_csv)\n  print(grid_to_window_mapper_df.columns)\n  \n  target_dt = datetime.strptime(target_day, '%Y%m%d')\n  for i in range(7):\n    past_dt = target_dt - timedelta(days=i+1)\n    print(\"preparing data for past date\", past_dt.strftime('%Y%m%d'))\n    past_df = read_original_txt_files(past_dt.strftime('%Y%m%d'))\n    column_to_append = past_df[\" FRP\"]\n    df[f' FRP_{i+1}_days_ago'] = column_to_append\n    \n  print(df.head())\n  \n  grid_to_window_mapper_df.set_index(['LAT', ' LON'], inplace=True)\n  \n  nearest_columns = grid_to_window_mapper_df.columns\n  print(\"nearest columns: \", nearest_columns)\n  print(\"df.shape: \", df.shape)\n  print(\"df.iloc[100] = \", df.iloc[100][\" FRP_1_days_ago\"])\n  \n  original_df = df\n  \n  def add_window_grid_cells(row):\n    result = grid_to_window_mapper_df.loc[row['LAT'], row[' LON']]\n    values = []\n    for column in nearest_columns:\n        #print(\"column = \", column)\n        nearest_index = result[column]\n        #print(\"nearest_index = \", nearest_index)\n        # for all the nearest grid cells, we will use yesterday (-1 day ago) value to fill. So all the neighbor grid cells' history will be used to inference the target day's current grid cell's FRP.\n        values.append(original_df.iloc[nearest_index][\" FRP_1_days_ago\"])\n    if len(values) != 24:\n      raise ValueError(\"The nearest values are not 24.\")\n    return pd.Series(values)\n  \n#   #dropped_df = grid_to_window_mapper_df.drop([\"LAT\", \"LON\"], axis=1)\n#   print(\"new columns: \", grid_to_window_mapper_df.columns)\n#   print(new_df.describe())\n  print(\"nearest_columns length: \", len(nearest_columns))\n  new_df = df.apply(add_window_grid_cells, axis=1)\n  print(\"new_df.shape = \", new_df.shape)\n  print(\"df.shape = \", df.shape)\n  df[nearest_columns] = new_df\n\n  print(\"New time series dataframe: \", df.head())\n  return df\n\n  \ndef create_training_time_series_dataframe(start_date, end_date):\n  start_dt = datetime.strptime(start_date, '%Y%m%d')\n  end_dt = datetime.strptime(end_date, '%Y%m%d')\n  \n  # Traverse each day and print\n  current_dt = start_dt\n  \n  while current_dt <= end_dt:\n    print(current_dt.strftime('%Y%m%d'))\n    current_dt += timedelta(days=1)\n    \n    \n    break\n    \n  return \n\ndef prepare_training_data(target_date, training_data_folder=training_data_folder):\n  # Assuming 'target' is the column to predict\n  create_grid_to_window_mapper()\n  \n  if not os.path.exists(training_data_folder):\n    os.makedirs(training_data_folder)\n    print(f\"Folder created: {training_data_folder}\")\n  else:\n    print(f\"Folder already exists: {training_data_folder}\")\n  \n  target_col = ' FRP'\n  \n  train_file_path = f\"{training_data_folder}/{target_date}_time_series_with_window.csv\"\n  \n  if os.path.exists(train_file_path):\n    print(f\"File {train_file_path} exists\")\n    existing_df = pd.read_csv(train_file_path)\n    X = existing_df.drop([target_col, 'LAT', ' LON'], axis=1)\n    y = existing_df[target_col]\n  else:\n    print(\"File does not exist\")\n    original_df = get_one_day_time_series_training_data(target_date)\n    df = original_df\n    \n    print(\"all feature names: \", df.columns)\n\n    #print(\"Lag/Shift the data for previous days' information\")\n    num_previous_days = 7  # Adjust the number of previous days to consider\n\n    # Drop rows with NaN values from the shifted columns\n    df_filled = df.fillna(-9999)\n\n    #print(\"drop rows where the previous day has no fire on that pixel\")\n    # df = df[df[' FRP'] != 0]\n    \n\n    # Drop rows where specified columns are equal to zero\n    #df = df[~(df[columns_to_check] == 0).all(axis=1)]\n    #print(\"we have removed all the rows that have no fire in any of the columns - \", columns_to_check)\n    \n    # Drop rows if the previous day FRP is zero and today's FRP is non-zero\n    # df = df[(df[' FRP'] != 0) & (df[columns_to_check] == 0)]\n    df = df[df[columns_to_check].eq(0).all(axis=1)]\n    \n    df.to_csv(train_file_path, index=False)\n    # Define features and target\n    X = df.drop([target_col, 'LAT', ' LON'], axis=1)\n    y = df[target_col]\n  \n  return X, y\n\n# target column is current day's FRP, previous days' FRP and all the other columns are inputs\n\n#read_original_txt_files()\n\nif __name__ == \"__main__\":\n  # this is today, and we want to use all the meteo data of today and FRP data of day -7 - yesterday to predict today's FRP. \n  training_end_date = \"20200715\" # the last day of the 7 day history\n  prepare_training_data(training_end_date)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "w4lpt8",
  "name" : "fc_model_creation",
  "description" : null,
  "code" : "# create a ML model for wildfire emission forecasting\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score\nfrom sklearn.metrics import mean_squared_log_error, median_absolute_error, max_error\nimport xgboost as xgb\nimport warnings\nimport shutil\nfrom datetime import datetime, timedelta\nfrom fc_train_data_preprocess import training_data_folder\n\nimport pickle\n\nfrom fc_train_data_preprocess import  prepare_training_data\n\n# Suppress the specific warning\nwarnings.filterwarnings(\"ignore\", message=\"DataFrame is highly fragmented\")\n\n# final model path\nmodel_path = \"/groups/ESS3/zsun/firecasting/model/fc_xgb_model_v1_latest.pkl\"\n\n\n\n\ndef train_model(start_date_str, end_date_str, training_data_folder=training_data_folder):\n  # start date and end date define the training period\n  # Convert the date strings to datetime objects\n  start_date = datetime.strptime(start_date_str, \"%Y%m%d\")\n  end_date = datetime.strptime(end_date_str, \"%Y%m%d\")\n\n  # Initialize a list to store the dates\n  dates_between = []\n  \n  # Introduce a threshold for target values\n  threshold = 100\n  \n  # Initialize and train a model (e.g., Linear Regression)\n  model = XGBRegressor(n_estimators=100,\n                       max_depth=8, \n                       #n_jobs = 16,\n                       learning_rate=0.1,\n                       eval_metric=\"rmse\",\n                       eta=0.1,\n                       subsample=0.8,\n                       colsample_bytree=0.8,\n                       min_child_weight=1,\n                       objective='reg:squarederror')\n                       #scale_pos_weight=abs((len(y_train) - y_train.sum()) / y_train.sum()))\n\n  # Iterate through the days between start and end dates\n  current_date = start_date\n  label = 0\n  while current_date <= end_date:\n    dates_between.append(current_date)\n    current_date += timedelta(days=1)\n    date_str = current_date.strftime(\"%Y%m%d\")\n    print(f\"training on {date_str}\")\n    X, y = prepare_training_data(date_str, training_data_folder)\n    print(\"input columns: \", X.columns)\n    y_df = pd.DataFrame(y, columns=['training_frp'])\n    print(y_df)\n    y_df.dropna(inplace=True)\n    print(\"get some statistics: \", y_df[\"training_frp\"].describe())\n    \n    # Define sample weights based on the threshold\n    #sample_weights = np.where(y > threshold, 10.0, 1.0)  # Assign a weight of 2 to values > 100\n    sample_weights = 10 * y\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test, sw_train, sw_test = train_test_split(X, y, sample_weights, test_size=0.2, random_state=42, shuffle=False)\n    \n    if label == 0:\n      model.fit(X_train, y_train, sample_weight=sw_train)\n      \n    else:\n      model.fit(X_train, y_train, xgb_model=model, sample_weight=sw_train)  # this is right incremental learning ? use the model as input to make sure the next training cycle takes all the things learnt in the previous cycles.\n    \n    label += 1\n    \n    if label % 30 == 0:\n      # save the model for every month\n      with open(model_path, 'wb') as model_file:\n        pickle.dump(model, model_file)\n  \n  # Save the model to a file\n  with open(model_path, 'wb') as model_file:\n      pickle.dump(model, model_file)\n      print(f\"The new model is saved to {model_path}\")\n      \n  now = datetime.now()\n  date_time = now.strftime(\"%Y%d%m%H%M%S\")\n  random_model_path = f\"/groups/ESS3/zsun/firecasting//model/fc_xgb_model_v1_{start_date_str}_{end_date_str}_{date_time}.pkl\"\n  # Save the model to a file\n  with open(random_model_path, 'wb') as model_file:\n      pickle.dump(model, model_file)\n      print(f\"The new model is saved to {random_model_path}\")\n\n  # copy a version to the latest file placeholder\n  shutil.copy(random_model_path, model_path)\n  print(f\"a copy of the model is saved to {model_path}\")\n\nif __name__ == \"__main__\":\n  # Define your start and end dates as strings\n  start_date_str = \"20200701\"\n  end_date_str = \"20200730\"\n  train_model(start_date_str, end_date_str)\n  print(\"this is just testing if it works\")",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "521ngk",
  "name" : "fc_test_data_preparation",
  "description" : null,
  "code" : "\n\n# Step 1: read and prepare the txt files by yunyao\n\nimport os\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\nimport dask\nfrom dask import delayed\nimport dask.dataframe as dd\n\n# Folder path containing the text files\nfolder_path = '/groups/ESS3/yli74/data/AI_Emis/firedata'  # The folder yunyao provided with two years of txt files\nmy_file_path = \"/groups/ESS3/zsun/firecasting/data/others/\"\ngrid_to_window_mapper_csv = f\"{my_file_path}/grid_cell_nearest_neight_mapper.csv\"\ntraining_data_folder = \"/groups/ESS3/zsun/firecasting/data/train/\"\n\nstart_date = \"20200107\"\nend_date = \"20211231\"\n\ndef read_original_txt_files(datestr):\n  # time range: 2020-01-01 to 2021-12-31\n  # Specify chunk size\n  #chunk_size = 1000\n  #row_limit = 1000\n\n  # Initialize an empty DataFrame\n  df_list = []\n  #total_rows = 0\n\n  # Traverse through files in the folder\n  # firedata_20201208.txt\n  file_path = os.path.join(folder_path, f\"firedata_{datestr}.txt\")\n  print(f\"Reading {file_path}\")\n  file_df = pd.read_csv(file_path)  # Adjust separator if needed\n  #for chunk in chunk_generator:\n  df_list.append(file_df)\n  # Concatenate all chunks into a single DataFrame\n  final_df = pd.concat(df_list, ignore_index=True)\n  \n  \n  # Display the DataFrame\n  #print(final_df)\n  return final_df\n\ndef read_txt_from_predicted_folder(target_datestr, current_prediction_output_folder):\n  # time range: 2020-01-01 to 2021-12-31\n  # Specify chunk size\n  #chunk_size = 1000\n  #row_limit = 1000\n\n  # Initialize an empty DataFrame\n#   df_list = []\n  #total_rows = 0\n\n  # Traverse through files in the folder\n  \n  # firedata_20201208.txt\n  file_path = os.path.join(current_prediction_output_folder, f\"firedata_{target_datestr}_predicted.txt\")\n  if not os.path.exists(file_path):\n    print(f\"WARNING: File {file_path} does not exist. Using the real FRP instead. This is mostly likely due to the use of dask parallelization which make it impossible to wait for the previous days' prediction results. Need to disable the parallelization in future to make this work.\")\n    return read_original_txt_files(target_datestr)\n  \n  #print(f\"Reading data from file : {file_path}\")\n  file_df = pd.read_csv(file_path)  # Adjust separator if needed\n  #for chunk in chunk_generator:\n#   df_list.append(file_df)\n  #total_rows += len(file_df)\n\n  #if total_rows >= row_limit:\n  #    break  # Stop reading files if row limit is reached\n  \n  \n  # Concatenate all chunks into a single DataFrame\n  #final_df = pd.concat(df_list, ignore_index=True)\n  final_df = file_df\n  print(\"current final_df head: \", final_df.head())\n  print(\"renaming Predicted_FRP to FRP\")\n  final_df[' FRP'] = final_df['Predicted_FRP']\n  # Remove the original column 'A'\n  print(\"remove the current predicted_frp\")\n  final_df.drop(columns=['Predicted_FRP'], inplace=True)\n\n\n  # Display the DataFrame\n  #print(final_df)\n  return final_df\n\n\ndef add_window_grid_cells(row, original_df, grid_to_window_mapper_df):\n    # print(\"add_window_grid_cells grid_to_window_mapper_df.columns = \", grid_to_window_mapper_df.columns)\n    # Implement your logic for adding window grid cells\n    #print(\"current index: \", row['LAT'].astype(str) + \"_\" + row[' LON'].astype(str))\n    # print(\"row values: \", row)\n    result = grid_to_window_mapper_df.loc[row['LAT'], row[' LON']]\n    values = []\n    for column in grid_to_window_mapper_df.columns:\n        nearest_index = result[column]\n        values.append(original_df.iloc[nearest_index][\" FRP_1_days_ago\"])\n    \n    if len(values) != 24:\n        raise ValueError(\"The nearest values are not 24.\")\n    return pd.Series(values)\n\ndef get_one_day_time_series_for_2_weeks_testing_data(target_day, current_start_day, current_prediction_output_folder):\n    if current_start_day == None or current_prediction_output_folder == None:\n        print(\"just get one day time series\")\n        return get_one_day_time_series_training_data(target_day)\n    else:\n        # get grid to window mapper csv\n        grid_to_window_mapper_df = pd.read_csv(grid_to_window_mapper_csv)\n\n        target_dt = datetime.strptime(target_day, '%Y%m%d')\n        current_start_dt = datetime.strptime(current_start_day, '%Y%m%d')\n\n        print(f\"Read from original folder for current date: {target_day}\")\n        df = read_original_txt_files(target_day)\n        \n        # go back 7 days to get all the history FRP and attach to the df with matched coordinates\n        for i in range(7):\n            past_dt = target_dt - timedelta(days=i+1)\n            print(f\"reading past files for {past_dt}\")\n            if past_dt >= current_start_dt and past_dt < target_dt:\n                print(f\"reading from predicted folder\")\n                past_df = read_txt_from_predicted_folder(past_dt.strftime('%Y%m%d'), current_prediction_output_folder)\n            else:\n                print(f\"reading from original folder\")\n                past_df = read_original_txt_files(past_dt.strftime('%Y%m%d'))\n            column_to_append = past_df[\" FRP\"]\n            df[f' FRP_{i+1}_days_ago'] = column_to_append\n\n        original_df = df\n        print(\"original_df.describe\", original_df.describe())\n        \n        # Reset the index before using set_index\n        #grid_to_window_mapper_df = grid_to_window_mapper_df.reset_index()\n        # adding the neighbor cell values of yesterday to the inputs\n        # grid_to_window_mapper_df = grid_to_window_mapper_df.set_index(['LAT', ' LON'])\n        #grid_to_window_mapper_df['Combined_Location'] = grid_to_window_mapper_df['LAT'].astype(str) + '_' + grid_to_window_mapper_df[' LON'].astype(str)\n        grid_to_window_mapper_df = grid_to_window_mapper_df.set_index(['LAT', ' LON'])\n        #grid_to_window_mapper_df.set_index('Combined_Location', inplace=True)\n\n        print(\"original_df columns: \", original_df.columns)\n        #print(\"original_df index: \", original_df.index)\n        #print(\"grid_to_window_mapper_df columns: \", grid_to_window_mapper_df.columns)\n        #print(\"grid_to_window_mapper_df index: \", grid_to_window_mapper_df.index)\n        new_df = original_df.apply(add_window_grid_cells, axis=1, args=(original_df, grid_to_window_mapper_df))\n        # Assuming df is a Dask DataFrame\n        #ddf = dd.from_pandas(original_df, npartitions=5)\n        # Adjust the number of partitions as needed\n        # Use the map function\n        #new_df = ddf.map_partitions(apply_dask_partition, original_df = original_df, grid_to_window_mapper_df = grid_to_window_mapper_df).compute()\n        #new_df = ddf.apply(add_window_grid_cells, original_df = original_df, grid_to_window_mapper_df = grid_to_window_mapper_df, axis=1)\n\n        # Convert back to Pandas DataFrame\n        #new_df = new_df.compute()\n        print(\"new_df.shape = \", new_df.shape)\n        print(\"df.shape = \", df.shape)\n        df[grid_to_window_mapper_df.columns] = new_df\n\n        print(\"New time series dataframe: \", df.head())\n        return df\n\ndef prepare_testing_data_for_2_weeks_forecasting(target_date, current_start_day, current_prediction_output_folder):\n  \"\"\"\n  Prepare testing data for a 2-week forecasting model.\n\n  Parameters:\n    - target_date (str): The target date for forecasting.\n    - current_start_day (str): The current start day for fetching time series data.\n    - current_prediction_output_folder (str): The folder path for the prediction output.\n\n  Returns:\n    - X (pd.DataFrame): Features DataFrame for model input.\n    - y (pd.Series): Target Series for model output (prediction).\n\n  Assumes the existence of a function get_one_day_time_series_for_2_weeks_testing_data(target_date, current_start_day, current_prediction_output_folder)\n    to fetch time series data for the given target date and start day.\n  \"\"\"\n  # Assuming 'target' is the column to predict\n  target_col = ' FRP'\n  original_df = get_one_day_time_series_for_2_weeks_testing_data(target_date, current_start_day, current_prediction_output_folder)\n  df = original_df\n  print(\"Original df is created: \", original_df.shape)\n\n  #print(\"Lag/Shift the data for previous days' information\")\n  num_previous_days = 7  # Adjust the number of previous days to consider\n\n  # Drop rows with NaN values from the shifted columns\n  df_filled = df.fillna(-9999)\n  print(\"Original df filled the na with -9999 \")\n\n  # Define features and target\n  X = df.drop([target_col, 'LAT', ' LON'], axis=1)\n  y = df[target_col]\n  return X, y\n  \n\n\n# target column is current day's FRP, previous days' FRP and all the other columns are inputs\n\n#read_original_txt_files()\n\nif __name__ == \"__main__\":\n  #training_end_date = \"20200715\"\n  #prepare_training_data(training_end_date)\n  output_folder_full_path = f'/groups/ESS3/zsun/firecasting/data/output/test_if_predicted_frp_used/20210718/'\n  prepare_testing_data_for_2_weeks_forecasting(\"20210714\", \"20210714\", output_folder_full_path)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "x8kqk7",
  "name" : "fc_install_dependencies",
  "description" : null,
  "code" : "#!/bin/bash\n# install all the required dependencies by python and other programs\n\n/home/zsun/anaconda3/bin/python -m pip install xgboost pickle5\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "wjq4fr",
  "name" : "fc_model_predict",
  "description" : null,
  "code" : "# create a ML model for wildfire emission forecasting\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score\nfrom sklearn.metrics import mean_squared_log_error, median_absolute_error, max_error\nimport warnings\nfrom datetime import datetime, timedelta\nfrom fc_model_creation import prepare_training_data\n\n\nimport pickle\n\nfrom fc_train_data_preprocess import read_original_txt_files, get_one_day_time_series_training_data\n\n# Suppress the specific warning\nwarnings.filterwarnings(\"ignore\", message=\"DataFrame is highly fragmented\")\n\n#model_path = \"/groups/ESS3/zsun/firecasting/model/fc_xgb_model_v2_one_month_202007.pkl\"\n#model_path = \"/groups/ESS3/zsun/firecasting/model/fc_xgb_model_v1_weighted_one_year_2020.pkl\"\nmodel_path = \"/groups/ESS3/zsun/firecasting/model/fc_xgb_model_v1_weighted_5_days_2020_maxdepth_8_linear_weights_100_slurm_test.pkl\"\noutput_folder_name = \"output_weighted_window_xgboost_2020_year_model\"\n#output_folder_name = \"output_xgboost_202007\"\n\n\ndef predict(start_date_str, end_date_str):\n  start_date = datetime.strptime(start_date_str, \"%Y%m%d\")\n  end_date = datetime.strptime(end_date_str, \"%Y%m%d\")\n\n  # Initialize a list to store the dates\n  dates_between = []\n  \n  # Load the saved model\n  with open(model_path, 'rb') as model_file:\n      loaded_model = pickle.load(model_file)\n\n  # create output folder\n  output_folder_full_path = f'/groups/ESS3/zsun/firecasting/data/output/{output_folder_name}/'\n  if not os.path.exists(output_folder_full_path):\n    os.makedirs(output_folder_full_path)\n      \n  # Iterate through the days between start and end dates\n  current_date = start_date\n  label = 0\n  while current_date <= end_date:\n    dates_between.append(current_date)\n    current_date += timedelta(days=1)\n    date_str = current_date.strftime(\"%Y%m%d\")\n    print(f\"generating prediction for {date_str}\")\n    \n    X, y = prepare_training_data(date_str)\n    # Make predictions\n    y_pred = loaded_model.predict(X)\n    y_pred[y_pred < 0] = 0\n\n    #print(\"y_pred : \", y_pred)\n\n    # merge the input and output into one df\n    #print(\"X_test shape: \", X.shape)\n    y_pred_df = pd.DataFrame(y_pred, columns=[\"Predicted_FRP\"])\n\n    #print(\"y_pred_df shape: \", y_pred_df.shape)\n    #print(\"y_pred_df head: \", y_pred_df.head())\n\n    #merged_df = X.join(y_pred_df)\n    #merged_df = pd.concat([X, y_pred_df], axis=1)\n    merged_df = X\n    merged_df[\"Predicted_FRP\"] = y_pred\n\n    #print(\"merged_df shape: \", merged_df.shape)\n    #print(\"the final merged df is: \", merged_df.head())\n\n    # save the df to a csv for plotting\n    merged_df.to_csv(f'/groups/ESS3/zsun/firecasting/data/output/{output_folder_name}/'\n              f'firedata_{date_str}_predicted.txt',\n              index=False)\n\n    # Calculate metrics\n    y_test = y\n    mae = mean_absolute_error(y_test, y_pred)\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = mean_squared_error(y_test, y_pred, squared=False)\n    r2 = r2_score(y_test, y_pred)\n    explained_var = explained_variance_score(y_test, y_pred)\n    msle = mean_squared_log_error(y_test, y_pred)\n    medae = median_absolute_error(y_test, y_pred)\n    max_err = max_error(y_test, y_pred)\n\n    # Print the metrics\n    lines_to_write = [f\"Mean Absolute Error (MAE): {mae}\",\n                      f\"Mean Squared Error (MSE): {mse}\",\n                      f\"Root Mean Squared Error (RMSE): {rmse}\",\n                      f\"R-squared (R2) Score: {r2}\",\n                      f\"Explained Variance Score: {explained_var}\",\n                      f\"Mean Squared Log Error (MSLE): {msle}\",\n                      f\"Median Absolute Error (MedAE): {medae}\",\n                      f\"Max Error: {max_err}\",\n                      f\"Mean: {y.mean()}\",\n                      f\"Median: {y.median()}\",\n                      f\"Standard Deviation: {y.std()}\",\n                      f\"Minimum: {y.min()}\",\n                      f\"Maximum: {y.max()}\",\n                      f\"Count: {y.count()}\"]\n    \n    print(lines_to_write)\n    # Specify the file path where you want to save the lines\n    metric_file_path = f'/groups/ESS3/zsun/firecasting/data/output/{output_folder_name}/metrics_{date_str}_predicted.txt'\n\n    # Open the file in write mode and write the lines\n    with open(metric_file_path, 'w') as file:\n        for line in lines_to_write:\n            file.write(line + '\\n')  # Add a newline character at the end of each line\n    print(f\"Metrics saved to {metric_file_path}\")\n\n\nstart_date = \"20210714\"\nend_date = \"20210731\"\n\npredict(start_date, end_date)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "l4eb73",
  "name" : "fc_model_predict_2weeks",
  "description" : null,
  "code" : "\n\"\"\"\nWildfire Emission Forecasting\n\nThis script creates a machine learning model for wildfire emission forecasting and uses it to predict emissions\nfor a period of two weeks. The script loads a pre-trained model, processes input data, makes predictions, \nand calculates various metrics for each day within the two-week period.\n\nThe predicted emissions are saved in separate folders for each date, along with corresponding metrics.\n\nUsage:\n    python fc_model_predict_2weeks.py\n\nDependencies:\n    - Python 3.x\n    - pandas\n    - numpy\n    - scikit-learn\n    - xgboost\n    - datetime\n\n\"\"\"\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport dask\nfrom dask import delayed\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score\nfrom sklearn.metrics import mean_squared_log_error, median_absolute_error, max_error\nimport warnings\nfrom datetime import datetime, timedelta\nimport pickle\nfrom fc_test_data_preparation import prepare_testing_data_for_2_weeks_forecasting\nfrom fc_train_data_preprocess import columns_to_check\nimport sys\nfrom fc_model_creation import model_path\n         \nlog_file = open('/scratch/zsun/output.log', 'w')\nsys.stdout = log_file\nprint(\"This will be written to the file\")\nsys.stdout = sys.__stdout__  # Reset stdout to its default value\n\nforecasting_days = 7\n\n# Suppress the specific warning\nwarnings.filterwarnings(\"ignore\", message=\"DataFrame is highly fragmented\")\n\n#model_path = \"/groups/ESS3/zsun/firecasting/model/fc_xgb_model_v2_one_month_202007.pkl\"\n#model_path = \"/groups/ESS3/zsun/firecasting/model/fc_xgb_model_v1_weighted_two_months_2020_maxdepth_7_slurm.pkl\"\n# model_path=\"/groups/ESS3/zsun/firecasting/model/fc_xgb_model_v1_weighted_one_month_2020_maxdepth_15_slurm.pkl\"\n#model_path=\"/groups/ESS3/zsun/firecasting/model/fc_xgb_model_v1_weighted_7_days_with_window_July2020_weighted_slurm_test.pkl\"\n#model_path = #\"/groups/ESS3/zsun/firecasting/model/fc_xgb_model_v1_weighted_one_year_2020.pkl\"\noutput_folder_name = f\"output_xgboost_window_{forecasting_days}_days_forecasting\"\n# output_folder_name = \"output_xgboost_202007\"\n\ndef remove_element_by_value(input_list, value_to_remove):\n    # Check if the value is in the list\n    if value_to_remove in input_list:\n        # Remove the first occurrence of the value\n        input_list.remove(value_to_remove)\n    return input_list\n\n#@delayed\ndef predict_single_day_in_the_2weeks(single_day_current_date_str, date_str, specific_date_result_folder, loaded_model):\n  \n    print(\"predicting: \", single_day_current_date_str)\n\n    X, y = prepare_testing_data_for_2_weeks_forecasting(\n      single_day_current_date_str,  # the forecasting day for the current starting day\n      date_str,  # starting day\n      specific_date_result_folder)\n    print(f\"X and y are loaded into memory for {single_day_current_date_str}\")\n    # Make predictions\n    print(\"the loaded model is: \", loaded_model)\n    y_pred = loaded_model.predict(X)\n    print(f\"Prediction for {single_day_current_date_str} of start day {date_str} is finished\")\n    y_pred[y_pred < 0] = 0  # if FRP is lower than 5, make it 0\n\n    # merge the input and output into one df\n    y_pred_df = pd.DataFrame(y_pred, columns=[\"Predicted_FRP\"])\n\n    merged_df = X\n    merged_df[\"Predicted_FRP\"] = y_pred\n    \n    # first remove all the rows with no fire nearby or in the past\n    new_columns_to_check = remove_element_by_value(columns_to_check, \" FRP\")\n    print(\"new columns to check: \", new_columns_to_check)\n    print(\"current merged_df columns: \", merged_df.columns)\n    # merged_df = merged_df[merged_df[new_columns_to_check].eq(0).all(axis=1)]\n\n    # Define a custom function to update 'Predicted_FRP' column\n    def update_FRP(row):\n      # if the pixel is in ocean or predicted value is negative, FRP is 0\n      if row[' VPD'] < 0 or row[' HT'] < 0 or row['Predicted_FRP'] < 0:\n        return 0\n      \n      return row['Predicted_FRP']\n\n    # Apply the custom function to update 'FRP' column\n    merged_df['Predicted_FRP'] = merged_df.apply(update_FRP, axis=1)\n    \n    merged_df.loc[merged_df[new_columns_to_check].eq(0).all(axis=1), \"Predicted_FRP\"] = 0\n\n    predict_file = f'{specific_date_result_folder}/firedata_{single_day_current_date_str}_predicted.txt'\n    # save the df to a csv for plotting\n    merged_df.to_csv(predict_file, index=False)\n    print(f\"Prediction results are saved to {predict_file}\")\n\n    # Calculate metrics\n    y_test = y\n    y_pred = merged_df['Predicted_FRP']\n    mae = mean_absolute_error(y_test, y_pred)\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = mean_squared_error(y_test, y_pred, squared=False)\n    r2 = r2_score(y_test, y_pred)\n    explained_var = explained_variance_score(y_test, y_pred)\n    #msle = mean_squared_log_error(y_test, y_pred)\n    medae = median_absolute_error(y_test, y_pred)\n    max_err = max_error(y_test, y_pred)\n\n    # Print the metrics\n    lines_to_write = [f\"Mean Absolute Error (MAE): {mae}\",\n                      f\"Mean Squared Error (MSE): {mse}\",\n                      f\"Root Mean Squared Error (RMSE): {rmse}\",\n                      f\"R-squared (R2) Score: {r2}\",\n                      f\"Explained Variance Score: {explained_var}\",\n                      #f\"Mean Squared Log Error (MSLE): {msle}\",\n                      f\"Median Absolute Error (MedAE): {medae}\",\n                      f\"Max Error: {max_err}\",\n                      f\"Mean: {y.mean()}\",\n                      f\"Median: {y.median()}\",\n                      f\"Standard Deviation: {y.std()}\",\n                      f\"Minimum: {y.min()}\",\n                      f\"Maximum: {y.max()}\",\n                      f\"Count: {y.count()}\"]\n\n    print(lines_to_write)\n    # Specify the file path where you want to save the lines\n    metric_file_path = f'{specific_date_result_folder}/metrics_{single_day_current_date_str}_predicted.txt'\n\n    # Open the file in write mode and write the lines\n    with open(metric_file_path, 'w') as file:\n      for line in lines_to_write:\n        file.write(line + '\\n')  # Add a newline character at the end of each line\n    print(f\"Metrics saved to {metric_file_path}\")\n    \n    \n#@delayed\ndef predict_2weeks_for_one_day(date_str, current_date, output_folder_full_path, loaded_model):\n    \n    # create a new folder for this date\n    specific_date_result_folder = f\"{output_folder_full_path}/{date_str}\"\n    if not os.path.exists(specific_date_result_folder):\n      os.makedirs(specific_date_result_folder)\n      print(f\"created folder for specific date: {specific_date_result_folder}\")\n\n    # Start a loop to iterate through dates\n    single_day_current_date = current_date\n    # Calculate the end date (one month later)\n    single_day_predict_end_date = single_day_current_date + timedelta(days=forecasting_days)\n    print(\"single_day_current_date = \", single_day_current_date)\n    print(\"single_day_predict_end_date = \", single_day_predict_end_date)\n\n    parallel_tasks = []\n    while single_day_current_date < single_day_predict_end_date:\n      single_day_current_date_str = single_day_current_date.strftime('%Y%m%d')\n      predict_single_day_in_the_2weeks(single_day_current_date_str, date_str, specific_date_result_folder, loaded_model)\n      #parallel_tasks.append(predict_single_day_in_the_2weeks(single_day_current_date_str, date_str, specific_date_result_folder, loaded_model))\n      single_day_current_date += timedelta(days=1)\n    #print(\"Compute the parallel tasks\")\n    #dask.compute(parallel_tasks)\n\ndef predict_2weeks(start_date_str, end_date_str):\n    \"\"\"\n    Predict wildfire emissions for a two-week period.\n\n    Args:\n        start_date_str (str): The start date in the format 'YYYYMMDD'.\n        end_date_str (str): The end date in the format 'YYYYMMDD'.\n\n    Returns:\n        None\n    \"\"\"\n    print(f\"the model in use is {model_path}\",)\n    \n    start_date = datetime.strptime(start_date_str, \"%Y%m%d\")\n    end_date = datetime.strptime(end_date_str, \"%Y%m%d\")\n\n    # Initialize a list to store the dates\n    dates_between = []\n\n    # Load the saved model\n    loaded_model = None\n    with open(model_path, 'rb') as model_file:\n        loaded_model = pickle.load(model_file)\n\n    # create output folder\n    output_folder_full_path = f'/groups/ESS3/zsun/firecasting/data/output/{output_folder_name}/'\n    if not os.path.exists(output_folder_full_path):\n        os.makedirs(output_folder_full_path)\n\n    # Iterate through the days between start and end dates\n    current_date = start_date\n    label = 0\n    parallel_tasks = []\n    while current_date <= end_date:\n        print(\"current date: \", current_date)\n        dates_between.append(current_date)\n        date_str = current_date.strftime(\"%Y%m%d\")\n        predict_2weeks_for_one_day(date_str, current_date, output_folder_full_path, loaded_model)\n        # parallel_tasks.append(predict_2weeks_for_one_day(date_str, current_date, output_folder_full_path, loaded_model))\n        \n        # increase the date by 1\n        current_date += timedelta(days=1)\n\n    # Compute the parallel tasks\n    # dask.compute(parallel_tasks)\n\nif __name__ == \"__main__\":\n  start_date = \"20210714\"\n  end_date = \"20210715\"\n\n  predict_2weeks(start_date, end_date)\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "pv7d8l",
  "name" : "fc_model_creation_train_slurm",
  "description" : null,
  "code" : "#!/bin/bash\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"fc_model_creation_train_slurm_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J fc_model_creation_train       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 12               # Number of CPUs per task (threads)\n#SBATCH --mem=50G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-03:00         # Runtime in D-HH:MM format\n## Slurm can send you updates via email\n#SBATCH --mail-type=FAIL  # BEGIN,END,FAIL         # ALL,NONE,BEGIN,END,FAIL,REQUEUE,..\n#SBATCH --mail-user=zsun@gmu.edu     # Put your GMU email address here\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython -u << INNER_EOF\n\nfrom fc_model_creation import train_model\n\n# Define your start and end dates as strings\nstart_date_str = \"20200701\"\nend_date_str = \"20200703\"\ntraining_data_folder = \"/groups/ESS3/zsun/firecasting/data/train/\"\ntrain_model(start_date_str, end_date_str, training_data_folder)\nprint(\"all training on {training_data_folder} is done\")\n\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nfile_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\nprevious_content=$(<\"${file_name}\")\nwhile true; do\n    # Capture the current content\n    file_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\n    current_content=$(<\"${file_name}\")\n\n    # Compare current content with previous content\n    diff_result=$(diff <(echo \"$previous_content\") <(echo \"$current_content\"))\n    # Check if there is new content\n    if [ -n \"$diff_result\" ]; then\n        echo \"$diff_result\"\n    fi\n    # Update previous content\n    previous_content=\"$current_content\"\n\n\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\n#find /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n\n\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "xku47i",
  "name" : "plot_results",
  "description" : null,
  "code" : "# traverse the output folder and create a PNG for every day\n# this doesn't use parallelization at all so it will be slow. \nimport os\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\nfrom matplotlib.cm import ScalarMappable\nimport numpy as np\nfrom fc_model_predict_2weeks import output_folder_name\nimport rasterio\nfrom rasterio.transform import from_origin\nfrom rasterio.enums import Resampling\nfrom scipy.interpolate import griddata\n\noutput_folder = f\"/groups/ESS3/zsun/firecasting/data/output/{output_folder_name}/20210714/\"\nsample_lat_lon_csv = \"/groups/ESS3/zsun/firecasting/data/others/sample_lat_lon.csv\"\n\n\ndef save_predicted_frp_to_standard_netcdf(csv_file, sample_lat_lon_df):\n    \"\"\"\n    Get the ML results ready for downstream model inputs\n    \"\"\"\n    \n    pass\n\n\ndef save_predicted_frp_to_geotif(csv_file, sample_lat_lon_df):\n    \"\"\"\n    Get the ML results ready for public download and access\n    \"\"\"\n    # Read CSV into GeoDataFrame\n    df = pd.read_csv(csv_file)\n    if 'LAT' not in df.columns:\n      # Merge 'lat' and 'lon' columns from df2 into df1\n      df[\"LAT\"] = sample_lat_lon_df[\"LAT\"]\n      df[\" LON\"] = sample_lat_lon_df[\" LON\"]\n\n    # Create GeoDataFrame with Point geometries\n    #geometry = [Point(lon, lat) for lon, lat in zip(df[' LON'], df['LAT'])]\n    #gdf = gpd.GeoDataFrame(df, geometry=geometry, crs='EPSG:4326')\n    \n    # Create a GeoDataFrame from the DataFrame\n    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df[\" LON\"], df[\"LAT\"]), crs='EPSG:4326')\n\t\n    # Set up rasterization parameters\n    # Define pixel size and latitude/longitude bounds\n    pixel_size = 0.01  # Adjust as needed\n    min_lat, max_lat = gdf['LAT'].min(), gdf['LAT'].max()\n    min_lon, max_lon = gdf[' LON'].min(), gdf[' LON'].max()\n    \n    print(\"min_lat = \", min_lat)\n    print(\"max_lat = \", max_lat)\n    print(\"min_lon = \", min_lon)\n    print(\"max_lon = \", max_lon)\n\n    # Calculate width and height based on pixel size\n    width = int((max_lon - min_lon) / pixel_size)\n    height = int((max_lat - min_lat) / pixel_size)\n    \n    print(\"width = \", width)\n    print(\"height = \", height)\n    \n    # Create a regular grid using numpy\n    xi = np.linspace(min_lon, max_lon, width)\n    yi = np.linspace(max_lat, min_lat, height)\n    xi, yi = np.meshgrid(xi, yi)\n    \n    print(\"xi = \", xi)\n    print(\"yi = \", yi)\n\n    # Interpolate data onto the regular grid\n    zi = griddata((gdf[' LON'], gdf['LAT']), gdf['Predicted_FRP'], (xi, yi), method='linear')\n\n    # Create a GeoTIFF from the interpolated data\n    with rasterio.open(f'{csv_file}_output.tif', \n                       'w', \n                       driver='GTiff', \n                       height=height, \n                       width=width,\n                       count=1, \n                       dtype='float32', \n                       crs='EPSG:4326',\n                       transform=from_origin(min_lon, \n                                             max_lat, \n                                             pixel_size, \n                                             pixel_size)) as dst:\n        dst.write(zi, 1)\n\n    print(f\"GeoTIFF file created: {csv_file}_output.tif\")\n    \n\ndef plot_png(file_path, sample_lat_lon_df):\n    # Read CSV into a DataFrame\n    df = pd.read_csv(file_path)\n    print(df.head())\n\n    if 'LAT' not in df.columns:\n      # Merge 'lat' and 'lon' columns from df2 into df1\n      df[\"LAT\"] = sample_lat_lon_df[\"LAT\"]\n      df[\" LON\"] = sample_lat_lon_df[\" LON\"]\n\n    real_col_num = len(df.columns) - 2\n    num_rows = int(np.ceil(np.sqrt(real_col_num)))\n    num_cols = int(np.ceil(real_col_num / num_rows))\n\n    # Create a figure and axis objects\n    fig, axs = plt.subplots(num_rows, num_cols, figsize=(28, 24))\n    # Flatten the axs array if it's more than 1D\n    axs = np.array(axs).flatten()\n    \n    for i in range(len(df.columns)):\n        col_name = df.columns[i]\n        \n        if col_name in [\"LAT\", \" LON\"]:\n          continue\n        \n        ax = axs[i]\n        # Create a scatter plot using two columns from the DataFrame\n        cmap = plt.get_cmap('hot')  # You can choose a different colormap\n        sm = ScalarMappable(cmap=cmap)\n        if \"FRP\" in col_name or \"Near\" in col_name:\n          # Define the minimum and maximum values for the color scale\n          min_value = 0  # Set your minimum value here\n          max_value = 50  # Set your maximum value here\n          # Create a color map and a normalization for the color scale\n          norm = Normalize(vmin=min_value, vmax=max_value)\n\n          ax.scatter(\n            df[' LON'], \n            df['LAT'], \n            c=df[col_name], \n            cmap=cmap, \n            s=5, \n            norm=norm,\n            edgecolors='none'\n          )\n          # Create a scalar mappable for the color bar\n          sm = ScalarMappable(cmap=cmap, norm=norm)\n        else:\n          min_value = df[col_name].min()\n          if min_value == -999:\n            min_value = 0\n          \n          max_value = df[col_name].max()\n          #cmap = plt.get_cmap('coolwarm')  # You can choose a different colormap\n          new_norm = Normalize(vmin=min_value, vmax=max_value)\n          sm = ScalarMappable(cmap=cmap, norm=new_norm)\n          ax.scatter(\n            df[' LON'], \n            df['LAT'], \n            c=df[col_name], \n            cmap=cmap, \n            norm=new_norm,\n            s=5,\n            edgecolors='none'\n          )\n          sm.set_array([])  # Set an empty array for the color bar\n\n        # Set the color bar's minimum and maximum values\n        # Add a color bar to the plot\n        color_bar = plt.colorbar(sm, orientation='horizontal', ax=ax)\n\n        # Set the color bar's minimum and maximum values using vmin and vmax\n        color_bar.set_ticks([min_value, max_value])\n        color_bar.set_ticklabels([min_value, max_value])\n\n        ax.set_title(f'{col_name}')\n\n        # Add labels and legend\n        #ax.set_xlabel('Longitude')\n        #ax.set_ylabel('Latitude')\n\n    plt.tight_layout()\n\n    res_png_path = f\"{file_path}.png\"\n    plt.savefig(res_png_path)\n    print(f\"test image is saved at {res_png_path}\")\n    plt.close()\n    \n\ndef plot_images():\n    # List all CSV files in the directory\n    csv_files = [f for f in os.listdir(output_folder) if f.endswith('.txt') and f.startswith('firedata_')]\n    \n    sample_lat_lon_df = pd.read_csv(sample_lat_lon_csv)\n    \n    # Iterate through each CSV file\n    for csv_file in csv_files:\n        # Construct the full file path\n        file_path = os.path.join(output_folder, csv_file)\n        plot_png(file_path, sample_lat_lon_df)\n        save_predicted_frp_to_geotif(file_path, sample_lat_lon_df)\n        save_predicted_frp_to_standard_netcdf(file_path, sample_lat_lon_df)\n        \n\n    print(\"All done\")\n\n    \nif __name__ == \"__main__\":\n    plot_images()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "dp0hiw",
  "name" : "explain_model_results",
  "description" : null,
  "code" : "import os\nimport pickle\nimport matplotlib.pyplot as plt\nfrom fc_model_creation import model_path\n\n# get one prediction and reverse inference the model and get explanation for that prediction\n\n# Explanable AI - SHAP\n\n# target_predict_file = \"/groups/ESS3/zsun/firecasting/data/output/output_xgboost_2020_two_months/20210714/firedata_20210714_predicted.txt\"\n\n#model_path=\"/groups/ESS3/zsun/firecasting/model/fc_xgb_model_v1_weighted_5_days_2020_maxdepth_8_linear_weights_100_slurm_test.pkl\"\n\n# # Load the saved model\n# with open(model_path, 'rb') as model_file:\n#   loaded_model = pickle.load(model_file)\n\n# df = pd.read_csv(target_predict_file)\n  \n# X, y = prepare_testing_data_for_2_weeks_forecasting(single_day_current_date_str, date_str, specific_date_result_folder)\n\n# # Make predictions\n# y_pred = loaded_model.predict(X)\n\n# calculate feature importance - indirect evaluation\n\n\n    \ndef plot_feature_importance():\n  \n    # Load the saved model\n    with open(model_path, 'rb') as model_file:\n      loaded_model = pickle.load(model_file)\n    \n    feature_importances = loaded_model.feature_importances_\n    \n    feature_names = ['LAT', ' LON', ' FWI', ' VPD', ' HT', ' T', ' RH', ' U', ' V', ' P',\n ' RAIN', ' CAPE', ' ST', ' SM', ' FRP_1_days_ago', ' FRP_2_days_ago',\n ' FRP_3_days_ago', ' FRP_4_days_ago', ' FRP_5_days_ago',\n ' FRP_6_days_ago', ' FRP_7_days_ago', 'Nearest_1', 'Nearest_2',\n 'Nearest_3', 'Nearest_4', 'Nearest_5', 'Nearest_6', 'Nearest_7',\n 'Nearest_8', 'Nearest_9', 'Nearest_10', 'Nearest_11', 'Nearest_12',\n 'Nearest_13', 'Nearest_14', 'Nearest_15', 'Nearest_16', 'Nearest_17',\n 'Nearest_18', 'Nearest_19', 'Nearest_20', 'Nearest_21', 'Nearest_22',\n 'Nearest_23', 'Nearest_24']\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(12, 6))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    file_name = os.path.basename(model_path)\n    plt.savefig(f'/groups/ESS3/zsun/firecasting/data/output/importance_summary_plot_{file_name}.png')\n    \n    \nif __name__ == \"__main__\":\n    plot_feature_importance()\n\n# explain why it makes that decision (look into the model itself) - direct evaluation\n\n\n# local explanation \n\n\n# global explanation\n\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "b3cx6j",
  "name" : "upload_prediction_to_website",
  "description" : null,
  "code" : "#!/bin/bash\n\n# move the generated PNG images and metrics to the public website folders\necho \"Copying FRP predicted png files to public server..\"\n#scp -i /home/zsun/.ssh/id_geobrain_no.pem /groups/ESS3/zsun/cmaq/ai_results/evaluation/* zsun@129.174.131.229:/var/www/html/cmaq_site/evaluation/\nrsync -u -e \"ssh -i /home/zsun/.ssh/id_geobrain_no.pem\" -avz /groups/ESS3/zsun/firecasting/data/output/output_xgboost_window_15_days_forecasting/20210714/* zsun@129.174.131.229:/var/www/html/wildfire_site/data/\n\nrsync -u -e \"ssh -i /home/zsun/.ssh/id_geobrain_no.pem\" -avz  /groups/ESS3/zsun/firecasting/data/output/importance_summary_plot_* zsun@129.174.131.229:/var/www/html/wildfire_site/model/\n\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "k84mqm",
  "name" : "fc_train_data_preprocess_slurm",
  "description" : null,
  "code" : "#!/bin/bash\n\n# This file is dedicated to prepare the training data.\n\n# 1) we need a complete rewrite of this process.\n# 2) separate the training data preparation and testing data preparation.\n# 3) All the share functions should go to the util process. \n\necho \"start to run test_data_slurm_generated.sh\"\npwd\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"fc_model_data_preprocess_slurm_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J fc_model_data_preprocessing       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 12               # Number of CPUs per task (threads)\n#SBATCH --mem=50G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-01:00         # Runtime in D-HH:MM format\n## Slurm can send you updates via email\n#SBATCH --mail-type=FAIL  # BEGIN,END,FAIL         # ALL,NONE,BEGIN,END,FAIL,REQUEUE,..\n#SBATCH --mail-user=zsun@gmu.edu     # Put your GMU email address here\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython -u << INNER_EOF\n\nfrom fc_train_data_preprocess import prepare_training_data\n\nif __name__ == \"__main__\":\n  # this is today, and we want to use all the meteo data of today and FRP data of day -7 - yesterday to predict today's FRP. \n  training_end_date = \"20200715\" # the last day of the 7 day history\n  training_data_folder = \"/groups/ESS3/zsun/firecasting/data/train/all_cells/\"\n  prepare_training_data(training_end_date, training_data_folder)\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nfile_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\nprevious_content=$(<\"${file_name}\")\nwhile true; do\n    # Capture the current content\n    file_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\n    current_content=$(<\"${file_name}\")\n\n    # Compare current content with previous content\n    diff_result=$(diff <(echo \"$previous_content\") <(echo \"$current_content\"))\n    # Check if there is new content\n    if [ -n \"$diff_result\" ]; then\n        echo \"$diff_result\"\n    fi\n    # Update previous content\n    previous_content=\"$current_content\"\n\n\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\n#find /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "55wou8",
  "name" : "fc_model_predict_2weeks_slurm",
  "description" : null,
  "code" : "#!/bin/bash\n\necho \"start to run fc_model_predict_2weeks.sh\"\npwd\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"fc_model_predict_2weeks_slurm_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J fc_model_predict_2weeks       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 12               # Number of CPUs per task (threads)\n#SBATCH --mem=50G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-10:00         # Runtime in D-HH:MM format\n## Slurm can send you updates via email\n#SBATCH --mail-type=FAIL  # BEGIN,END,FAIL         # ALL,NONE,BEGIN,END,FAIL,REQUEUE,..\n#SBATCH --mail-user=zsun@gmu.edu     # Put your GMU email address here\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\n# Call the Python script using process substitution\npython -u << INNER_EOF\n\nfrom fc_model_predict_2weeks import predict_2weeks\n\nstart_date = \"20210714\"\nend_date = \"20210714\"\n\npredict_2weeks(start_date, end_date)\n\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nfile_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\nprevious_content=$(cat file_name)\nexit_code=0\nwhile true; do\n    # Capture the current content\\\n    #echo ${job_id}\n    file_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\n    #echo \"file_name=\"$file_name\n    current_content=$(<\"${file_name}\")\n    #echo \"current_content = \"$current_content\n\n    # Compare current content with previous content\n    diff_result=$(diff <(echo \"$previous_content\") <(echo \"$current_content\"))\n    # Check if there is new content\n    if [ -n \"$diff_result\" ]; then\n        # Print the newly added content\n        #echo \"New content added:\"\n        echo \"$diff_result\"\n        #echo \"---------------------\"\n    fi\n    # Update previous content\n    previous_content=\"$current_content\"\n\n\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    #echo \"job_status \"$job_status\n    #if [[ $job_status == \"JobState=COMPLETED\" ]]; then\n    #    break\n    #fi\n    if [[ $job_status == *\"COMPLETED\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    elif [[ $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        exit_code=1\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\n# find /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\n#cat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n\nexit $exit_code\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "uhet3k",
  "name" : "fc_test_data_preprocess_slurm",
  "description" : null,
  "code" : "#!/bin/bash\n# This file is dedicated to prepare the testing data. \n\n# 1) we need a complete rewrite of this process.\n# 2) separate the training data preparation and testing data preparation.\n# 3) All the share functions should go to the util process. \n\necho \"start to run test_data_slurm_generated.sh\"\npwd\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"fc_model_data_preprocess_slurm_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J fc_model_data_preprocessing       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 12               # Number of CPUs per task (threads)\n#SBATCH --mem=50G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-01:00         # Runtime in D-HH:MM format\n## Slurm can send you updates via email\n#SBATCH --mail-type=FAIL  # BEGIN,END,FAIL         # ALL,NONE,BEGIN,END,FAIL,REQUEUE,..\n#SBATCH --mail-user=zsun@gmu.edu     # Put your GMU email address here\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython -u << INNER_EOF\n\nfrom fc_test_data_preparation import prepare_testing_data_for_2_weeks_forecasting\n\nif __name__ == \"__main__\":\n  #training_end_date = \"20200715\"\n  #prepare_training_data(training_end_date)\n  output_folder_full_path = f'/groups/ESS3/zsun/firecasting/data/output/test_if_predicted_frp_used/20210718/'\n  prepare_testing_data_for_2_weeks_forecasting(\"20210714\", \"20210714\", output_folder_full_path)\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nfile_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\nprevious_content=$(<\"${file_name}\")\nwhile true; do\n    # Capture the current content\n    file_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\n    current_content=$(<\"${file_name}\")\n\n    # Compare current content with previous content\n    diff_result=$(diff <(echo \"$previous_content\") <(echo \"$current_content\"))\n    # Check if there is new content\n    if [ -n \"$diff_result\" ]; then\n        echo \"$diff_result\"\n    fi\n    # Update previous content\n    previous_content=\"$current_content\"\n\n\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\n#find /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "b6x5kk",
  "name" : "data_preparation_utils",
  "description" : null,
  "code" : "import os\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# this file contains all the function that are required by both the training data preparation and testing data preparation.\n\n# Folder path containing the text files\nfolder_path = '/groups/ESS3/yli74/data/AI_Emis/firedata'  # The folder yunyao provided with two years of txt files\nmy_file_path = \"/groups/ESS3/zsun/firecasting/data/others/\"\ngrid_to_window_mapper_csv = f\"{my_file_path}/grid_cell_nearest_neight_mapper.csv\"\ntraining_data_folder = \"/groups/ESS3/zsun/firecasting/data/train/\"\n\n\ndef create_grid_to_window_mapper(the_folder_path = folder_path):\n  if os.path.exists(grid_to_window_mapper_csv):\n    print(f\"The file '{grid_to_window_mapper_csv}' exists.\")\n  else:\n    # this function will find the nearest 24 pixels for one pixel\n    # we only start from 2\n    # choose any txt \n    # Replace 'path_to_folder' with the path to your folder containing text files\n    txt_folder_path = f'{the_folder_path}/*.txt'\n    import glob\n    # Get a list of text files in the folder\n    text_files = glob.glob(txt_folder_path)\n\n    # Choose the first text file (you can modify this to select any specific file)\n    file_to_read = text_files[0]\n\n    # Read the chosen text file into a DataFrame\n    df = pd.read_csv(file_to_read)  # Modify delimiter as needed\n    print(df.head())\n    # Convert all values in the DataFrame to numeric\n    df = df.applymap(pd.to_numeric, errors='coerce')\n\n    print(df.columns)\n\n    # Use groupby to get unique pairs of LAT and LON\n    # Use groupby to get unique pairs of LAT and LON\n    unique_pairs = df.groupby(['LAT', ' LON']).size().reset_index(name='Count')\n    unique_pairs_df = unique_pairs[['LAT', ' LON']]\n    print(\"unique_pairs = \", unique_pairs_df)\n    # find the nearest 24 pixels for every single pixels\n    # Create a KDTree using 'LAT' and 'LON' columns\n    from scipy.spatial import cKDTree\n    tree = cKDTree(unique_pairs_df[['LAT', ' LON']])\n\n    # Find the 24 nearest neighbors for each point\n    distances, indices = tree.query(unique_pairs_df, k=25)\n\n    print(\"distances = \", distances)\n\n    # Extract the nearest 24 neighbors (excluding the point itself)\n    nearest_24 = indices[:, 1:]\n    print(\"nearest_24 = \", nearest_24)\n    print(\"nearest_24.shape = \", nearest_24.shape)\n\n    # Create column names for the new columns\n    new_columns = [f'Nearest_{i}' for i in range(1, 25)]\n\n    nearest_24_df = pd.DataFrame(nearest_24, columns=new_columns)\n\n    print(\"unique_pairs_df.shape: \", unique_pairs_df.shape)\n\n    # Merge the DataFrames row by row\n    result = pd.concat([unique_pairs_df.reset_index(drop=True), nearest_24_df.reset_index(drop=True)], axis=1)\n\n    print(result.head())\n    print(result.shape)\n\n    result.to_csv(grid_to_window_mapper_csv, index=False)\n    print(f\"grid to window mapper csv is saved to {grid_to_window_mapper_csv}\")\n    \n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "rg5g1a",
  "name" : "plot_results_slurm",
  "description" : null,
  "code" : "#!/bin/bash\n\necho \"start to run plot_results.sh\"\npwd\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"plot_results_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J fc_model_predict_2weeks       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 12               # Number of CPUs per task (threads)\n#SBATCH --mem=20G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-10:00         # Runtime in D-HH:MM format\n## Slurm can send you updates via email\n#SBATCH --mail-type=FAIL  # BEGIN,END,FAIL         # ALL,NONE,BEGIN,END,FAIL,REQUEUE,..\n#SBATCH --mail-user=zsun@gmu.edu     # Put your GMU email address here\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\n# Call the Python script using process substitution\npython -u << INNER_EOF\n\nfrom plot_results import plot_images\nfrom generate_mapfiles import generate_mapfiles\n\nplot_images()\n\ngenerate_mapfiles()\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nfile_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\nprevious_content=$(cat file_name)\nexit_code=0\nwhile true; do\n    # Capture the current content\\\n    #echo ${job_id}\n    file_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\n    #echo \"file_name=\"$file_name\n    current_content=$(<\"${file_name}\")\n    #echo \"current_content = \"$current_content\n\n    # Compare current content with previous content\n    diff_result=$(diff <(echo \"$previous_content\") <(echo \"$current_content\"))\n    # Check if there is new content\n    if [ -n \"$diff_result\" ]; then\n        # Print the newly added content\n        #echo \"New content added:\"\n        echo \"$diff_result\"\n        #echo \"---------------------\"\n    fi\n    # Update previous content\n    previous_content=\"$current_content\"\n\n\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    #echo \"job_status \"$job_status\n    #if [[ $job_status == \"JobState=COMPLETED\" ]]; then\n    #    break\n    #fi\n    if [[ $job_status == *\"COMPLETED\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    elif [[ $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        exit_code=1\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\n# find /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\n#cat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n\nexit $exit_code\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "k09hf7",
  "name" : "generate_mapfiles",
  "description" : null,
  "code" : "# Generate new map files for new geotifs\n\nimport os\nimport shutil\nimport re\nimport pandas as pd\nfrom datetime import datetime\n\n#from plot_results import output_folder\n\ndef create_mapserver_map_config(target_geotiff_file_path, force=False):\n  geotiff_file_name = os.path.basename(target_geotiff_file_path)\n  geotiff_dir_name = os.path.dirname(target_geotiff_file_path)\n  geotiff_mapserver_file_path = f\"{geotiff_dir_name}/{geotiff_file_name}.map\"\n  if not geotiff_file_name.endswith(\".tif\"):\n    print(f\"{geotiff_file_name} is not geotiff\")\n    return\n  \n  if os.path.exists(geotiff_mapserver_file_path) and not force:\n    print(f\"{geotiff_mapserver_file_path} already exists\")\n    return geotiff_mapserver_file_path\n  \n  # Define a regular expression pattern to match the date in the filename\n  pattern = r\"\\d{4}\\d{2}\\d{2}\"\n\n  # Use re.search to find the match\n  match = re.search(pattern, geotiff_file_name)\n\n  # Check if a match is found\n  if match:\n      date_string = match.group()\n      print(\"Date:\", date_string)\n  else:\n      print(\"No date found in the filename.\")\n      return f\"The file's name {target_geotiff_file} is wrong\"\n  \n#   Driver: GTiff/GeoTIFF\n# Files: firedata_20210725_predicted.txt_output.tif\n# Size is 6000, 2600\n# Coordinate System is:\n# GEOGCS[\"WGS 84\",\n#     DATUM[\"WGS_1984\",\n#         SPHEROID[\"WGS 84\",6378137,298.257223563,\n#             AUTHORITY[\"EPSG\",\"7030\"]],\n#         AUTHORITY[\"EPSG\",\"6326\"]],\n#     PRIMEM[\"Greenwich\",0],\n#     UNIT[\"degree\",0.0174532925199433],\n#     AUTHORITY[\"EPSG\",\"4326\"]]\n# Origin = (-126.000000000000000,50.500000000000000)\n# Pixel Size = (0.010000000000000,-0.010000000000000)\n# Metadata:\n#   AREA_OR_POINT=Area\n# Image Structure Metadata:\n#   INTERLEAVE=BAND\n# Corner Coordinates:\n# Upper Left  (-126.0000000,  50.5000000) (126d 0' 0.00\"W, 50d30' 0.00\"N)\n# Lower Left  (-126.0000000,  24.5000000) (126d 0' 0.00\"W, 24d30' 0.00\"N)\n# Upper Right ( -66.0000000,  50.5000000) ( 66d 0' 0.00\"W, 50d30' 0.00\"N)\n# Lower Right ( -66.0000000,  24.5000000) ( 66d 0' 0.00\"W, 24d30' 0.00\"N)\n# Center      ( -96.0000000,  37.5000000) ( 96d 0' 0.00\"W, 37d30' 0.00\"N)\n# Band 1 Block=6000x1 Type=Float32, ColorInterp=Gray\n  \n  mapserver_config_content = f\"\"\"\nMAP\n  NAME \"wildfiremap\"\n  STATUS ON\n  EXTENT -126 24.5 -66 50.5\n  SIZE 6000 2600\n  UNITS DD\n  SHAPEPATH \"/var/www/html/wildfire_site/data\"\n\n  PROJECTION\n    \"init=epsg:4326\"\n  END\n\n  WEB\n    IMAGEPATH \"/temp/\"\n    IMAGEURL \"/temp/\"\n    METADATA\n      \"wms_title\" \"Wildfire MapServer WMS\"\n      \"wms_onlineresource\" \"http://geobrain.csiss.gmu.edu/cgi-bin/mapserv?map=/var/www/html/wildfire_site/data/wildfire.map&\"\n      WMS_ENABLE_REQUEST      \"*\"\n      WCS_ENABLE_REQUEST      \"*\"\n      \"wms_srs\" \"epsg:5070 epsg:4326 epsg:3857\"\n    END\n  END\n\n\n  LAYER\n    NAME \"predicted_wildfire_{date_string}\"\n    TYPE RASTER\n    STATUS DEFAULT\n    DATA \"/var/www/html/wildfire_site/data/{geotiff_file_name}\"\n\n    PROJECTION\n      \"init=epsg:4326\"\n    END\n\n    METADATA\n      \"wms_include_items\" \"all\"\n    END\n    PROCESSING \"SCALE=0.0,30.0\"\n    PROCESSING \"SCALE_BUCKETS=15\"\n    PROCESSING \"NODATA=0\"\n    STATUS ON\n    DUMP TRUE\n    TYPE RASTER\n    OFFSITE 0 0 0\n    CLASSITEM \"[pixel]\"\n    TEMPLATE \"template.html\"\n    INCLUDE \"legend_wildfire.map\"\n  END\nEND\n\"\"\"\n  \n  with open(geotiff_mapserver_file_path, \"w\") as file:\n    file.write(mapserver_config_content)\n    \n  print(f\"Mapserver config is created at {geotiff_mapserver_file_path}\")\n  return geotiff_mapserver_file_path\n\ndef refresh_available_date_list(target_geotiff_file_path):\n  geotiff_dir_name = os.path.dirname(target_geotiff_file_path)\n  \n  # Define columns for the DataFrame\n  columns = [\"date\", \"predicted_wildfire_url_prefix\"]\n\n  # Create an empty DataFrame with columns\n  df = pd.DataFrame(columns=columns)\n  \n  for filename in os.listdir(geotiff_dir_name):\n    target_geotiff_file = os.path.join(geotiff_dir_name, filename)\n    \n    if \".tif\" not in target_geotiff_file:\n      continue\n      \n    print(\"Processing \", target_geotiff_file)\n      \n    # generate map file first\n    create_mapserver_map_config(target_geotiff_file, force=True)\n    \n    date_str = re.search(r\"\\d{4}\\d{2}\\d{2}\", filename).group()\n    date = datetime.strptime(date_str, \"%Y%m%d\")\n    formatted_date = date.strftime(\"%Y-%m-%d\")\n    # Append a new row to the DataFrame\n    df = df.append({\n      \"date\": formatted_date, \n      \"predicted_wildfire_url_prefix\": f\"{filename}\"\n    }, ignore_index=True)\n  \n  # Save DataFrame to a CSV file\n  df.to_csv(f\"{geotiff_dir_name}/date_list.csv\", index=False)\n\n  # Display the final DataFrame\n  print(df)\n  \n\n\n\ndef generate_mapfiles():\n  output_folder = \"/groups/ESS3/zsun/firecasting/data/output/output_xgboost_window_15_days_forecasting/20210714/\"\n  print(\"Current folder: \", output_folder)\n  refresh_available_date_list(output_folder)\n\nif __name__ == \"__main__\":\n  generate_mapfiles()\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
}]
