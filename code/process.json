[{
  "id" : "1jepv8",
  "name" : "fc_train_data_preprocess",
  "description" : null,
  "code" : "# Step 1: read and prepare the txt files by yunyao\n\nimport os\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom data_preparation_utils import create_grid_to_window_mapper\n\n# Folder path containing the text files\n#folder_path = '/groups/ESS3/yli74/data/AI_Emis/firedata'  # The folder yunyao provided with two years of txt files\n# folder_path = '/groups/ESS3/yli74/data/AI_Emis/firedata_VHI'\nfolder_path = \"/groups/ESS3/yli74/data/AI_Emis/GLOB/\"\nmy_file_path = \"/groups/ESS3/zsun/firecasting/data/others/\"\ngrid_to_window_mapper_csv = f\"{my_file_path}/grid_cell_nearest_neight_mapper.csv\"\ntraining_data_folder = \"/groups/ESS3/zsun/firecasting/data/train/\"\n\nstart_date = \"20200901\"\nend_date = \"20200907\"\n\ncolumns_to_be_time_series = ['FRP']\n\n# Define the columns to check for zero values\ncolumns_to_check = ['FRP_1_days_ago', \n#                     'FRP_2_days_ago',\n\n# 'FRP_3_days_ago', 'FRP_4_days_ago', 'FRP_5_days_ago',\n\n# 'FRP_6_days_ago', 'FRP_7_days_ago', \n                    'Nearest_1', 'Nearest_2',\n                    'Nearest_3', 'Nearest_4', \n                    'Nearest_5', 'Nearest_6', \n                    'Nearest_7', 'Nearest_8', \n#                     'Nearest_9', 'Nearest_10', 'Nearest_11', 'Nearest_12',\n\n# 'Nearest_13', 'Nearest_14', 'Nearest_15', 'Nearest_16', 'Nearest_17',\n\n# 'Nearest_18', 'Nearest_19', 'Nearest_20', 'Nearest_21', 'Nearest_22',\n\n# 'Nearest_23', 'Nearest_24', \n                   ]\n\n\ndef read_original_txt_files(datestr):\n  # time range: 2020-01-01 to 2021-12-31\n  # Specify chunk size\n  #chunk_size = 1000\n  #row_limit = 1000\n\n  # Initialize an empty DataFrame\n  df_list = []\n  #total_rows = 0\n\n  # Traverse through files in the folder\n  # firedata_20201208.txt\n  file_path = os.path.join(folder_path, f\"firedata_{datestr}.txt\")\n  print(f\"Reading original file:  {file_path}\")\n  file_df = pd.read_csv(file_path)  # Adjust separator if needed\n  #for chunk in chunk_generator:\n  df_list.append(file_df)\n  # Concatenate all chunks into a single DataFrame\n  final_df = pd.concat(df_list, ignore_index=True)\n  \n  \n  # Display the DataFrame\n  #print(final_df)\n  return final_df\n\ndef get_one_day_time_series_training_data(target_day):\n  # this function is used to get 7 days time series for one day prediction\n  # From now on, `target_day` will be Day_0. \n  # So remember to change all the `Dayx` columents to `FRP_{i+1}_days_ago` \n  # to eliminate the confusion. \n  print(\"preparing training data for \", target_day)\n  df = read_original_txt_files(target_day)\n  # go back 7 days to get all the history FRP and attach to the df with matched coordinates\n  \n  # get grid to window mapper csv\n  #grid_to_window_mapper_df = pd.read_csv(grid_to_window_mapper_csv)\n  #print(grid_to_window_mapper_df.columns)\n  \n  target_dt = datetime.strptime(target_day, '%Y%m%d')\n  for i in range(7):\n    past_dt = target_dt - timedelta(days=i+1)\n    print(\"preparing data for past date\", past_dt.strftime('%Y%m%d'))\n    past_df = read_original_txt_files(past_dt.strftime('%Y%m%d'))\n    for c in columns_to_be_time_series:\n      column_to_append = past_df[c]\n      df[f'{c}_{i+1}_days_ago'] = column_to_append\n    \n  #print(df.head())\n  \n  #grid_to_window_mapper_df.set_index(['LAT', 'LON'], inplace=True)\n  \n  #nearest_columns = grid_to_window_mapper_df.columns\n  #print(\"nearest columns: \", nearest_columns)\n  #print(\"df.shape: \", df.shape)\n  #print(\"df.iloc[100] = \", df.iloc[100][\"FRP_1_days_ago\"])\n  \n#   original_df = df\n  \n#   def add_window_grid_cells(row):\n#     result = grid_to_window_mapper_df.loc[row['LAT'], row[' LON']]\n#     values = []\n#     for column in nearest_columns:\n#         #print(\"column = \", column)\n#         nearest_index = result[column]\n#         #print(\"nearest_index = \", nearest_index)\n#         # for all the nearest grid cells, we will use yesterday (-1 day ago) value to fill. So all the neighbor grid cells' history will be used to inference the target day's current grid cell's FRP.\n#         values.append(original_df.iloc[nearest_index][\" FRP_1_days_ago\"])\n#     if len(values) != 24:\n#       raise ValueError(\"The nearest values are not 24.\")\n#     return pd.Series(values)\n  \n#   #dropped_df = grid_to_window_mapper_df.drop([\"LAT\", \"LON\"], axis=1)\n#   print(\"new columns: \", grid_to_window_mapper_df.columns)\n#   print(new_df.describe())\n#   print(\"nearest_columns length: \", len(nearest_columns))\n#   new_df = df.apply(add_window_grid_cells, axis=1)\n#   print(\"new_df.shape = \", new_df.shape)\n#   print(\"df.shape = \", df.shape)\n#   df[nearest_columns] = new_df\n\n  #print(\"New time series dataframe: \", df.head())\n  return df\n\n  \ndef create_training_time_series_dataframe(start_date, end_date):\n  start_dt = datetime.strptime(start_date, '%Y%m%d')\n  end_dt = datetime.strptime(end_date, '%Y%m%d')\n  \n  # Traverse each day and print\n  current_dt = start_dt\n  \n  while current_dt <= end_dt:\n    print(current_dt.strftime('%Y%m%d'))\n    current_dt += timedelta(days=1)\n    \n    \n    break\n    \n  return \n\ndef prepare_training_data(target_date, training_data_folder=training_data_folder):\n  # Assuming 'target' is the column to predict\n  # create_grid_to_window_mapper()\n  \n  if not os.path.exists(training_data_folder):\n    os.makedirs(training_data_folder)\n    print(f\"Folder created: {training_data_folder}\")\n  else:\n    print(f\"Folder already exists: {training_data_folder}\")\n  \n  target_col = 'FRP'\n  \n  train_file_path = f\"{training_data_folder}/{target_date}_time_series_with_new_window.csv\"\n  \n  if os.path.exists(train_file_path):\n    print(f\"File {train_file_path} exists\")\n    existing_df = pd.read_csv(train_file_path)\n    # X = existing_df.drop([target_col, 'LAT', 'LON'], axis=1)\n    X = existing_df\n    y = existing_df[target_col]\n  else:\n    print(\"File does not exist\")\n    original_df = get_one_day_time_series_training_data(target_date)\n    df = original_df\n    \n    #print(\"all feature names: \", df.columns)\n\n    #print(\"Lag/Shift the data for previous days' information\")\n    num_previous_days = 7  # Adjust the number of previous days to consider\n\n    # Drop rows with NaN values from the shifted columns\n    df_filled = df.fillna(-999)\n\n    #print(\"drop rows where the previous day has no fire on that pixel\")\n    # df = df[df[' FRP'] != 0]\n    \n\n    # Drop rows where specified columns are equal to zero\n    #df = df[~(df[columns_to_check] == 0).all(axis=1)]\n    #print(\"we have removed all the rows that have no fire in any of the columns - \", columns_to_check)\n    \n    # Drop rows if the previous day FRP is zero and today's FRP is non-zero\n    # df = df[(df[' FRP'] != 0) & (df[columns_to_check] == 0)]\n    # keep the row when any column value is greater than zero\n    df = df[(df[columns_to_check] > 0).any(axis=1)]\n    # df = df[(df[' FRP_1_days_ago'] != 0) | (df[' FRP'] != 0)]\n    \n    df.to_csv(train_file_path, index=False)\n    # Define features and target\n    X = df.drop([target_col,], axis=1)\n    # don't use neighbors \n#     X = df.drop([target_col, 'LAT', ' LON', 'Nearest_1', 'Nearest_2',\n                 \n# 'Nearest_3', 'Nearest_4', 'Nearest_5', 'Nearest_6', 'Nearest_7',\n\n# 'Nearest_8', 'Nearest_9', 'Nearest_10', 'Nearest_11', 'Nearest_12',\n\n# 'Nearest_13', 'Nearest_14', 'Nearest_15', 'Nearest_16', 'Nearest_17',\n\n# 'Nearest_18', 'Nearest_19', 'Nearest_20', 'Nearest_21', 'Nearest_22',\n\n# 'Nearest_23', 'Nearest_24',], axis=1)\n    y = df[target_col]\n  \n  return X, y\n\n# target column is current day's FRP, previous days' FRP and all the other columns are inputs\n\n#read_original_txt_files()\n\nif __name__ == \"__main__\":\n  # this is today, and we want to use all the meteo data of today and FRP data of day -7 - yesterday to predict today's FRP. \n  training_end_date = \"20200715\" # the last day of the 7 day history\n  prepare_training_data(training_end_date)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "w4lpt8",
  "name" : "fc_model_creation",
  "description" : null,
  "code" : "import os\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom lightgbm import LGBMRegressor\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nimport pickle\nimport warnings\nimport random\nfrom dask import delayed, compute\nfrom dask.distributed import LocalCluster, Client\nimport dask\nimport time\nfrom sklearn.impute import SimpleImputer\n\nwarnings.filterwarnings(\"ignore\", message=\"DataFrame is highly fragmented\")\n\ndef check_df_memory_usage(df):\n    # Calculate the total memory usage of the DataFrame\n    total_memory_usage = df.memory_usage(deep=True).sum()\n\n    # Convert the total memory usage to MB\n    total_memory_usage_mb = total_memory_usage / (1024 ** 2)\n\n    print(f\"Total memory usage: {total_memory_usage_mb:.2f} MB\")\n\nclass ModelHandler:\n    def train(\n        self,\n        X_train,\n        y_train,\n        read_existing: bool=False,\n        model_path: str=\"/path/to/model\",\n    ):\n        raise NotImplementedError\n\n    def predict(self, X_test):\n        raise NotImplementedError\n\n    def save_model(self, model, model_path):\n        raise NotImplementedError\n\n    def load_model(self, model_path):\n        raise NotImplementedError\n\n    def print_model_metadata(self):\n        raise NotImplementedError\n\n\nclass LightGBMHandler(ModelHandler):\n    def __init__(self):\n        self.model = None\n\n    def train(\n        self,\n        X_train,\n        y_train,\n        read_existing=False,\n        model_path: str=\"/path/to/model\",\n    ):\n        if np.any(y_train == -999) or np.any(np.isnan(y_train)):\n            raise ValueError(\"The DataFrame 'y_train' contains -999 or NaN values, which are invalid.\")\n        # # Train the model with the current batch\n        if self.model and hasattr(self.model, \"booster_\"):\n            print(\"Model is trained.\")\n            self.model = self.model.fit(X_train, y_train, init_model=self.model.booster_)\n        else:\n            print(\"Model is not trained.\")\n            if read_existing and os.path.exists(model_path):\n                self.load_model(model_path)\n                self.model = self.model.fit(X_train, y_train, init_model=self.model.booster_)\n            else:\n                self.model = LGBMRegressor(n_jobs=-1, random_state=42)\n                self.model = self.model.fit(X_train, y_train)\n\n    def print_model_metadata(self):\n        # Dump the model's metadata\n        model = self.model  # assuming self.model is an instance of LGBMRegressor\n\n        # Print general information about the model\n        print(\"LightGBM Model Metadata:\")\n        \n        # Print model parameters\n        print(\"Model Parameters:\", model.get_params())\n        \n        # Print number of boosting rounds\n        print(\"Number of boosting rounds:\", model.n_estimators)\n        \n        # Print best score achieved by the model\n        if hasattr(model, 'best_score_'):\n            print(\"Best Score:\", model.best_score_)\n        \n        # Print feature importances\n        if hasattr(model, 'feature_importances_'):\n            print(\"Feature Importances:\", model.feature_importances_)\n        \n        # Other attributes like best iteration\n        if hasattr(model, 'best_iteration_'):\n            print(\"Best Iteration:\", model.best_iteration_)\n\n    def predict(self, X_test):\n        self.print_model_metadata()\n        return self.model.predict(X_test)\n\n    def save_model(self, model_path):\n        with open(model_path, \"wb\") as model_file:\n            pickle.dump(self.model, model_file)\n\n    def load_model(self, model_path):\n        with open(model_path, \"rb\") as model_file:\n            print(f\"loading model from {model_file}\")\n            self.model = pickle.load(model_file)\n\n\nclass TabNetHandler(ModelHandler):\n    def __init__(self):\n        self.model = TabNetRegressor(\n            n_d=17,\n            n_a=41,\n            n_steps=4,\n            gamma=1.154,\n            lambda_sparse=0.000426,\n        )\n        self.is_trained = False\n\n    def train(\n        self,\n        X_train,\n        y_train,\n        read_existing: bool=False,\n        model_path: str=\"/path/to/model\",\n    ):\n        y_train = y_train.to_numpy().reshape(-1, 1)\n        if not self.is_trained and read_existing:\n            self.load_model(model_path)\n        self.model.fit(\n            X_train.values,\n            y_train,\n            eval_set=[(X_train.values, y_train)],\n            eval_metric=[\"mae\"],\n            max_epochs=5,\n            patience=2,\n            batch_size=4096,\n        )\n\n    def print_model_metadata(self):\n        model = self.model\n        # Print general information about the model\n        print(\"TabNet Model Metadata:\")\n        \n        # Print model architecture details\n        print(\"Number of Features (input_dim):\", model.input_dim)\n        print(\"Number of Classes (output_dim):\", model.output_dim)\n        print(\"Number of Decision Steps (n_steps):\", model.n_steps)\n        print(\"Number of Independent Gated Layers (n_independent):\", model.n_independent)\n        print(\"Number of Shared Gated Layers (n_shared):\", model.n_shared)\n        \n        # Print feature importances if available\n        if hasattr(model, 'feature_importances_'):\n            print(\"Feature Importances:\", model.feature_importances_)\n\n        # TabNet-specific parameters (if available)\n        if hasattr(model, 'gamma'):\n            print(\"Gamma:\", model.gamma)\n        if hasattr(model, 'lambda_sparse'):\n            print(\"Lambda Sparse:\", model.lambda_sparse)\n        if hasattr(model, 'mask_type'):\n            print(\"Mask Type:\", model.mask_type)\n        if hasattr(model, 'virtual_batch_size'):\n            print(\"Virtual Batch Size:\", model.virtual_batch_size)\n\n    def predict(self, X_test):\n        self.print_model_metadata()\n        return self.model.predict(X_test.values)\n\n    def save_model(self, model_path):\n        if not self.is_trained:\n            print(\"The model hasn't been trained yet. Skipping saving\")\n            return\n        self.model.save_model(model_path)\n\n    def load_model(self, model_path):\n        if os.path.exists(model_path+\".zip\"):\n            self.model.load_model(model_path+\".zip\")\n\n\nclass WildfireModelTrainer:\n    def __init__(\n        self,\n        model_type=\"lightgbm\",\n        chosen_input_columns=[],\n        training_data_folder=\"/path/to/training/data\",\n    ):\n        self.training_data_folder = training_data_folder\n        self.target_col = \"FRP\"\n        self.key = \"single\"\n        self.chosen_input_columns = chosen_input_columns\n        self.model_handlers = {\n            \"single\": self.init_model_handler(model_type),\n            \"single_giant\": self.init_model_handler(model_type),\n            \"large_west\": self.init_model_handler(model_type),\n            \"small_west\": self.init_model_handler(model_type),\n            \"large_east\": self.init_model_handler(model_type),\n            \"small_east\": self.init_model_handler(model_type),\n        }\n\n    def init_model_handler(self, model_type):\n        if model_type == \"tabnet\":\n            return TabNetHandler()\n        else:\n            return LightGBMHandler()\n\n    def read_original_txt_files(self, folder_path, datestr):\n        file_path = os.path.join(folder_path, f\"firedata_{datestr}.txt\")\n        print(f\"Reading original file: {file_path}\")\n        return pd.read_csv(file_path)\n\n    def get_one_day_time_series_training_data(self, folder_path, target_day):\n        df = self.read_original_txt_files(folder_path, target_day)\n        target_dt = datetime.strptime(target_day, \"%Y%m%d\")\n        for i in range(7):\n            past_dt = target_dt - timedelta(days=i + 1)\n            past_df = self.read_original_txt_files(\n                folder_path, past_dt.strftime(\"%Y%m%d\")\n            )\n            for c in [\"FRP\"]:\n                df[f\"{c}_{i + 1}_days_ago\"] = past_df[c]\n        return df\n\n    def prepare_training_data(\n        self, folder_path, target_date, skip_generation_if_exists: bool = True\n    ):\n        if not os.path.exists(self.training_data_folder):\n            os.makedirs(self.training_data_folder)\n            print(f\"Folder created: {self.training_data_folder}\")\n        else:\n            print(f\"Folder already exists: {self.training_data_folder}\")\n\n        train_file_path = os.path.join(\n            self.training_data_folder, f\"{target_date}_time_series_with_new_window.csv\"\n        )\n\n        if os.path.exists(train_file_path) and skip_generation_if_exists:\n            print(f\"File {train_file_path} exists\")\n            df = pd.read_csv(train_file_path)\n        else:\n            df = self.get_one_day_time_series_training_data(folder_path, target_date)\n            df.fillna(-999, inplace=True)\n            df = df[\n                (\n                    df[\n                        [\n                            \"FRP_1_days_ago\",\n                            \"Nearest_1\",\n                            \"Nearest_2\",\n                            \"Nearest_3\",\n                            \"Nearest_4\",\n                            \"Nearest_5\",\n                            \"Nearest_6\",\n                            \"Nearest_7\",\n                            \"Nearest_8\",\n                        ]\n                    ]\n                    > 0\n                ).any(axis=1)\n            ]\n            df.to_csv(train_file_path, index=False)\n\n        # drop all the rows that have any column = -999 in the input and target columns\n        df.dropna(subset=self.chosen_input_columns + [self.target_col], inplace=True)\n\n        X = df[self.chosen_input_columns]\n        y = df[self.target_col]\n        return X, y\n\n    def filling_missing_value(self, df):\n        num_imputer = SimpleImputer(strategy='mean')\n        print(\"missing value filled with mean values\")\n        # Apply the imputer to fill missing values\n        df_imputed = num_imputer.fit_transform(df)\n        # Convert the result back to a DataFrame\n        df_imputed = pd.DataFrame(df_imputed, columns=df.columns, index=df.index)\n        return df_imputed\n\n    # @delayed\n    def process_date(\n        self,\n        current_date,\n        folder_path,\n        model_path,\n        skip_generation_if_exists,\n        prepare_data_only: bool = False,\n        do_test: bool = False,\n    ):\n        date_str = current_date.strftime(\"%Y%m%d\")\n        print(f\"Processing data for {date_str}\")\n\n        # Prepare training data\n        X, y = self.prepare_training_data(\n            folder_path, date_str, skip_generation_if_exists\n        )\n\n        if X.empty or y.empty:\n            print(f\"No data available for {date_str}. Skipping...\")\n            return\n\n        if prepare_data_only:\n            print(\"Only generate training data. Don't train AI model.\")\n            return\n        \n        # Drop rows with NaN values in the target column\n        X[self.target_col] = np.log10(y + 1e-2)\n\n        X.replace(-999, np.nan, inplace=True)\n        X = X[self.chosen_input_columns + [self.target_col]]\n        X = self.filling_missing_value(X)\n        # X.dropna(inplace=True)\n        check_df_memory_usage(X)\n        \n        print(\"X.columns = \", X.columns)\n        # Separate the input features and the target\n        y = X[self.target_col]\n        X = X[self.chosen_input_columns]\n\n        if X.empty:\n            return\n\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.2, random_state=42, shuffle=False\n        )\n\n        # Perform incremental training\n        self.model_handlers[self.key].train(\n            X_train,\n            y_train,\n            read_existing=True,\n            model_path=model_path,\n        )\n\n        self.model_handlers[self.key].save_model(model_path)\n        print(f\"Save to {model_path}\")\n\n        if do_test:\n            self.test_model(X_test, y_test, model_path)\n\n    def test_model(self, X_test, y_test, model_path):\n        y_pred_test = self.model_handlers[self.key].predict(X_test)\n\n        mse = mean_squared_error(y_test, y_pred_test)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(y_test, y_pred_test)\n        r2 = r2_score(y_test, y_pred_test)\n\n        print(f\"Category: {self.key}\")\n        print(f\"Mean Squared Error (MSE): {mse}\")\n        print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n        print(f\"Mean Absolute Error (MAE): {mae}\")\n        print(f\"R-squared (R2): {r2}\")\n        \n\n    def train_model_on_one_file(\n        self,\n        start_date_str,\n        end_date_str,\n        training_csv_path,\n        model_paths,\n        fire_size_threshold=300,\n        region_dividing_longitude=-100,\n    ):\n        start_date = datetime.strptime(start_date_str, \"%Y%m%d\")\n        end_date = datetime.strptime(end_date_str, \"%Y%m%d\")\n        self.key = \"single_giant\"\n        model_path = model_paths[self.key]\n\n        df = pd.read_csv(\n            training_csv_path\n            , nrows=1000\n        )\n        check_df_memory_usage(df)\n\n        df.replace(-999, np.nan, inplace=True)\n        df = df[self.chosen_input_columns + [self.target_col]]\n        df = self.filling_missing_value(df)\n        # df.dropna(inplace=True)\n        check_df_memory_usage(df)\n\n        X = df[self.chosen_input_columns]\n        y = df[self.target_col]\n\n        if X.empty or y.empty:\n            print(f\"No data available for {date_str}. Skipping...\")\n            return\n        \n        # Drop rows with NaN values in the target column\n        X[self.target_col] = np.log10(y + 1e-2)\n        check_df_memory_usage(X)\n\n        print(\"X.columns = \", X.columns)\n        # Separate the input features and the target\n        y = X[self.target_col]\n        X = X[self.chosen_input_columns]\n\n        if X.empty:\n            return\n\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.2, random_state=42, shuffle=True\n        )\n        check_df_memory_usage(X_train)\n\n        # Perform incremental training\n        self.model_handlers[self.key].train(\n            X_train,\n            y_train,\n            read_existing=True,\n            model_path=model_path,\n        )\n\n        self.model_handlers[self.key].save_model(model_path)\n        print(f\"Save to {model_path}\")\n\n        self.check_point(\n            model_path, \n            start_date_str, \n            end_date_str,\n            backup_random=True\n        )\n\n        check_df_memory_usage(X_test)\n        self.test_model(X_test, y_test, model_path)\n\n\n    def train_model(\n        self,\n        start_date_str,\n        end_date_str,\n        folder_path,\n        model_paths,\n        fire_size_threshold=300,\n        region_dividing_longitude=-100,\n        skip_generation_if_exists: bool = True,\n    ):\n        start_date = datetime.strptime(start_date_str, \"%Y%m%d\")\n        end_date = datetime.strptime(end_date_str, \"%Y%m%d\")\n        current_date = start_date\n        self.key = \"single\"\n\n        all_data_frames = []\n\n        # Save model for each category\n        model_path = model_paths[self.key]\n\n        # Generate a list of all dates between the start and end dates\n        all_dates = []\n        current_date = start_date\n        while current_date <= end_date:\n            all_dates.append(current_date)\n            current_date += timedelta(days=1)\n\n        # Shuffle the dates\n        random.shuffle(all_dates)\n\n        # Create a list of delayed tasks\n        tasks = []\n        count = 1\n        for current_date in all_dates:\n            print(f\"Processing {current_date}\")\n            start_time = time.time()  # Record start time\n            do_test = False\n            if count % 10 == 0:\n                self.check_point(\n                    model_path, \n                    start_date_str, \n                    end_date_str,\n                    backup_random=True\n                )\n                do_test = True\n            self.process_date(\n                current_date,\n                folder_path,\n                model_path,\n                skip_generation_if_exists,\n                # prepare_data_only = True,  # first generate all training data\n                do_test=do_test,\n            )\n            # tasks.append(task)\n            # dask.compute(task)  # tabnet/lightgbm need single thread\n            count += 1\n            end_time = time.time()  # Record end time\n            elapsed_time = end_time - start_time  # Calculate elapsed time\n            print(f\"Time taken for {current_date}: {elapsed_time:.2f} seconds\")\n\n        # Execute all tasks in parallel\n        # compute(*tasks, scheduler='threads')\n        self.check_point(\n            model_path, start_date_str, end_date_str,\n            backup_random=True\n        )\n        print(\"Training completed for all categories.\")\n\n    def check_point(\n        self, model_path, start_date_str, \n        end_date_str,\n        backup_random: bool=True\n    ):\n        self.model_handlers[self.key].save_model(model_path)\n        if backup_random:\n            now = datetime.now()\n            date_time = now.strftime(\"%Y%d%m%H%M%S\")\n            random_model_path = (\n                f\"{model_path}_{start_date_str}_{end_date_str}_{date_time}.pkl\"\n            )\n            self.model_handlers[self.key].save_model(random_model_path)\n            print(f\"A copy of the model is saved to {random_model_path}\")\n\n    def stratified_sampling(\n        self,\n        start_date_str,\n        end_date_str,\n        folder_path,\n        model_paths,\n        giant_output_file,\n        fire_size_threshold=300,\n        region_dividing_longitude=-100,\n        skip_generation_if_exists: bool = True,\n    ):\n        start_date = datetime.strptime(start_date_str, \"%Y%m%d\")\n        end_date = datetime.strptime(end_date_str, \"%Y%m%d\")\n        current_date = start_date\n        self.key = \"single\"\n\n        all_data_frames = []\n\n        # Save model for each category\n        model_path = model_paths[self.key]\n\n        # Generate a list of all dates between the start and end dates\n        all_dates = []\n        current_date = start_date\n        while current_date <= end_date:\n            all_dates.append(current_date)\n            current_date += timedelta(days=1)\n\n        # Shuffle the dates\n        random.shuffle(all_dates)\n\n        sample_ratio = (10 / 500)\n\n        # Create a list of delayed tasks\n        tasks = []\n        count = 1\n\n        @delayed\n        def process_single_date(current_date):\n            print(f\"Processing {current_date}\")\n            date_str = current_date.strftime(\"%Y%m%d\")\n            start_time = time.time()  # Record start time\n\n            X, y = self.prepare_training_data(\n                folder_path, date_str, skip_generation_if_exists\n            )\n\n            if X.empty or y.empty:\n                print(f\"No data available for {date_str}. Skipping...\")\n                return None  # Return None if no data is available\n\n            X[self.target_col] = y\n            # Drop rows with NaN values in the target column\n            X = X.dropna()\n            print(\"dropped na\")\n            check_df_memory_usage(X)\n\n            y = X[self.target_col]\n            X = X.drop(columns=[self.target_col])\n\n            if len(y.unique()) < 5:\n                print(f\"Insufficient samples in one of the bins for {current_date}. Use random\")\n                X_train, _, y_train, _ = train_test_split(\n                    X, y, \n                    test_size=0.9,\n                    random_state=42,  # Ensure reproducibility\n                )\n            else:\n                # Binning or quantile stratification (if needed)\n                binned_y = pd.qcut(\n                    y,\n                    q=10, \n                    duplicates=\"drop\"\n                )  # Example of stratification\n                X_train, _, y_train, _ = train_test_split(\n                    X, y, \n                    test_size=0.9,  # Use 90% of data as the test set\n                    stratify=binned_y  # Stratify by binned/quantile-based target if necessary\n                )\n            print(f\"Training set size: {X_train.shape[0]}\")\n\n            X_train[self.target_col] = y_train\n            end_time = time.time()  # Record end time\n            elapsed_time = end_time - start_time  # Calculate elapsed time\n            print(f\"Time taken for {current_date}: {elapsed_time:.2f} seconds\")\n\n            return X_train\n\n        # Add each task to the list\n        for current_date in all_dates:\n            task = process_single_date(current_date)\n            tasks.append(task)\n\n        # Compute the delayed tasks in parallel\n        train_dfs = dask.compute(*tasks)\n\n        # Filter out any None values (for dates with no data)\n        train_dfs = [df for df in train_dfs if df is not None]\n\n        # Concatenate all the dataframes into a single large dataframe\n        big_train_df = pd.concat(train_dfs, ignore_index=True)\n\n        # Save the final dataframe to a CSV file\n        big_train_df.to_csv(giant_output_file, index=False)\n        print(f\"Final training data saved to {giant_output_file}\")\n\n# Define global variables that can be imported by others\nmodel_type = \"lightgbm\"  # Can be 'lightgbm' or 'tabnet'\ndef get_model_paths(model_type):\n    return {\n        \"single\": f\"/groups/ESS3/zsun/firecasting/model/fc_{model_type}_single.pkl\",\n        \"single_giant\": f\"/groups/ESS3/zsun/firecasting/model/fc_{model_type}_single_giant.pkl\",\n        \"large_west\": f\"/groups/ESS3/zsun/firecasting/model/fc_{model_type}_large_west.pkl\",\n        \"small_west\": f\"/groups/ESS3/zsun/firecasting/model/fc_{model_type}_small_west.pkl\",\n        \"large_east\": f\"/groups/ESS3/zsun/firecasting/model/fc_{model_type}_large_east.pkl\",\n        \"small_east\": f\"/groups/ESS3/zsun/firecasting/model/fc_{model_type}_small_east.pkl\",\n    }\n# folder_path = '/groups/ESS3/yli74/data/AI_Emis/firedata_VHI'\nfolder_path = \"/groups/ESS3/yli74/data/AI_Emis/GLOB\"\ntraining_data_folder = \"/groups/ESS3/zsun/firecasting/data/train/\"\ngiant_training_csv_path = f\"{training_data_folder}\"\nchosen_input_columns = [\n    \"FRP_1_days_ago\",\n    \"Nearest_1\",\n    \"Nearest_5\",\n    \"Nearest_7\",\n    \"Nearest_3\",\n    \"FRP_2_days_ago\",\n    \"V\",\n    \"U\",\n    \"LAT\",\n    \"LON\",\n    \"Nearest_17\",\n    \"Land_Use\",\n    \"RH\",\n    \"T\",\n    \"RAIN\",\n]\n\nif __name__ == \"__main__\":\n    trainer = WildfireModelTrainer(\n        model_type=model_type,\n        training_data_folder=training_data_folder,\n        chosen_input_columns=chosen_input_columns,\n    )\n    # start_date_str = \"20200901\"\n    # end_date_str = \"20200907\"\n\n    # 20160110_20191231\n    start_date_str = \"20160110\"\n    end_date_str = \"20191231\"\n    \n\n    # trainer.train_model(\n    #     start_date_str,\n    #     end_date_str,\n    #     folder_path,\n    #     model_paths,\n    #     fire_size_threshold=1,\n    #     region_dividing_longitude=-100,\n    # )\n\n    key = \"single_giant\"\n    trainer.train_model_on_one_file(\n        start_date_str=start_date_str,\n        end_date_str=end_date_str,\n        training_csv_path=f\"{training_data_folder}/giant_few_shot_samples/stratified_{start_date_str}_{end_date_str}.csv\",\n        model_paths=get_model_paths(model_type),\n        fire_size_threshold=300,\n        region_dividing_longitude=-100,\n    )\n\n    # trainer.stratified_sampling(\n    #     start_date_str,\n    #     end_date_str,\n    #     folder_path,\n    #     model_paths,\n    #     giant_output_file=f\"{training_data_folder}/giant_few_shot_samples/stratified_{start_date_str}_{end_date_str}.csv\",\n    #     fire_size_threshold=1,\n    #     region_dividing_longitude=-100,\n    # )\n    print(f\"Training completed and models saved to {get_model_paths(model_type)[key]}\")\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "521ngk",
  "name" : "fc_test_data_preparation",
  "description" : null,
  "code" : "\n\n# Step 1: read and prepare the txt files by yunyao\n\nimport os\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\nimport dask\nfrom dask import delayed\nimport dask.dataframe as dd\nfrom fc_train_data_preprocess import columns_to_be_time_series\n\n\n# Folder path containing the text files\nfolder_path = \"/groups/ESS3/yli74/data/AI_Emis/firedata_VHI\"\nmy_file_path = \"/groups/ESS3/zsun/firecasting/data/others/\"\ngrid_to_window_mapper_csv = f\"{my_file_path}/grid_cell_nearest_neight_mapper.csv\"\ntraining_data_folder = \"/groups/ESS3/zsun/firecasting/data/train/\"\n\nstart_date = \"20200107\"\nend_date = \"20211231\"\n\ndef read_original_txt_files(datestr):\n  # time range: 2020-01-01 to 2021-12-31\n  # Specify chunk size\n  #chunk_size = 1000\n  #row_limit = 1000\n\n  # Initialize an empty DataFrame\n  df_list = []\n  #total_rows = 0\n\n  # Traverse through files in the folder\n  # firedata_20201208.txt\n  file_path = os.path.join(folder_path, f\"firedata_{datestr}.txt\")\n  print(f\"Reading {file_path}\")\n  file_df = pd.read_csv(file_path)  # Adjust separator if needed\n  #for chunk in chunk_generator:\n  df_list.append(file_df)\n  # Concatenate all chunks into a single DataFrame\n  final_df = pd.concat(df_list, ignore_index=True)\n  \n  # Display the DataFrame\n  #print(final_df)\n  return final_df\n\ndef read_txt_from_predicted_folder(target_datestr, current_prediction_output_folder):\n  # time range: 2020-01-01 to 2021-12-31\n  # Specify chunk size\n  #chunk_size = 1000\n  #row_limit = 1000\n\n  # Initialize an empty DataFrame\n#   df_list = []\n  #total_rows = 0\n\n  # Traverse through files in the folder\n  \n  # firedata_20201208.txt\n  file_path = os.path.join(current_prediction_output_folder, f\"firedata_{target_datestr}_predicted.txt\")\n  if not os.path.exists(file_path):\n    print(f\"WARNING: File {file_path} does not exist. Using the real FRP instead. This is mostly likely due to the use of dask parallelization which make it impossible to wait for the previous days' prediction results. Need to disable the parallelization in future to make this work.\")\n    return read_original_txt_files(target_datestr)\n  \n  #print(f\"Reading data from file : {file_path}\")\n  file_df = pd.read_csv(file_path)  # Adjust separator if needed\n  #for chunk in chunk_generator:\n#   df_list.append(file_df)\n  #total_rows += len(file_df)\n\n  # Concatenate all chunks into a single DataFrame\n  #final_df = pd.concat(df_list, ignore_index=True)\n  final_df = file_df\n  #print(\"current final_df head: \", final_df.head())\n  print(\"renaming Predicted_FRP to FRP\")\n  final_df['FRP'] = final_df['Predicted_FRP']\n  # Remove the original column 'A'\n  print(\"remove the current predicted_frp\")\n  final_df.drop(columns=['Predicted_FRP'], inplace=True)\n  return final_df\n\n\ndef add_window_grid_cells(row, original_df, grid_to_window_mapper_df):\n    result = grid_to_window_mapper_df.loc[row['LAT'], row['LON']]\n    values = []\n    for column in grid_to_window_mapper_df.columns:\n        nearest_index = result[column]\n        values.append(original_df.iloc[nearest_index][\"FRP_1_days_ago\"])\n    \n    if len(values) != 24:\n        raise ValueError(\"The nearest values are not 24.\")\n    return pd.Series(values)\n\ndef get_one_day_time_series_for_2_weeks_testing_data(target_day, current_start_day, current_prediction_output_folder):\n    if current_start_day == None or current_prediction_output_folder == None:\n        print(\"just get one day time series\")\n        return get_one_day_time_series_training_data(target_day)\n    else:\n        # get grid to window mapper csv\n        #grid_to_window_mapper_df = pd.read_csv(grid_to_window_mapper_csv)\n\n        target_dt = datetime.strptime(target_day, '%Y%m%d')\n        current_start_dt = datetime.strptime(current_start_day, '%Y%m%d')\n\n        print(f\"Read from original folder for current date: {target_day}\")\n        df = read_original_txt_files(target_day)\n        \n        # go back 7 days to get all the history FRP and attach to the df with matched coordinates\n        for i in range(7):\n            past_dt = target_dt - timedelta(days=i+1)\n            print(f\"reading past files for {past_dt}\")\n            if past_dt >= current_start_dt and past_dt < target_dt:\n                print(f\"reading from predicted folder\")\n                past_df = read_txt_from_predicted_folder(past_dt.strftime('%Y%m%d'), current_prediction_output_folder)\n            else:\n                print(f\"reading from original folder\")\n                past_df = read_original_txt_files(past_dt.strftime('%Y%m%d'))\n                \n            for c in columns_to_be_time_series:\n                column_to_append = past_df[c]\n                df[f'{c}_{i+1}_days_ago'] = column_to_append\n            #column_to_append = past_df[\"FRP\"]\n            #df[f'FRP_{i+1}_days_ago'] = column_to_append\n\n        #original_df = df\n        #print(\"original_df.describe\", original_df.describe())\n        \n        # Reset the index before using set_index\n        #grid_to_window_mapper_df = grid_to_window_mapper_df.reset_index()\n        # adding the neighbor cell values of yesterday to the inputs\n        # grid_to_window_mapper_df = grid_to_window_mapper_df.set_index(['LAT', ' LON'])\n        #grid_to_window_mapper_df['Combined_Location'] = grid_to_window_mapper_df['LAT'].astype(str) + '_' + grid_to_window_mapper_df[' LON'].astype(str)\n        #grid_to_window_mapper_df = grid_to_window_mapper_df.set_index(['LAT', ' LON'])\n        #grid_to_window_mapper_df.set_index('Combined_Location', inplace=True)\n\n        #print(\"original_df columns: \", original_df.columns)\n        #print(\"original_df index: \", original_df.index)\n        #print(\"grid_to_window_mapper_df columns: \", grid_to_window_mapper_df.columns)\n        #print(\"grid_to_window_mapper_df index: \", grid_to_window_mapper_df.index)\n        #new_df = original_df.apply(add_window_grid_cells, axis=1, args=(original_df, grid_to_window_mapper_df))\n        # Assuming df is a Dask DataFrame\n        #ddf = dd.from_pandas(original_df, npartitions=5)\n        # Adjust the number of partitions as needed\n        # Use the map function\n        #new_df = ddf.map_partitions(apply_dask_partition, original_df = original_df, grid_to_window_mapper_df = grid_to_window_mapper_df).compute()\n        #new_df = ddf.apply(add_window_grid_cells, original_df = original_df, grid_to_window_mapper_df = grid_to_window_mapper_df, axis=1)\n\n        # Convert back to Pandas DataFrame\n        #new_df = new_df.compute()\n        #print(\"new_df.shape = \", new_df.shape)\n        #print(\"df.shape = \", df.shape)\n        #df[grid_to_window_mapper_df.columns] = new_df\n\n        #print(\"New time series dataframe: \", df.head())\n        return df\n\ndef prepare_testing_data_for_2_weeks_forecasting(target_date, current_start_day, current_prediction_output_folder):\n  \"\"\"\n  Prepare testing data for a 2-week forecasting model.\n\n  Parameters:\n    - target_date (str): The target date for forecasting.\n    - current_start_day (str): The current start day for fetching time series data.\n    - current_prediction_output_folder (str): The folder path for the prediction output.\n\n  Returns:\n    - X (pd.DataFrame): Features DataFrame for model input.\n    - y (pd.Series): Target Series for model output (prediction).\n\n  Assumes the existence of a function get_one_day_time_series_for_2_weeks_testing_data(target_date, current_start_day, current_prediction_output_folder)\n    to fetch time series data for the given target date and start day.\n  \"\"\"\n  # Assuming 'target' is the column to predict\n  target_col = 'FRP'\n  original_df = get_one_day_time_series_for_2_weeks_testing_data(target_date, current_start_day, current_prediction_output_folder)\n  df = original_df\n  print(\"Original df is created: \", original_df.shape)\n\n  #print(\"Lag/Shift the data for previous days' information\")\n  num_previous_days = 7  # Adjust the number of previous days to consider\n\n  # Drop rows with NaN values from the shifted columns\n  df_filled = df.fillna(-999)\n  print(\"Original df filled the na with -999 \")\n\n  # Define features and target\n  #X = df.drop([target_col, 'LAT', ' LON'], axis=1)\n  # Drop all the neighbor colums\n#   X = df.drop([target_col, 'LAT', ' LON', 'Nearest_1', 'Nearest_2',\n                 \n# 'Nearest_3', 'Nearest_4', 'Nearest_5', 'Nearest_6', 'Nearest_7',\n\n# 'Nearest_8', 'Nearest_9', 'Nearest_10', 'Nearest_11', 'Nearest_12',\n\n# 'Nearest_13', 'Nearest_14', 'Nearest_15', 'Nearest_16', 'Nearest_17',\n\n# 'Nearest_18', 'Nearest_19', 'Nearest_20', 'Nearest_21', 'Nearest_22',\n\n# 'Nearest_23', 'Nearest_24',], axis=1)\n  X = df.drop([target_col], axis=1)\n  y = df[target_col]\n  return X, y\n\nif __name__ == \"__main__\":\n  #training_end_date = \"20200715\"\n  #prepare_training_data(training_end_date)\n  output_folder_full_path = f'/groups/ESS3/zsun/firecasting/data/output/test_if_predicted_frp_used/20210714/'\n  prepare_testing_data_for_2_weeks_forecasting(\"20210714\", \"20210714\", output_folder_full_path)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "x8kqk7",
  "name" : "fc_install_dependencies",
  "description" : null,
  "code" : "#!/bin/bash\n# install all the required dependencies by python and other programs\n\n/home/zsun/anaconda3/bin/python -m pip install xgboost pickle5\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "wjq4fr",
  "name" : "fc_model_predict",
  "description" : null,
  "code" : "# create a ML model for wildfire emission forecasting\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score\nfrom sklearn.metrics import mean_squared_log_error, median_absolute_error, max_error\nimport warnings\nfrom datetime import datetime, timedelta\nfrom fc_model_creation import prepare_training_data\n\n\nimport pickle\n\nfrom fc_train_data_preprocess import read_original_txt_files, get_one_day_time_series_training_data\n\n# Suppress the specific warning\nwarnings.filterwarnings(\"ignore\", message=\"DataFrame is highly fragmented\")\n\n#model_path = \"/groups/ESS3/zsun/firecasting/model/fc_xgb_model_v2_one_month_202007.pkl\"\n#model_path = \"/groups/ESS3/zsun/firecasting/model/fc_xgb_model_v1_weighted_one_year_2020.pkl\"\nmodel_path = \"/groups/ESS3/zsun/firecasting/model/fc_xgb_model_v1_weighted_5_days_2020_maxdepth_8_linear_weights_100_slurm_test.pkl\"\noutput_folder_name = \"output_weighted_window_xgboost_2020_year_model\"\n#output_folder_name = \"output_xgboost_202007\"\n\n\ndef predict(start_date_str, end_date_str):\n  start_date = datetime.strptime(start_date_str, \"%Y%m%d\")\n  end_date = datetime.strptime(end_date_str, \"%Y%m%d\")\n\n  # Initialize a list to store the dates\n  dates_between = []\n  \n  # Load the saved model\n  with open(model_path, 'rb') as model_file:\n      loaded_model = pickle.load(model_file)\n\n  # create output folder\n  output_folder_full_path = f'/groups/ESS3/zsun/firecasting/data/output/{output_folder_name}/'\n  if not os.path.exists(output_folder_full_path):\n    os.makedirs(output_folder_full_path)\n      \n  # Iterate through the days between start and end dates\n  current_date = start_date\n  label = 0\n  while current_date <= end_date:\n    dates_between.append(current_date)\n    current_date += timedelta(days=1)\n    date_str = current_date.strftime(\"%Y%m%d\")\n    print(f\"generating prediction for {date_str}\")\n    \n    X, y = prepare_training_data(date_str)\n    # Make predictions\n    y_pred = loaded_model.predict(X)\n    y_pred[y_pred < 0] = 0\n\n    #print(\"y_pred : \", y_pred)\n\n    # merge the input and output into one df\n    #print(\"X_test shape: \", X.shape)\n    y_pred_df = pd.DataFrame(y_pred, columns=[\"Predicted_FRP\"])\n\n    #print(\"y_pred_df shape: \", y_pred_df.shape)\n    #print(\"y_pred_df head: \", y_pred_df.head())\n\n    #merged_df = X.join(y_pred_df)\n    #merged_df = pd.concat([X, y_pred_df], axis=1)\n    merged_df = X\n    merged_df[\"Predicted_FRP\"] = y_pred\n\n    #print(\"merged_df shape: \", merged_df.shape)\n    #print(\"the final merged df is: \", merged_df.head())\n\n    # save the df to a csv for plotting\n    merged_df.to_csv(f'/groups/ESS3/zsun/firecasting/data/output/{output_folder_name}/'\n              f'firedata_{date_str}_predicted.txt',\n              index=False)\n\n    # Calculate metrics\n    y_test = y\n    mae = mean_absolute_error(y_test, y_pred)\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = mean_squared_error(y_test, y_pred, squared=False)\n    r2 = r2_score(y_test, y_pred)\n    explained_var = explained_variance_score(y_test, y_pred)\n    msle = mean_squared_log_error(y_test, y_pred)\n    medae = median_absolute_error(y_test, y_pred)\n    max_err = max_error(y_test, y_pred)\n\n    # Print the metrics\n    lines_to_write = [f\"Mean Absolute Error (MAE): {mae}\",\n                      f\"Mean Squared Error (MSE): {mse}\",\n                      f\"Root Mean Squared Error (RMSE): {rmse}\",\n                      f\"R-squared (R2) Score: {r2}\",\n                      f\"Explained Variance Score: {explained_var}\",\n                      f\"Mean Squared Log Error (MSLE): {msle}\",\n                      f\"Median Absolute Error (MedAE): {medae}\",\n                      f\"Max Error: {max_err}\",\n                      f\"Mean: {y.mean()}\",\n                      f\"Median: {y.median()}\",\n                      f\"Standard Deviation: {y.std()}\",\n                      f\"Minimum: {y.min()}\",\n                      f\"Maximum: {y.max()}\",\n                      f\"Count: {y.count()}\"]\n    \n    print(lines_to_write)\n    # Specify the file path where you want to save the lines\n    metric_file_path = f'/groups/ESS3/zsun/firecasting/data/output/{output_folder_name}/metrics_{date_str}_predicted.txt'\n\n    # Open the file in write mode and write the lines\n    with open(metric_file_path, 'w') as file:\n        for line in lines_to_write:\n            file.write(line + '\\n')  # Add a newline character at the end of each line\n    print(f\"Metrics saved to {metric_file_path}\")\n\n\nstart_date = \"20210714\"\nend_date = \"20210731\"\n\npredict(start_date, end_date)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "l4eb73",
  "name" : "fc_model_predict_2weeks",
  "description" : null,
  "code" : "\"\"\"\nWildfire Emission Forecasting\n\nThis script creates a machine learning model for wildfire emission forecasting and uses it to predict emissions\nfor a period of two weeks. The script loads a pre-trained model, processes input data, makes predictions, \nand calculates various metrics for each day within the two-week period.\n\nThe predicted emissions are saved in separate folders for each date, along with corresponding metrics.\n\nUsage:\n    python fc_model_predict_2weeks.py\n\nDependencies:\n    - Python 3.x\n    - pandas\n    - numpy\n    - scikit-learn\n    - xgboost\n    - datetime\n\n\"\"\"\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport dask\nfrom dask import delayed\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score, median_absolute_error, max_error\nimport warnings\nfrom datetime import datetime, timedelta\nimport pickle\nfrom fc_test_data_preparation import prepare_testing_data_for_2_weeks_forecasting\nfrom fc_train_data_preprocess import columns_to_check\nimport sys\nfrom fc_model_creation import get_model_paths, chosen_input_columns, model_type, TabNetHandler, LightGBMHandler\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom sklearn.impute import SimpleImputer\n\nforecasting_days = 7\n\n# Suppress the specific warning\nwarnings.filterwarnings(\"ignore\", message=\"DataFrame is highly fragmented\")\n\nclass WildfireEmissionPredictor:\n    def __init__(self, model_path, chosen_input_columns, model_type, forecasting_days=7):\n        self.model_path = model_path\n        self.chosen_input_columns = chosen_input_columns\n        self.model_type = model_type\n        self.forecasting_days = forecasting_days\n\n        self.output_folder_name = f\"output_{self.model_type}_window_{self.forecasting_days}_days_4yrs\"\n\n        self.model_handler = self.get_model_handler()\n        self.model_handler.load_model(self.model_path)\n\n    def get_model_handler(self):\n        if self.model_type == \"tabnet\":\n            return TabNetHandler()\n        else:\n            return LightGBMHandler()\n\n    def check_model_is_tabnet(self, loaded_model):\n        if isinstance(loaded_model, TabNetRegressor):\n            print(\"Loaded model is a TabNet model.\")\n            return True\n        else:\n            print(\"Loaded model is not a TabNet model.\")\n            return False\n\n    def remove_element_by_value(self, input_list, value_to_remove):\n        if value_to_remove in input_list:\n            input_list.remove(value_to_remove)\n        return input_list\n\n    def filling_missing_value(self, df):\n        num_imputer = SimpleImputer(strategy='mean')\n        print(\"missing value filled with mean values\")\n        # Apply the imputer to fill missing values\n        df_imputed = num_imputer.fit_transform(df)\n        # Convert the result back to a DataFrame\n        df_imputed = pd.DataFrame(df_imputed, columns=df.columns, index=df.index)\n        return df_imputed\n\n    def predict_single_day_in_the_2weeks(self, single_day_current_date_str, date_str, specific_date_result_folder):\n        print(\"Predicting: \", single_day_current_date_str)\n\n        X, y = prepare_testing_data_for_2_weeks_forecasting(single_day_current_date_str, date_str, specific_date_result_folder)\n        print(f\"X and y are loaded into memory for {single_day_current_date_str}\")\n\n        direct_X = X[self.chosen_input_columns]\n\n        direct_X = self.filling_missing_value(direct_X)\n        print(\"direct_X = \", direct_X.head())\n        y_pred = self.model_handler.predict(direct_X)\n        print(f\"Prediction for {single_day_current_date_str} of start day {date_str} is finished\")\n\n        y_pred[y_pred < -2.0] = -2.0\n        y_pred[y_pred > 4.05] = 4.05\n\n        y_pred_df = pd.DataFrame(y_pred, columns=[\"Predicted_FRP\"])\n\n        print(\"After filtering out max/min: \", y_pred_df[\"Predicted_FRP\"].describe())\n\n        y_pred_df[\"y_pred_inversed\"] = np.power(10, y_pred_df[\"Predicted_FRP\"]) - 1e-2\n\n        merged_df = X\n        merged_df[\"Predicted_FRP\"] = y_pred_df[\"y_pred_inversed\"]\n\n        new_columns_to_check = self.remove_element_by_value(columns_to_check, \"FRP\")\n\n        def update_FRP(row):\n            if row['VPD'] < 0 or row['HT'] < 0 or row['Predicted_FRP'] < 0:\n                return 0\n            if (row[columns_to_check] == 0).all():\n                return 0\n            return row['Predicted_FRP']\n\n        merged_df['Predicted_FRP'] = merged_df.apply(update_FRP, axis=1)\n\n        print(\"Final saved predicted values: \", merged_df['Predicted_FRP'].describe())\n\n        predict_file = f'{specific_date_result_folder}/firedata_{single_day_current_date_str}_predicted.txt'\n        merged_df.to_csv(predict_file, index=False)\n        print(f\"Prediction results are saved to {predict_file}\")\n\n        y_test = y\n        y_pred = merged_df['Predicted_FRP']\n\n        mask = (y_pred > 10) & (y_test > 10)\n        filtered_y_pred = y_pred[mask]\n        filtered_y_test = y_test[mask]\n\n        if filtered_y_pred.size == 0 or filtered_y_test.size == 0:\n            return\n\n        mae = mean_absolute_error(filtered_y_test, filtered_y_pred)\n        mse = mean_squared_error(filtered_y_test, filtered_y_pred)\n        rmse = mean_squared_error(filtered_y_test, filtered_y_pred, squared=False)\n        r2 = r2_score(filtered_y_test, filtered_y_pred)\n        explained_var = explained_variance_score(filtered_y_test, filtered_y_pred)\n        medae = median_absolute_error(filtered_y_test, filtered_y_pred)\n        max_err = max_error(filtered_y_test, filtered_y_pred)\n\n        lines_to_write = [f\"Mean Absolute Error (MAE): {mae}\",\n                          f\"Mean Squared Error (MSE): {mse}\",\n                          f\"Root Mean Squared Error (RMSE): {rmse}\",\n                          f\"R-squared (R2) Score: {r2}\",\n                          f\"Explained Variance Score: {explained_var}\",\n                          f\"Median Absolute Error (MedAE): {medae}\",\n                          f\"Max Error: {max_err}\",\n                          f\"Mean: {y.mean()}\",\n                          f\"Median: {y.median()}\",\n                          f\"Standard Deviation: {y.std()}\",\n                          f\"Minimum: {y.min()}\",\n                          f\"Maximum: {y.max()}\",\n                          f\"Count: {y.count()}\"]\n\n        print(lines_to_write)\n        metric_file_path = f'{specific_date_result_folder}/metrics_{single_day_current_date_str}_predicted.txt'\n\n        with open(metric_file_path, 'w') as file:\n            for line in lines_to_write:\n                file.write(line + '\\n')\n        print(f\"Metrics saved to {metric_file_path}\")\n\n    def predict_2weeks_for_one_day(self, date_str, current_date, output_folder_full_path):\n        specific_date_result_folder = f\"{output_folder_full_path}/{date_str}\"\n        if not os.path.exists(specific_date_result_folder):\n            os.makedirs(specific_date_result_folder)\n            print(f\"Created folder for specific date: {specific_date_result_folder}\")\n\n        single_day_current_date = current_date\n        single_day_predict_end_date = single_day_current_date + timedelta(days=self.forecasting_days)\n        print(\"single_day_current_date = \", single_day_current_date)\n        print(\"single_day_predict_end_date = \", single_day_predict_end_date)\n\n        while single_day_current_date < single_day_predict_end_date:\n            single_day_current_date_str = single_day_current_date.strftime('%Y%m%d')\n            self.predict_single_day_in_the_2weeks(single_day_current_date_str, date_str, specific_date_result_folder)\n            single_day_current_date += timedelta(days=1)\n\n    def predict_2weeks(self, start_date_str, end_date_str):\n        print(f\"The model in use is {self.model_path}\")\n\n        start_date = datetime.strptime(start_date_str, \"%Y%m%d\")\n        end_date = datetime.strptime(end_date_str, \"%Y%m%d\")\n\n        output_folder_full_path = f'/groups/ESS3/zsun/firecasting/data/output/{self.output_folder_name}/'\n        if not os.path.exists(output_folder_full_path):\n            os.makedirs(output_folder_full_path)\n\n        current_date = start_date\n        while current_date <= end_date:\n            print(\"Current date: \", current_date)\n            date_str = current_date.strftime(\"%Y%m%d\")\n            self.predict_2weeks_for_one_day(date_str, current_date, output_folder_full_path)\n            current_date += timedelta(days=1)\n\nif __name__ == \"__main__\":\n    start_date = \"20210714\"\n    end_date = \"20210715\"\n    model_paths = get_model_paths(model_type)\n    predictor = WildfireEmissionPredictor(\n        model_paths[\"single_giant\"], \n        chosen_input_columns, \n        model_type)\n    predictor.predict_2weeks(start_date, end_date)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "pv7d8l",
  "name" : "fc_model_creation_train_slurm",
  "description" : null,
  "code" : "#!/bin/bash\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"fc_model_creation_train_slurm_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J fc_model_creation_train_slurm       # Job name\n#SBATCH --account=qtong\n#SBATCH --qos=qtong             #\n#SBATCH --partition=contrib     # partition (queue): debug, interactive, contrib, normal, orc-test\n#SBATCH --time=24:00:00         # walltime\n#SBATCH --nodes=1               # Number of nodes I want to use, max is 15 for lin-group, each node has 48 cores\n#SBATCH --ntasks-per-node=8    # Number of MPI tasks, multiply number of nodes with cores per node. 2*48=96\n#SBATCH --mail-user=zsun@gmu.edu    #Email account\n#SBATCH --mail-type=FAIL           #When to email\n#SBATCH --mem=150G\n#SBATCH --cores-per-socket=4\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n\nset echo\numask 0027\n\n# to see ID and state of GPUs assigned\nnvidia-smi\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython -u << INNER_EOF\n\nfrom fc_model_creation import WildfireModelTrainer, chosen_input_columns, model_type, get_model_paths, folder_path, training_data_folder\n\nmodel_type = \"lightgbm\"  # Can be 'lightgbm' or 'tabnet'\nmodel_paths = get_model_paths(model_type)\nprint(f\"training model: {model_type}\")\ntrainer = WildfireModelTrainer(\n    model_type=model_type,\n    training_data_folder=training_data_folder, \n    chosen_input_columns=chosen_input_columns\n)\n\nstart_date_str = \"20160110\"\nend_date_str = \"20191231\"\n\ntrainer.train_model(\n    start_date_str, end_date_str, \n    folder_path, model_paths, \n    fire_size_threshold=1, region_dividing_longitude=-100\n)\n\n# trainer.train_model_on_one_file(\n#     start_date_str=start_date_str,\n#     end_date_str=end_date_str,\n#     training_csv_path=f\"{training_data_folder}/giant_few_shot_samples/stratified_{start_date_str}_{end_date_str}.csv\",\n#     model_paths=model_paths,\n#     fire_size_threshold=300,\n#     region_dividing_longitude=-100,\n# )\n\n# trainer.stratified_sampling(\n#     start_date_str,\n#     end_date_str,\n#     folder_path,\n#     model_paths,\n#     giant_output_file=f\"{training_data_folder}/giant_few_shot_samples/stratified_{start_date_str}_{end_date_str}.csv\",\n#     fire_size_threshold=1,\n#     region_dividing_longitude=-100,\n# )\n\nprint(f\"Training completed and models saved to {model_paths[model_type]}\")\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nfile_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\nprevious_content=$(<\"${file_name}\")\nexit_code=0\nwhile true; do\n    # Capture the current content\n    file_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\n    current_content=$(<\"${file_name}\")\n\n    # Compare current content with previous content\n    diff_result=$(diff <(echo \"$previous_content\") <(echo \"$current_content\"))\n    # Check if there is new content\n    if [ -n \"$diff_result\" ]; then\n        echo \"$diff_result\"\n    fi\n    # Update previous content\n    previous_content=\"$current_content\"\n\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        exit_code=1;\n        break;\n    fi\n    sleep 100  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\n#find /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n\njob_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\nif [[ $job_status != *\"COMPLETED\"* ]]; then\n    exit 1\nfi\n\nexit $exit_code\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "xku47i",
  "name" : "plot_results",
  "description" : null,
  "code" : "# traverse the output folder and create a PNG for every day\n# this doesn't use parallelization at all so it will be slow. \nimport os\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\nfrom matplotlib.cm import ScalarMappable\nimport numpy as np\nfrom fc_model_predict_2weeks import WildfireEmissionPredictor\nimport rasterio\nfrom rasterio.transform import from_origin\nfrom rasterio.enums import Resampling\nfrom scipy.interpolate import griddata\nfrom fc_model_creation import get_model_paths, chosen_input_columns, model_type, TabNetHandler, LightGBMHandler\n\nmodel_type = \"lightgbm\"\nmodel_paths = get_model_paths(model_type)\npredictor = WildfireEmissionPredictor(\n    \"/groups/ESS3/zsun/firecasting/model/fc_lightgbm_single_giant.pkl_20160110_20191231_20243108220215.pkl\",\n    chosen_input_columns, \n    model_type)\noutput_folder = f\"/groups/ESS3/zsun/firecasting/data/output/{predictor.output_folder_name}/\"\nsample_lat_lon_csv = \"/groups/ESS3/zsun/firecasting/data/others/sample_lat_lon.csv\"\n\n\ndef save_predicted_frp_to_standard_netcdf(csv_file, sample_lat_lon_df):\n    \"\"\"\n    Get the ML results ready for downstream model inputs\n    \"\"\"\n    \n    pass\n\n\ndef save_predicted_frp_to_geotif(csv_file, sample_lat_lon_df):\n    \"\"\"\n    Get the ML results ready for public download and access\n    \"\"\"\n    if os.path.exists(f'{csv_file}_output.tif'):\n      print(f'{csv_file}_output.tif exists. skipping..')\n      return\n    \n    # Read CSV into GeoDataFrame\n    df = pd.read_csv(csv_file)\n    if 'LAT' not in df.columns:\n      # Merge 'lat' and 'lon' columns from df2 into df1\n      df[\"LAT\"] = sample_lat_lon_df[\"LAT\"]\n      df[\"LON\"] = sample_lat_lon_df[\"LON\"]\n\n    # Create GeoDataFrame with Point geometries\n    #geometry = [Point(lon, lat) for lon, lat in zip(df[' LON'], df['LAT'])]\n    #gdf = gpd.GeoDataFrame(df, geometry=geometry, crs='EPSG:4326')\n    \n    # Create a GeoDataFrame from the DataFrame\n    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df[\"LON\"], df[\"LAT\"]), crs='EPSG:4326')\n\t\n    # Set up rasterization parameters\n    # Define pixel size and latitude/longitude bounds\n    pixel_size = 0.01  # Adjust as needed\n    min_lat, max_lat = gdf['LAT'].min(), gdf['LAT'].max()\n    min_lon, max_lon = gdf['LON'].min(), gdf['LON'].max()\n    \n    print(\"min_lat = \", min_lat)\n    print(\"max_lat = \", max_lat)\n    print(\"min_lon = \", min_lon)\n    print(\"max_lon = \", max_lon)\n\n    # Calculate width and height based on pixel size\n    width = int((max_lon - min_lon) / pixel_size)\n    height = int((max_lat - min_lat) / pixel_size)\n    \n    print(\"width = \", width)\n    print(\"height = \", height)\n    \n    # Create a regular grid using numpy\n    xi = np.linspace(min_lon, max_lon, width)\n    yi = np.linspace(max_lat, min_lat, height)\n    xi, yi = np.meshgrid(xi, yi)\n    \n    # print(\"xi = \", xi)\n    # print(\"yi = \", yi)\n\n    # Interpolate data onto the regular grid\n    zi = griddata((gdf['LON'], gdf['LAT']), gdf['Predicted_FRP'], (xi, yi), method='linear')\n\n    # Create a GeoTIFF from the interpolated data\n    with rasterio.open(f'{csv_file}_output.tif', \n                       'w', \n                       driver='GTiff', \n                       height=height, \n                       width=width,\n                       count=1, \n                       dtype='float32', \n                       crs='EPSG:4326',\n                       transform=from_origin(min_lon, \n                                             max_lat, \n                                             pixel_size, \n                                             pixel_size)) as dst:\n        dst.write(zi, 1)\n\n    print(f\"GeoTIFF file created: {csv_file}_output.tif\")\n    \n\ndef plot_png(file_path, sample_lat_lon_df):\n    \n    res_png_path = f\"{file_path}.png\"\n    if os.path.exists(res_png_path):\n      print(f'{res_png_path} exists. skipping..')\n      return\n    \n    # Read CSV into a DataFrame\n    df = pd.read_csv(file_path)\n    # print(df.head())\n\n    if 'LAT' not in df.columns:\n      # Merge 'lat' and 'lon' columns from df2 into df1\n      df[\"LAT\"] = sample_lat_lon_df[\"LAT\"]\n      df[\"LON\"] = sample_lat_lon_df[\"LON\"]\n\n    real_col_num = len(df.columns) - 2\n    num_rows = int(np.ceil(np.sqrt(real_col_num)))\n    num_cols = int(np.ceil(real_col_num / num_rows))\n\n    # Create a figure and axis objects\n    fig, axs = plt.subplots(num_rows, num_cols, figsize=(28, 24))\n    # Flatten the axs array if it's more than 1D\n    axs = np.array(axs).flatten()\n    \n    for i in range(len(df.columns)-2):\n        col_name = df.columns[i]\n        \n        if col_name in [\"LAT\", \"LON\"]:\n          continue\n        \n        ax = axs[i]\n        # Create a scatter plot using two columns from the DataFrame\n        cmap = plt.get_cmap('hot')  # You can choose a different colormap\n        sm = ScalarMappable(cmap=cmap)\n        if \"FRP\" in col_name or \"Near\" in col_name:\n          # Define the minimum and maximum values for the color scale\n          min_value = 0  # Set your minimum value here\n          max_value = 50  # Set your maximum value here\n          # Create a color map and a normalization for the color scale\n          norm = Normalize(vmin=min_value, vmax=max_value)\n\n          ax.scatter(\n            df['LON'], \n            df['LAT'], \n            c=df[col_name], \n            cmap=cmap, \n            s=5, \n            norm=norm,\n            edgecolors='none'\n          )\n          # Create a scalar mappable for the color bar\n          sm = ScalarMappable(cmap=cmap, norm=norm)\n        else:\n          min_value = df[col_name].min()\n          if min_value == -999:\n            min_value = 0\n          \n          max_value = df[col_name].max()\n          #cmap = plt.get_cmap('coolwarm')  # You can choose a different colormap\n          new_norm = Normalize(vmin=min_value, vmax=max_value)\n          sm = ScalarMappable(cmap=cmap, norm=new_norm)\n          ax.scatter(\n            df['LON'], \n            df['LAT'], \n            c=df[col_name], \n            cmap=cmap, \n            norm=new_norm,\n            s=5,\n            edgecolors='none'\n          )\n          sm.set_array([])  # Set an empty array for the color bar\n\n        # Set the color bar's minimum and maximum values\n        # Add a color bar to the plot\n        color_bar = plt.colorbar(sm, orientation='horizontal', ax=ax)\n\n        # Set the color bar's minimum and maximum values using vmin and vmax\n        color_bar.set_ticks([min_value, max_value])\n        color_bar.set_ticklabels([min_value, max_value])\n\n        ax.set_title(f'{col_name}')\n\n        # Add labels and legend\n        #ax.set_xlabel('Longitude')\n        #ax.set_ylabel('Latitude')\n\n    plt.tight_layout()\n\n    \n    plt.savefig(res_png_path)\n    print(f\"test image is saved at {res_png_path}\")\n    plt.close()\n    \n\ndef plot_images():\n    # List all CSV files in the directory\n    \n    csv_files = []\n    for root, dirs, files in os.walk(output_folder):\n        for file in files:\n            if file.startswith(\"firedata_\") and file.endswith(\".txt\"):\n                csv_files.append(os.path.join(root, file))\n        \n    \n    sample_lat_lon_df = pd.read_csv(sample_lat_lon_csv)\n    \n    # Iterate through each CSV file\n    for file_path in csv_files:\n        # Construct the full file path\n        #file_path = os.path.join(output_folder, csv_file)\n        plot_png(file_path, sample_lat_lon_df)\n        save_predicted_frp_to_geotif(file_path, sample_lat_lon_df)\n        save_predicted_frp_to_standard_netcdf(file_path, sample_lat_lon_df)\n        \n\n    print(\"All done\")\n\n    \nif __name__ == \"__main__\":\n    plot_images()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "dp0hiw",
  "name" : "explain_model_results",
  "description" : null,
  "code" : "import os\nimport pickle\nimport matplotlib.pyplot as plt\nfrom fc_model_creation import model_path\n\n# get one prediction and reverse inference the model and get explanation for that prediction\n\n# Explanable AI - SHAP\n\n# target_predict_file = \"/groups/ESS3/zsun/firecasting/data/output/output_xgboost_2020_two_months/20210714/firedata_20210714_predicted.txt\"\n\n#model_path=\"/groups/ESS3/zsun/firecasting/model/fc_xgb_model_v1_weighted_5_days_2020_maxdepth_8_linear_weights_100_slurm_test.pkl\"\n\n# # Load the saved model\n# with open(model_path, 'rb') as model_file:\n#   loaded_model = pickle.load(model_file)\n\n# df = pd.read_csv(target_predict_file)\n  \n# X, y = prepare_testing_data_for_2_weeks_forecasting(single_day_current_date_str, date_str, specific_date_result_folder)\n\n# # Make predictions\n# y_pred = loaded_model.predict(X)\n\n# calculate feature importance - indirect evaluation\n\n\n    \ndef plot_feature_importance():\n  \n    # Load the saved model\n    with open(model_path, 'rb') as model_file:\n      loaded_model = pickle.load(model_file)\n    \n    feature_importances = loaded_model.feature_importances_\n    \n#     feature_names = ['FWI', 'VPD', 'HT', 'T', 'RH', 'U', 'V', 'P', 'RAIN', 'CAPE', 'ST',\n#  'SM', 'Nearest_1', 'Nearest_2', 'Nearest_3', 'Nearest_4', 'Nearest_5',\n#  'Nearest_6', 'Nearest_7', 'Nearest_8', 'Nearest_9', 'Nearest_10',\n#  'Nearest_11', 'Nearest_12', 'Nearest_13', 'Nearest_14', 'Nearest_15',\n#  'Nearest_16', 'Nearest_17', 'Nearest_18', 'Nearest_19', 'Nearest_20',\n#  'Nearest_21', 'Nearest_22', 'Nearest_23', 'Nearest_24',\n#  'FRP_1_days_ago', 'FRP_2_days_ago', 'FRP_3_days_ago', 'FRP_4_days_ago',\n#  'FRP_5_days_ago', 'FRP_6_days_ago', 'FRP_7_days_ago']\n\n    feature_names = ['LAT', 'LON', 'FWI', 'VPD', 'HT', 'T', 'RH', 'U', 'V', 'P', 'RAIN', 'CAPE', 'ST',\n'SM', 'Nearest_1', 'Nearest_2', 'Nearest_3', 'Nearest_4', 'Nearest_5',\n'Nearest_6', 'Nearest_7', 'Nearest_8', 'Nearest_9', 'Nearest_10',\n'Nearest_11', 'Nearest_12', 'Nearest_13', 'Nearest_14', 'Nearest_15',\n'Nearest_16', 'Nearest_17', 'Nearest_18', 'Nearest_19', 'Nearest_20',\n'Nearest_21', 'Nearest_22', 'Nearest_23', 'Nearest_24', 'Land_Use', 'VCI_AVE', 'TCI_AVE', 'VHI_AVE', 'VCI_TOT', 'TCI_TOT', 'VHI_TOT',\n'FWI_1_days_ago', 'VPD_1_days_ago', 'P_1_days_ago', 'FRP_1_days_ago',\n'FWI_2_days_ago', 'VPD_2_days_ago', 'P_2_days_ago', 'FRP_2_days_ago',\n'FWI_3_days_ago', 'VPD_3_days_ago', 'P_3_days_ago', 'FRP_3_days_ago',\n'FWI_4_days_ago', 'VPD_4_days_ago', 'P_4_days_ago', 'FRP_4_days_ago',\n'FWI_5_days_ago', 'VPD_5_days_ago', 'P_5_days_ago', 'FRP_5_days_ago',\n'FWI_6_days_ago', 'VPD_6_days_ago', 'P_6_days_ago', 'FRP_6_days_ago',\n'FWI_7_days_ago', 'VPD_7_days_ago', 'P_7_days_ago', 'FRP_7_days_ago']\n\n# ['FWI', 'VPD', 'HT', 'T', 'RH', 'U', 'V', 'P', 'RAIN', 'CAPE', 'ST',\n# > 'SM', 'Nearest_1', 'Nearest_2', 'Nearest_3', 'Nearest_4', 'Nearest_5',\n# > 'Nearest_6', 'Nearest_7', 'Nearest_8', 'Nearest_9', 'Nearest_10',\n# > 'Nearest_11', 'Nearest_12', 'Nearest_13', 'Nearest_14', 'Nearest_15',\n# > 'Nearest_16', 'Nearest_17', 'Nearest_18', 'Nearest_19', 'Nearest_20',\n# > 'Nearest_21', 'Nearest_22', 'Nearest_23', 'Nearest_24', 'Land_Use',\n# > 'VCI_AVE', 'TCI_AVE', 'VHI_AVE', 'VCI_TOT', 'TCI_TOT', 'VHI_TOT',\n# > 'FWI_1_days_ago', 'VPD_1_days_ago', 'P_1_days_ago', 'FRP_1_days_ago',\n# > 'FWI_2_days_ago', 'VPD_2_days_ago', 'P_2_days_ago', 'FRP_2_days_ago',\n# > 'FWI_3_days_ago', 'VPD_3_days_ago', 'P_3_days_ago', 'FRP_3_days_ago',\n# > 'FWI_4_days_ago', 'VPD_4_days_ago', 'P_4_days_ago', 'FRP_4_days_ago',\n# > 'FWI_5_days_ago', 'VPD_5_days_ago', 'P_5_days_ago', 'FRP_5_days_ago',\n# > 'FWI_6_days_ago', 'VPD_6_days_ago', 'P_6_days_ago', 'FRP_6_days_ago',\n# > 'FWI_7_days_ago', 'VPD_7_days_ago', 'P_7_days_ago', 'FRP_7_days_ago',\n# > 'Predicted_FRP']\n\n#     [\n# #       'LAT', ' LON', \n#       'FWI', 'VPD', 'HT', 'T', 'RH', 'U', 'V', 'P',\n#  'RAIN', 'CAPE', 'ST', 'SM', 'FRP_1_days_ago', 'FRP_2_days_ago',\n#  'FRP_3_days_ago', 'FRP_4_days_ago', 'FRP_5_days_ago',\n#  'FRP_6_days_ago', 'FRP_7_days_ago', 'Nearest_1', 'Nearest_2',\n#  'Nearest_3', 'Nearest_4', 'Nearest_5', 'Nearest_6', 'Nearest_7',\n#  'Nearest_8', 'Nearest_9', 'Nearest_10', 'Nearest_11', 'Nearest_12',\n#  'Nearest_13', 'Nearest_14', 'Nearest_15', 'Nearest_16', 'Nearest_17',\n#  'Nearest_18', 'Nearest_19', 'Nearest_20', 'Nearest_21', 'Nearest_22',\n#  'Nearest_23', 'Nearest_24']\n    \n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(12, 12))\n    print(feature_names)\n    print(feature_importances)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    file_name = os.path.basename(model_path)\n    new_png_path = f'/groups/ESS3/zsun/firecasting/data/output/importance_summary_plot_{file_name}.png'\n    plt.savefig(new_png_path)\n    print(\"new_png_path = \", new_png_path)\n    \n    \nif __name__ == \"__main__\":\n    plot_feature_importance()\n\n# explain why it makes that decision (look into the model itself) - direct evaluation\n\n\n# local explanation \n\n\n# global explanation\n\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "b3cx6j",
  "name" : "upload_prediction_to_website",
  "description" : null,
  "code" : "#!/bin/bash\n\n# move the generated PNG images and metrics to the public website folders\necho \"Copying FRP predicted png files to public server..\"\n#scp -i /home/zsun/.ssh/id_geobrain_no.pem /groups/ESS3/zsun/cmaq/ai_results/evaluation/* zsun@129.174.131.229:/var/www/html/cmaq_site/evaluation/\nrsync -u -e \"ssh -i /home/zsun/.ssh/geobrain_upload_fire.pem\" -avz /groups/ESS3/zsun/firecasting/data/output/output_lightgbm_window_7_days_forecasting_with_new_yunyao_window_time_series_landuse_vhi_latlon_10_vars/ chetana@129.174.131.229:/var/www/html/wildfire_site/data/\n\nrsync -u -e \"ssh -i /home/zsun/.ssh/geobrain_upload_fire.pem\" -avz  /groups/ESS3/zsun/firecasting/data/output/importance_summary_plot_* chetana@129.174.131.229:/var/www/html/wildfire_site/model/\n\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "k84mqm",
  "name" : "fc_train_data_preprocess_slurm",
  "description" : null,
  "code" : "#!/bin/bash\n\n# This file is dedicated to prepare the training data.\n\n# 1) we need a complete rewrite of this process.\n# 2) separate the training data preparation and testing data preparation.\n# 3) All the share functions should go to the util process. \n\necho \"start to run test_data_slurm_generated.sh\"\npwd\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"fc_model_data_preprocess_slurm_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J fc_model_data_preprocessing       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 12               # Number of CPUs per task (threads)\n#SBATCH --mem=20G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-10:00         # Runtime in D-HH:MM format\n## Slurm can send you updates via email\n#SBATCH --mail-type=FAIL  # BEGIN,END,FAIL         # ALL,NONE,BEGIN,END,FAIL,REQUEUE,..\n#SBATCH --mail-user=zsun@gmu.edu     # Put your GMU email address here\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython -u << INNER_EOF\n\nfrom fc_train_data_preprocess import prepare_training_data\n\nfrom datetime import datetime, timedelta\nimport pandas as pd\n\nif __name__ == \"__main__\":\n  # this is today, and we want to use all the meteo data of today and FRP data of day -7 - yesterday to predict today's FRP. \n  \n\n  # Start date\n  start_date = datetime(2020, 9, 1)\n\n  # End date\n  end_date = datetime(2020, 9, 7)\n\n  # Define the step size for traversal\n  step = timedelta(days=1)\n  \n  training_data_folder = \"/groups/ESS3/zsun/firecasting/data/train/all_cells_new_5yrs/\"\n\n  # Traverse the dates\n  current_date = start_date\n  while current_date <= end_date:\n      print(\"Preparing for date : \", current_date.strftime('%Y%m%d'))  # Print date in YYYYMMDD format\n      \n      training_end_date = current_date.strftime('%Y%m%d')\n      #training_end_date = \"20201030\" # the last day of the 7 day history\n      prepare_training_data(training_end_date, training_data_folder)\n      current_date += step\n      \n      file_path = \"/groups/ESS3/zsun/firecasting/data/train/all_cells_new_5yrs/20200715_time_series_with_window.csv\"\n\n      #file_path = \"/groups/ESS3/yli74/data/AI_Emis/firedata/firedata_20200715.txt\"\n\n      df = pd.read_csv(file_path)\n\n      print(df.head())\n\n      # Assuming you want to calculate statistics of a column named 'column_name'\n      column_name = 'FRP'\n\n      # Basic statistics\n      stats = df[column_name].describe()\n      \n      if df[column_name].max() == 0:\n        print(\"The maximum value of the column is zero.\")\n        raise Exception(\"The maximum value of the column is zero.\")\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nfile_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\nprevious_content=$(<\"${file_name}\")\nwhile true; do\n    # Capture the current content\n    file_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\n    current_content=$(<\"${file_name}\")\n\n    # Compare current content with previous content\n    diff_result=$(diff <(echo \"$previous_content\") <(echo \"$current_content\"))\n    # Check if there is new content\n    if [ -n \"$diff_result\" ]; then\n        echo \"$diff_result\"\n    fi\n    # Update previous content\n    previous_content=\"$current_content\"\n\n\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\n#find /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "55wou8",
  "name" : "fc_model_predict_2weeks_slurm",
  "description" : null,
  "code" : "#!/bin/bash\n\necho \"start to run fc_model_predict_2weeks.sh\"\npwd\n\n# clean up the old log\n> /home/zsun/fc_model_predict_2weeks.out\n> /home/zsun/fc_model_predict_2weeks.err\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"fc_model_predict_2weeks_slurm_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J fc_model_predict_2weeks       # Job name\n#SBATCH --account=qtong\n#SBATCH --qos=qtong             #\n#SBATCH --partition=contrib     # partition (queue): debug, interactive, contrib, normal, orc-test\n#SBATCH --time=22:00:00         # walltime\n#SBATCH --nodes=1               # Number of nodes I want to use, max is 15 for lin-group, each node has 48 cores\n#SBATCH --ntasks-per-node=4    # Number of MPI tasks, multiply number of nodes with cores per node. 2*48=96\n#SBATCH --mail-user=zsun@gmu.edu    #Email account\n#SBATCH --mail-type=FAIL           #When to email\n#SBATCH --mem=20G\n#SBATCH --cores-per-socket=4\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\n# Call the Python script using process substitution\npython -u << INNER_EOF\n\nfrom fc_model_predict_2weeks import  WildfireEmissionPredictor\nfrom fc_model_creation import get_model_paths, chosen_input_columns, model_type, TabNetHandler, LightGBMHandler\n\nstart_date = \"20210701\"\nend_date = \"20210831\"\n\nmodel_type = \"lightgbm\"\nmodel_paths = get_model_paths(model_type)\npredictor = WildfireEmissionPredictor(\n    \"/groups/ESS3/zsun/firecasting/model/fc_lightgbm_single_giant.pkl_20160110_20191231_20243108220215.pkl\",\n    chosen_input_columns, \n    model_type)\npredictor.predict_2weeks(start_date, end_date)\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nfile_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\nprevious_content=$(cat file_name)\nexit_code=0\nwhile true; do\n    # Capture the current content\\\n    #echo ${job_id}\n    file_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\n    #echo \"file_name=\"$file_name\n    current_content=$(<\"${file_name}\")\n    #echo \"current_content = \"$current_content\n\n    # Compare current content with previous content\n    diff_result=$(diff <(echo \"$previous_content\") <(echo \"$current_content\"))\n    # Check if there is new content\n    if [ -n \"$diff_result\" ]; then\n        # Print the newly added content\n        #echo \"New content added:\"\n        echo \"$diff_result\"\n        #echo \"---------------------\"\n    fi\n    # Update previous content\n    previous_content=\"$current_content\"\n\n\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    #echo \"job_status \"$job_status\n    #if [[ $job_status == \"JobState=COMPLETED\" ]]; then\n    #    break\n    #fi\n    if [[ $job_status == *\"COMPLETED\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    elif [[ $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        exit_code=1\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\nfind /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\ncat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n\nexit $exit_code\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "uhet3k",
  "name" : "fc_test_data_preprocess_slurm",
  "description" : null,
  "code" : "#!/bin/bash\n# This file is dedicated to prepare the testing data. \n\n# 1) we need a complete rewrite of this process.\n# 2) separate the training data preparation and testing data preparation.\n# 3) All the share functions should go to the util process. \n\necho \"start to run test_data_slurm_generated.sh\"\npwd\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"fc_model_data_preprocess_slurm_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J fc_model_data_preprocessing       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 12               # Number of CPUs per task (threads)\n#SBATCH --mem=50G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-01:00         # Runtime in D-HH:MM format\n## Slurm can send you updates via email\n#SBATCH --mail-type=FAIL  # BEGIN,END,FAIL         # ALL,NONE,BEGIN,END,FAIL,REQUEUE,..\n#SBATCH --mail-user=zsun@gmu.edu     # Put your GMU email address here\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython -u << INNER_EOF\n\nfrom fc_test_data_preparation import prepare_testing_data_for_2_weeks_forecasting\n\nif __name__ == \"__main__\":\n  #training_end_date = \"20200715\"\n  #prepare_training_data(training_end_date)\n  output_folder_full_path = f'/groups/ESS3/zsun/firecasting/data/output/test_if_predicted_frp_used/20210718/'\n  prepare_testing_data_for_2_weeks_forecasting(\"20210714\", \"20210714\", output_folder_full_path)\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nfile_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\nprevious_content=$(<\"${file_name}\")\nwhile true; do\n    # Capture the current content\n    file_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\n    current_content=$(<\"${file_name}\")\n\n    # Compare current content with previous content\n    diff_result=$(diff <(echo \"$previous_content\") <(echo \"$current_content\"))\n    # Check if there is new content\n    if [ -n \"$diff_result\" ]; then\n        echo \"$diff_result\"\n    fi\n    # Update previous content\n    previous_content=\"$current_content\"\n\n\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\n#find /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "b6x5kk",
  "name" : "data_preparation_utils",
  "description" : null,
  "code" : "import os\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# this file contains all the function that are required by both the training data preparation and testing data preparation.\n\n# Folder path containing the text files\nfolder_path = '/groups/ESS3/yli74/data/AI_Emis/firedata'  # The folder yunyao provided with two years of txt files\nmy_file_path = \"/groups/ESS3/zsun/firecasting/data/others/\"\ngrid_to_window_mapper_csv = f\"{my_file_path}/grid_cell_fixed_order_mapper.csv\"\ntraining_data_folder = \"/groups/ESS3/zsun/firecasting/data/train/\"\n\n\ndef create_grid_to_window_mapper(the_folder_path = folder_path):\n  if os.path.exists(grid_to_window_mapper_csv):\n    print(f\"The file '{grid_to_window_mapper_csv}' exists.\")\n  else:\n    # this function will find the nearest 24 pixels for one pixel\n    # we only start from 2\n    # choose any txt \n    # Replace 'path_to_folder' with the path to your folder containing text files\n    txt_folder_path = f'{the_folder_path}/*.txt'\n    import glob\n    # Get a list of text files in the folder\n    text_files = glob.glob(txt_folder_path)\n\n    # Choose the first text file (you can modify this to select any specific file)\n    file_to_read = text_files[0]\n\n    # Read the chosen text file into a DataFrame\n    df = pd.read_csv(file_to_read)  # Modify delimiter as needed\n    print(df.head())\n    # Convert all values in the DataFrame to numeric\n    df = df.applymap(pd.to_numeric, errors='coerce')\n\n    print(df.columns)\n\n    # Use groupby to get unique pairs of LAT and LON\n    # Use groupby to get unique pairs of LAT and LON\n    unique_pairs = df.groupby(['LAT', ' LON']).size().reset_index(name='Count')\n    unique_pairs_df = unique_pairs[['LAT', ' LON']]\n    print(\"unique_pairs = \", unique_pairs_df)\n    # find the nearest 24 pixels for every single pixels\n    # Create a KDTree using 'LAT' and 'LON' columns\n    from scipy.spatial import cKDTree\n    tree = cKDTree(unique_pairs_df[['LAT', ' LON']])\n\n    # Find the 24 nearest neighbors for each point\n    distances, indices = tree.query(unique_pairs_df, k=25)\n\n    print(\"distances = \", distances)\n\n    # Extract the nearest 24 neighbors (excluding the point itself)\n    nearest_24 = indices[:, 1:]\n    print(\"nearest_24 = \", nearest_24)\n    print(\"nearest_24.shape = \", nearest_24.shape)\n    \n    clockwise_indices = []\n    for neighbor_indices in nearest_24:\n      # Calculate angle of each neighbor with respect to current cell\n      angles = []\n      for neighbor_index in neighbor_indices[1:]:\n        neighbor_coords = unique_pairs_df.iloc[neighbor_index]\n        neighbor_coords[' LAT']\n        cell_coords[' LON']\n\n        # Sort neighbors based on angles in clockwise order\n        sorted_indices = [x for _, x in sorted(zip(angles, cell_indices[1:]))]\n\n        # Append the sorted indices to the clockwise_indices list\n        clockwise_indices.append(np.concatenate(([cell_indices[0]], sorted_indices)))\n\n\n    # Create column names for the new columns\n    new_columns = [f'Nearest_{i}' for i in range(1, 25)]\n\n    nearest_24_df = pd.DataFrame(nearest_24, columns=new_columns)\n\n    print(\"unique_pairs_df.shape: \", unique_pairs_df.shape)\n\n    # Merge the DataFrames row by row\n    result = pd.concat([unique_pairs_df.reset_index(drop=True), nearest_24_df.reset_index(drop=True)], axis=1)\n\n    print(result.head())\n    print(result.shape)\n\n    result.to_csv(grid_to_window_mapper_csv, index=False)\n    print(f\"grid to window mapper csv is saved to {grid_to_window_mapper_csv}\")\n    \n# create_grid_to_window_mapper()\n    ",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "rg5g1a",
  "name" : "plot_results_slurm",
  "description" : null,
  "code" : "#!/bin/bash\n\necho \"start to run plot_results.sh\"\npwd\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"plot_results_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J plot_results       # Job name\n#SBATCH --account=qtong\n#SBATCH --qos=qtong             #\n#SBATCH --partition=contrib     # partition (queue): debug, interactive, contrib, normal, orc-test\n#SBATCH --time=12:00:00         # walltime\n#SBATCH --nodes=1               # Number of nodes I want to use, max is 15 for lin-group, each node has 48 cores\n#SBATCH --ntasks-per-node=12    # Number of MPI tasks, multiply number of nodes with cores per node. 2*48=96\n#SBATCH --mail-user=zsun@gmu.edu    #Email account\n#SBATCH --mail-type=FAIL           #When to email\n#SBATCH --mem=18000M\n#SBATCH --cores-per-socket=8\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\n# Call the Python script using process substitution\npython -u << INNER_EOF\n\nfrom plot_results import plot_images\n\nplot_images()\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nfile_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\nprevious_content=$(cat file_name)\nexit_code=0\nwhile true; do\n    # Capture the current content\\\n    #echo ${job_id}\n    file_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\n    #echo \"file_name=\"$file_name\n    if [ -e \"$file_name\" ]; then\n      current_content=$(<\"${file_name}\")\n      #echo \"current_content = \"$current_content\n\n      # Compare current content with previous content\n      diff_result=$(diff <(echo \"$previous_content\") <(echo \"$current_content\"))\n      # Check if there is new content\n      if [ -n \"$diff_result\" ]; then\n          # Print the newly added content\n          #echo \"New content added:\"\n          echo \"$diff_result\"\n          #echo \"---------------------\"\n      fi\n      # Update previous content\n      previous_content=\"$current_content\"\n    fi\n\n\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    #echo \"job_status \"$job_status\n    #if [[ $job_status == \"JobState=COMPLETED\" ]]; then\n    #    break\n    #fi\n    if [[ $job_status == *\"COMPLETED\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    elif [[ $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        exit_code=1\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\n#find /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n\nexit $exit_code\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "k09hf7",
  "name" : "generate_mapfiles",
  "description" : null,
  "code" : "# Generate new map files for new geotifs\n\nimport os\nimport shutil\nimport re\nimport pandas as pd\nfrom datetime import datetime\n\nfrom plot_results import output_folder\n\n# Define a regular expression pattern to match the date in the filename\npattern = r\"\\d{4}\\d{2}\\d{2}\"\n\ndef create_mapserver_map_config(target_geotiff_file_path, force=False):\n  geotiff_file_name = os.path.basename(target_geotiff_file_path)\n  geotiff_dir_name = os.path.dirname(target_geotiff_file_path)\n  \n  match = re.search(pattern, geotiff_dir_name)\n\n  # Check if a match is found\n  if match:\n      eye_date_string = match.group()\n      print(\"Date:\", eye_date_string)\n  else:\n      print(\"No date found in the filename.\")\n      return f\"The folder's name {geotiff_dir_name} is wrong\"\n  \n  geotiff_mapserver_file_path = f\"{geotiff_dir_name}/{geotiff_file_name}.map\"\n  if not geotiff_file_name.endswith(\".tif\"):\n    print(f\"{geotiff_file_name} is not geotiff\")\n    return\n  \n  if os.path.exists(geotiff_mapserver_file_path) and not force:\n    print(f\"{geotiff_mapserver_file_path} already exists\")\n    return geotiff_mapserver_file_path\n  \n  # Use re.search to find the match\n  match = re.search(pattern, geotiff_file_name)\n\n  # Check if a match is found\n  if match:\n      date_string = match.group()\n      print(\"Date:\", date_string)\n  else:\n      print(\"No date found in the filename.\")\n      return f\"The file's name {target_geotiff_file} is wrong\"\n  \n#   Driver: GTiff/GeoTIFF\n# Files: firedata_20210725_predicted.txt_output.tif\n# Size is 6000, 2600\n# Coordinate System is:\n# GEOGCS[\"WGS 84\",\n#     DATUM[\"WGS_1984\",\n#         SPHEROID[\"WGS 84\",6378137,298.257223563,\n#             AUTHORITY[\"EPSG\",\"7030\"]],\n#         AUTHORITY[\"EPSG\",\"6326\"]],\n#     PRIMEM[\"Greenwich\",0],\n#     UNIT[\"degree\",0.0174532925199433],\n#     AUTHORITY[\"EPSG\",\"4326\"]]\n# Origin = (-126.000000000000000,50.500000000000000)\n# Pixel Size = (0.010000000000000,-0.010000000000000)\n# Metadata:\n#   AREA_OR_POINT=Area\n# Image Structure Metadata:\n#   INTERLEAVE=BAND\n# Corner Coordinates:\n# Upper Left  (-126.0000000,  50.5000000) (126d 0' 0.00\"W, 50d30' 0.00\"N)\n# Lower Left  (-126.0000000,  24.5000000) (126d 0' 0.00\"W, 24d30' 0.00\"N)\n# Upper Right ( -66.0000000,  50.5000000) ( 66d 0' 0.00\"W, 50d30' 0.00\"N)\n# Lower Right ( -66.0000000,  24.5000000) ( 66d 0' 0.00\"W, 24d30' 0.00\"N)\n# Center      ( -96.0000000,  37.5000000) ( 96d 0' 0.00\"W, 37d30' 0.00\"N)\n# Band 1 Block=6000x1 Type=Float32, ColorInterp=Gray\n  \n  mapserver_config_content = f\"\"\"\nMAP\n  NAME \"wildfiremap\"\n  STATUS ON\n  EXTENT -126 24.5 -66 50.5\n  SIZE 6000 2600\n  UNITS DD\n  SHAPEPATH \"/var/www/html/wildfire_site/data\"\n\n  PROJECTION\n    \"init=epsg:4326\"\n  END\n\n  WEB\n    IMAGEPATH \"/temp/\"\n    IMAGEURL \"/temp/\"\n    METADATA\n      \"wms_title\" \"Wildfire MapServer WMS\"\n      \"wms_onlineresource\" \"http://geobrain.csiss.gmu.edu/cgi-bin/mapserv?map=/var/www/html/wildfire_site/data/wildfire.map&\"\n      WMS_ENABLE_REQUEST      \"*\"\n      WCS_ENABLE_REQUEST      \"*\"\n      \"wms_srs\" \"epsg:5070 epsg:4326 epsg:3857\"\n    END\n  END\n\n\n  LAYER\n    NAME \"predicted_wildfire_{eye_date_string}_{date_string}\"\n    TYPE RASTER\n    STATUS DEFAULT\n    DATA \"/var/www/html/wildfire_site/data/{eye_date_string}/{geotiff_file_name}\"\n\n    PROJECTION\n      \"init=epsg:4326\"\n    END\n\n    METADATA\n      \"wms_include_items\" \"all\"\n    END\n    #PROCESSING \"SCALE=0.0,300.0\"\n    #PROCESSING \"SCALE_BUCKETS=15\"\n    PROCESSING \"NODATA=0\"\n    STATUS ON\n    DUMP TRUE\n    TYPE RASTER\n    OFFSITE 0 0 0\n    CLASSITEM \"[pixel]\"\n    TEMPLATE \"../template.html\"\n    INCLUDE \"../legend_wildfire.map\"\n  END\nEND\n\"\"\"\n  \n  with open(geotiff_mapserver_file_path, \"w\") as file:\n    file.write(mapserver_config_content)\n    \n  print(f\"Mapserver config is created at {geotiff_mapserver_file_path}\")\n  return geotiff_mapserver_file_path\n\ndef refresh_available_date_list(target_geotiff_date_parent_folder):\n  # geotiff_dir_name = os.path.dirname(target_geotiff_date_parent_folder)\n  \n  # Define columns for the DataFrame\n  columns = [\"date\", \"predicted_wildfire_url_prefix\"]\n  eye_columns = [\"date\"]\n  eye_df = pd.DataFrame(columns=eye_columns)\n  # Create an empty DataFrame with columns\n  for root, dirs, files in os.walk(target_geotiff_date_parent_folder):\n    print(f\"going through {root}\")\n    df = pd.DataFrame(columns=columns)\n    #eye_date_str = re.search(r\"\\d{4}\\d{2}\\d{2}\", root).group()\n    #print(f\"eye_date_str = {eye_date_str}\")\n    try:\n      # Search for the date string in the folder path\n      eye_date_str = re.search(r\"\\d{4}\\d{2}\\d{2}\", root).group()\n      # Print the date string\n      print(\"Date:\", eye_date_str)\n      date = datetime.strptime(eye_date_str, \"%Y%m%d\")\n      formatted_date = date.strftime(\"%Y-%m-%d\")\n      #eye_df.append({\"date\": eye_date_str}, ignore_index=True)\n      new_row = pd.DataFrame([{\"date\": formatted_date}])\n      eye_df = pd.concat([eye_df, new_row], ignore_index=True)\n    except AttributeError:\n      pass\n    # Iterate through files in the current directory\n    for target_geotiff_file in files:\n      if target_geotiff_file.endswith(\".tif\"):\n        print(\"Processing \", target_geotiff_file)\n\n        target_geotiff_file_path = os.path.join(root, target_geotiff_file)\n        # generate map file first\n        create_mapserver_map_config(target_geotiff_file_path, force=True)\n\n        date_str = re.search(r\"\\d{4}\\d{2}\\d{2}\", target_geotiff_file).group()\n        print(f\"forecasting date str = {date_str}\")\n        date = datetime.strptime(date_str, \"%Y%m%d\")\n        formatted_date = date.strftime(\"%Y-%m-%d\")\n        # Append a new row to the DataFrame\n        new_row = pd.DataFrame([{\n          \"date\": formatted_date, \n          \"predicted_wildfire_url_prefix\": f\"{target_geotiff_file}\"\n        }])\n        df = pd.concat([df, new_row], ignore_index=True)\n  \n    # Save DataFrame to a CSV file\n    df.to_csv(f\"{root}/date_list.csv\", index=False)\n  \n  eye_df.to_csv(f\"{target_geotiff_date_parent_folder}/eye_date_list.csv\")\n  print(\"All done\")\n\n\ndef generate_mapfiles():\n  #output_folder = f\"/groups/ESS3/zsun/firecasting/data/output/{output_folder}/\"\n  print(\"Current folder: \", output_folder)\n  refresh_available_date_list(output_folder)\n\nif __name__ == \"__main__\":\n  generate_mapfiles()\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "r7g97f",
  "name" : "check_on_results",
  "description" : null,
  "code" : "# Write first python in Geoweaver\nimport pandas as pd\n\noutput_folder = \"/groups/ESS3/zsun/firecasting/data/output/output_xgboost_window_7_days_forecasting//20210714/\"\n\n#file_path=f\"{output_folder}/firedata_20210720_predicted.txt\"\n\n# file_path = \"/groups/ESS3/zsun/firecasting/data/train/all_cells_new//20200715_time_series_with_window.csv\"\n\n#file_path = \"/groups/ESS3/zsun/firecasting/data/train/all_cells_new_3/20200702_time_series_with_window.csv\"\n#file_path = \"/groups/ESS3/zsun/firecasting/data/train/all_cells_new_5/20201029_time_series_with_window.csv\"\nfile_path = \"/groups/ESS3/zsun/firecasting/data/output/output_xgboost_window_7_days_forecasting//20210714/firedata_20210714_predicted.txt\"\n\n#file_path = \"/groups/ESS3/yli74/data/AI_Emis/firedata/firedata_20200715.txt\"\n\ndf = pd.read_csv(file_path)\n\nprint(df.head())\n\n# Assuming you want to calculate statistics of a column named 'column_name'\n#column_name = ' FRP'\n\n# Basic statistics\n#stats = df[column_name].describe()\n\n#print(stats)\nprint(\"Summary Statistics of the DataFrame:\")\n#print(df.describe(include='all'))\nfor column in df.columns:\n    print(f\"\\nColumn: {column}\")\n    if pd.api.types.is_numeric_dtype(df[column]):\n        print(df[column].describe())\n    else:\n        print(df[column].describe(include=['object']))\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "0lwt5a",
  "name" : "download_prepare_5yrs_ncl",
  "description" : null,
  "code" : "#!/bin/bash\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"download_prepare_5yrs_ncl_slurm.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << 'EOF'\n#!/bin/bash\n#SBATCH -J generate_images_ncl_slurm       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 4               # Number of CPUs per task (threads)\n#SBATCH --mem=20G          # Memory per node (use units like G for gigabytes) - this job must need 20GB lol\n#SBATCH -t 0-10:00         # Runtime in D-HH:MM format\n\nmodule load imagemagick\nif command -v convert >/dev/null 2>&1; then\n    echo \"Command exists.\"\nelse\n    echo \"Command does not exist.\"\nfi\n\nmkdir -p /groups/ESS3/zsun/data/AI_Emis/GLOB\n\necho \"Loading NCL\"\nsource /home/zsun/.bashrc\nmodule load ncl\necho \"Loaded NCL\"\n\nexport YYYYMMDD=\"20160101\"\n\necho \"Drafting download_prepare_5yrs_training_data.ncl\"\ncat <<INNER_EOF >> download_prepare_5yrs_training_data.ncl\n\nsdate=getenv(\"YYYYMMDD\")\nsyyyy=str_get_cols(sdate,0,3)\niyyyy=stringtointeger(syyyy)\nimm=stringtointeger(str_get_cols(sdate,4,5))\nidd=stringtointeger(str_get_cols(sdate,6,7))\nj0=greg2jul(iyyyy,1,1,-1)\njday=greg2jul(iyyyy,imm,idd,-1)\niww=(jday-j0)/7+1\nsww=tostring_with_format(iww,\"%0.3i\")\ndx=0.05\n\ndir_out=\"/groups/ESS3/zsun/data/AI_Emis/GLOBv2\"\n\n;dir_out=\".\"\nfn_out=dir_out+\"/firedata_\"+sdate+\".txt\"\n\nfn_grid=\"/groups/ESS3/yli74/data/S2S/emission_v4/ENS_EMISv4/2016/ENS_Emis_20161231.nc\"\ndir_frp=\"/groups/ESS3/yli74/data/S2S/emission_v4/ENS_EMISv4/\"+syyyy\ndir_fwi=\"/groups/ESS3/yli74/data/FWI/ORI/\"+syyyy\ndir_gdas=\"/scratch/yli74/data/FNL\"\ndir_igbp=\"/groups/ESS/yli74/data/IGBP/0.05_degree\"\ndir_vhi=\"/groups/ESS/yli74/data/VHI/\"+syyyy\n;dir_vhi=\"/groups/ESS3/sma8/data/GHG/ODIAC\"\n\nhlist=[/\"LAT,LON,FRP,FWI,RH,T,U,V,RAIN,VHI_AVE,Land_Use,Nearest_1,Nearest_2,Nearest_3,Nearest_4,Nearest_5,Nearest_6,Nearest_7,Nearest_8,Nearest_9,Nearest_10,Nearest_11,Nearest_12,Nearest_13,Nearest_14,Nearest_15,Nearest_16,Nearest_17,Nearest_18,Nearest_19,Nearest_20,Nearest_21,Nearest_22,Nearest_23,Nearest_24\"/]\nwrite_table(fn_out,\"w\", hlist, \"%s\")\n\nlatmax=90\nlatmin=-90\nlonmax=-180\nlonmin=180\n\n;1. read grid\nf1=addfile(fn_grid,\"r\")\nmlat=f1->lat  ;lat(lat)1791\nmlon=f1->lon  ;lon(lon)3600\nnx=dimsizes(mlon)\nny=dimsizes(mlat)\nprint(\"grid ready\")\nvar=new((/nx*ny,35/),float)\nvar=-999.0\nvar@_FillValue=-999.0\nvartemp=new((/ny,nx,35/),float)\nvartemp=-999.0\nvartemp@_FillValue=-999.0\n\n;2.frp\nf1=addfile(dir_frp+\"/ENS_Emis_\"+sdate+\".nc\",\"r\")\nfrp=tofloat(f1->FRP_MMA2(0,:,:))\nfrp=where(frp.eq.frp@_FillValue,-999.0,frp)\nfrp@_FillValue=-999.0\ndelete(f1)\nprint(\"frp ready\")\n\n;3.fwi\nilon_fwi=asciiread(\"./mask/lon_index_fwi_glob.txt\",-1,\"integer\")\nilat_fwi=asciiread(\"./mask/lat_index_fwi_glob.txt\",-1,\"integer\")\nf1=addfile(dir_fwi+\"/ECMWF_FWI_FWI_\"+sdate+\"_1200_hr_v4.0_con.nc\",\"r\")\nfwi=f1->fwi(0,:,:)\nfwi=where(fwi.eq.fwi@_FillValue,-999.0,fwi)\nfwi@_FillValue=-999.0\ndelete(f1)\nprint(\"fwi ready\")\n\n;4.gdas\nilon_gdas=asciiread(\"./mask/lon_index_gdas_glob.txt\",-1,\"integer\")\nilat_gdas=asciiread(\"./mask/lat_index_gdas_glob.txt\",-1,\"integer\")\nf1=addfile(dir_gdas+\"/gdas1.fnl0p25.\"+sdate+\"00.f00.grib2\",\"r\")\nf2=addfile(dir_gdas+\"/gdas1.fnl0p25.\"+sdate+\"06.f00.grib2\",\"r\")\nf3=addfile(dir_gdas+\"/gdas1.fnl0p25.\"+sdate+\"12.f00.grib2\",\"r\")\nf4=addfile(dir_gdas+\"/gdas1.fnl0p25.\"+sdate+\"18.f00.grib2\",\"r\")\ntemp=f1->TMP_P0_L1_GLL0\ntnx=dimsizes(temp(0,:))\ntny=dimsizes(temp(:,0))\nprint(\"gdas ready\")\n\n;t\nttemp=new((/4,tny,tnx/),float)\nttemp@_FillValue=temp@_FillValue\nttemp(0,:,:)=f1->TMP_P0_L1_GLL0\nttemp(1,:,:)=f2->TMP_P0_L1_GLL0\nttemp(2,:,:)=f3->TMP_P0_L1_GLL0\nttemp(3,:,:)=f4->TMP_P0_L1_GLL0\nt=dim_avg_n(ttemp,0)\nt=where(t.eq.temp@_FillValue,-999.0,t)\nt@_FillValue=-999.0\ndelete([/temp,ttemp/])\nprint(\"t ready\")\n\n;rh\nnh=25\ntemp=f1->RH_P0_L100_GLL0(nh,:,:)\nttemp=new((/4,tny,tnx/),float)\nttemp@_FillValue=temp@_FillValue\nttemp(0,:,:)=f1->RH_P0_L100_GLL0(nh,:,:)\nttemp(1,:,:)=f2->RH_P0_L100_GLL0(nh,:,:)\nttemp(2,:,:)=f3->RH_P0_L100_GLL0(nh,:,:)\nttemp(3,:,:)=f4->RH_P0_L100_GLL0(nh,:,:)\nrh=dim_avg_n(ttemp,0)\nrh=where(rh.eq.temp@_FillValue,-999.0,rh)\nrh@_FillValue=-999.0\ndelete([/temp,ttemp/])\nprint(\"rh ready\")\n\n;u\ntemp=f1->UGRD_P0_L103_GLL0(0,:,:)\nttemp=new((/4,tny,tnx/),float)\nttemp@_FillValue=temp@_FillValue\nttemp(0,:,:)=f1->UGRD_P0_L103_GLL0(0,:,:)\nttemp(1,:,:)=f2->UGRD_P0_L103_GLL0(0,:,:)\nttemp(2,:,:)=f3->UGRD_P0_L103_GLL0(0,:,:)\nttemp(3,:,:)=f4->UGRD_P0_L103_GLL0(0,:,:)\nu=dim_avg_n(ttemp,0)\nu=where(u.eq.temp@_FillValue,-999.0,u)\nu@_FillValue=-999.0\ndelete([/temp,ttemp/])\nprint(\"u ready\")\n\n;v\ntemp=f1->VGRD_P0_L103_GLL0(0,:,:)\nttemp=new((/4,tny,tnx/),float)\nttemp@_FillValue=temp@_FillValue\nttemp(0,:,:)=f1->VGRD_P0_L103_GLL0(0,:,:)\nttemp(1,:,:)=f2->VGRD_P0_L103_GLL0(0,:,:)\nttemp(2,:,:)=f3->VGRD_P0_L103_GLL0(0,:,:)\nttemp(3,:,:)=f4->VGRD_P0_L103_GLL0(0,:,:)\nv=dim_avg_n(ttemp,0)\nv=where(v.eq.temp@_FillValue,-999.0,v)\nv@_FillValue=-999.0\ndelete([/temp,ttemp/])\n\n;rain\ntemp=f1->PWAT_P0_L200_GLL0\nttemp=new((/4,tny,tnx/),float)\nttemp@_FillValue=temp@_FillValue\nttemp(0,:,:)=f1->PWAT_P0_L200_GLL0\nttemp(1,:,:)=f2->PWAT_P0_L200_GLL0\nttemp(2,:,:)=f3->PWAT_P0_L200_GLL0\nttemp(3,:,:)=f4->PWAT_P0_L200_GLL0\nrain=dim_avg_n(ttemp,0)\nrain=where(rain.eq.temp@_FillValue,-999.0,rain)\nrain@_FillValue=-999.0\ndelete([/f1,temp,ttemp/])\n\n;VHI\nif (jday.eq.((iww-1)*7+j0)) then\n  data=asciiread(dir_vhi+\"/VHI.\"+syyyy+sww+\".txt\",-1,\"string\")\n  tvhi=stringtofloat(str_get_field(data,3,\" \"))\n  tvhilat=stringtofloat(str_get_field(data,2,\" \"))\n  tvhilon=stringtofloat(str_get_field(data,1,\" \"))\n  tvhilon=where(tvhilon.ge.180,tvhilon-360,tvhilon)\n  tlat=onedtond(tvhilat,(/3616,10000/))\n  tlon=onedtond(tvhilon,(/3616,10000/))\n  vhilat=tlat(:,0)\n  vhilon=tlon(0,:)\n  vhi=onedtond(tvhi,(/3616,10000/))\n  delete([/data,tlat,tlon,tvhilon,tvhilat/])\nelse\n  tvhi=asciiread(\"./temp/VHI.\"+syyyy+sww+\".txt\",-1,\"float\")\n  vhi=onedtond(tvhi,(/ny,nx/))\n  vartemp(:,:,9)=vhi\n  delete([/tvhi,vhi/])\nend if\n;ilon_vhi=asciiread(\"./mask/lon_index_vhi_glob.txt\",(/3600,10/),\"integer\")\n;ilat_vhi=asciiread(\"./mask/lat_index_vhi_glob.txt\",(/1791,10/),\"integer\")\n;f1=addfile(dir_vhi+\"/VHI.\"+syyyy+sww+\".nc\",\"r\")\n;vhilat=f1->lat\n;vhilon=f1->lon\n;vhilon=where(vhilon.ge.180,vhilon-360,vhilon)\n;vhi=f1->VHI\n;delete(f1)\n\n;Land use\nif (jday.eq.j0) then\n  f1=addfile(dir_igbp+\"/IGBP_\"+syyyy+\".nc\",\"r\")\n  LUper=byte2flt(f1->Land_Cover_Type_1_Percent)\n  LUper=where(LUper.ne.LUper@_FillValue,LUper,-999)\n  lulat=tofloat(f1->latitude)\n  lulon=tofloat(f1->longitude)\n  lulon=where(lulon.ge.180,lulon-360,lulon)\n  delete([/f1/])\nelse\n  tLUper=asciiread(\"./temp/LU.\"+syyyy+\".txt\",-1,\"float\")\n  LUper=onedtond(tLUper,(/ny,nx/))\n  vartemp(:,:,10)=LUper\n  delete([/tLUper,LUper/])\nend if\n\n;----add to var-----\ndo ix=2,nx-3\n  do iy=2,ny-3\n;1-2,lat lon\n    vartemp(iy,ix,0)=(/mlat(iy)/)\n    vartemp(iy,ix,1)=(/mlon(ix)/)\n;3 frp\n    vartemp(iy,ix,2)=(/frp(iy,ix)/)\n;4 fwi\n    vartemp(iy,ix,3)=(/fwi(ilat_fwi(iy),ilon_fwi(ix))/)\n;5-8 t, rh, u, v\n    vartemp(iy,ix,4)=(/t(ilat_gdas(iy),ilon_gdas(ix))/)\n    vartemp(iy,ix,5)=(/rh(ilat_gdas(iy),ilon_gdas(ix))/)\n    vartemp(iy,ix,6)=(/u(ilat_gdas(iy),ilon_gdas(ix))/)\n    vartemp(iy,ix,7)=(/v(ilat_gdas(iy),ilon_gdas(ix))/)\n    vartemp(iy,ix,8)=(/rain(ilat_gdas(iy),ilon_gdas(ix))/)\n;9 VHI\n    if (jday.eq.((iww-1)*7+j0)) then\n      aa=ind((vhilat.ge.(mlat(iy)-dx)).and.(vhilat.le.(mlat(iy)+dx)))\n      bb=ind((vhilon.ge.(mlon(ix)-dx)).and.(vhilon.le.(mlon(ix)+dx)))\n      if ((.not.all(ismissing(aa))).and.(.not.all(ismissing(bb))))\n        temp=ndtooned(vhi(aa,bb))\n        if (.not.all(ismissing(temp)))\n          vartemp(iy,ix,9)=avg(temp)\n        end if\n        delete(temp)\n      end if\n      delete([/aa,bb/])\n    end if\n;;10 lu\n    if (jday.eq.j0) then\n      temp=new(dimsizes(LUper(0,0,:)),float)\n      aa=ind((lulat.ge.(mlat(iy)-dx)).and.(lulat.le.(mlat(iy)+dx)))\n      bb=ind((lulon.ge.(mlon(ix)-dx)).and.(lulon.le.(mlon(ix)+dx)))\n      if ((.not.all(ismissing(aa))).and.(.not.all(ismissing(bb)))) then\n        do il=0,dimsizes(temp)-1\n          temp(il)=avg(LUper(aa,bb,il))\n        end do\n        vartemp(iy,ix,10)=maxind(temp)\n        delete(temp)\n      end if\n      delete([/aa,bb/])\n    end if\n;11- nearby FRP\n    vartemp(iy,ix,11)=(/frp((iy),(ix+1))/)\n    vartemp(iy,ix,12)=(/frp((iy-1),(ix+1))/)\n    vartemp(iy,ix,13)=(/frp((iy-1),(ix))/)\n    vartemp(iy,ix,14)=(/frp((iy-1),(ix-1))/)\n    vartemp(iy,ix,15)=(/frp((iy),(ix-1))/)\n    vartemp(iy,ix,16)=(/frp((iy+1),(ix-1))/)\n    vartemp(iy,ix,17)=(/frp((iy+1),(ix))/)\n    vartemp(iy,ix,18)=(/frp((iy+1),(ix+1))/)\n    vartemp(iy,ix,19)=(/frp((iy),(ix+2))/)\n    vartemp(iy,ix,20)=(/frp((iy-1),(ix+2))/)\n    vartemp(iy,ix,21)=(/frp((iy-2),(ix+2))/)\n    vartemp(iy,ix,22)=(/frp((iy-2),(ix+1))/)\n    vartemp(iy,ix,23)=(/frp((iy-2),(ix))/)\n    vartemp(iy,ix,24)=(/frp((iy-2),(ix-1))/)\n    vartemp(iy,ix,25)=(/frp((iy-2),(ix-2))/)\n    vartemp(iy,ix,26)=(/frp((iy-1),(ix-2))/)\n    vartemp(iy,ix,27)=(/frp((iy),(ix-2))/)\n    vartemp(iy,ix,28)=(/frp((iy+1),(ix-2))/)\n    vartemp(iy,ix,29)=(/frp((iy+2),(ix-2))/)\n    vartemp(iy,ix,30)=(/frp((iy+2),(ix-1))/)\n    vartemp(iy,ix,31)=(/frp((iy+2),(ix))/)\n    vartemp(iy,ix,32)=(/frp((iy+2),(ix+1))/)\n    vartemp(iy,ix,33)=(/frp((iy+2),(ix+2))/)\n    vartemp(iy,ix,34)=(/frp((iy+1),(ix+2))/)\n  end do\nend do\ndo iv=0,34\n  var(:,iv)=ndtooned(vartemp(:,:,iv))\nend do\nwrite_table(fn_out,\"a\",\\\n         [/var(:,0),var(:,1),var(:,2),var(:,3),\\\n           var(:,4),var(:,5),var(:,6),var(:,7),\\\n           var(:,8),var(:,9),var(:,10),var(:,11),\\\n           var(:,12),var(:,13),var(:,14),var(:,15),\\\n           var(:,16),var(:,17),var(:,18),var(:,19),\\\n           var(:,20),var(:,21),var(:,22),var(:,23),\\\n           var(:,24),var(:,25),var(:,26),var(:,27),\\\n           var(:,28),var(:,29),var(:,30),var(:,31),\\\n           var(:,32),var(:,33),var(:,34)/],\\\n         \"%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f\")\nif (jday.eq.((iww-1)*7+j0)) then\n  write_table(\"./temp/VHI.\"+syyyy+sww+\".txt\",\"w\",[/var(:,9)/],\"%f\")\nend if\nif (jday.eq.j0) then\n  write_table(\"./temp/LU.\"+syyyy+\".txt\",\"w\",[/var(:,10)/],\"%f\")\nend if\n\nexit\nINNER_EOF\n\necho \"Start to run the NCL script: download_prepare_5yrs_training_data.ncl\"\n\necho \"ncl download_prepare_5yrs_training_data.ncl\"\n\nncl download_prepare_5yrs_training_data.ncl\n\necho \"Finished download_prepare_5yrs_training_data.ncl\"\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nfile_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\nprevious_content=$(cat file_name)\nexit_code=0\nwhile true; do\n    # Capture the current content\n    file_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\n    current_content=$(<\"${file_name}\")\n\n    # Compare current content with previous content\n    diff_result=$(diff <(echo \"$previous_content\") <(echo \"$current_content\"))\n    # Check if there is new content\n    if [ -n \"$diff_result\" ]; then\n        # Print the newly added content\n        echo \"$diff_result\"\n    fi\n    # Update previous content\n    previous_content=\"$current_content\"\n\n\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    #echo \"job_status \"$job_status\n    #if [[ $job_status == \"JobState=COMPLETED\" ]]; then\n    #    break\n    #fi\n    if [[ $job_status == *\"COMPLETED\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    elif [[ $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        exit_code=1\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\nfind /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\ncat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n\nexit $exit_code\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
}]
